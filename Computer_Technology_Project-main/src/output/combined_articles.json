[
  {
    "title": "The Impact of the Object-Oriented Software Evolution on Software\n  Metrics: The Iris Approach",
    "date": "2018-03-15",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ra'Fat Al-Msie'deen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1803.09823v1"
    },
    "publicTags": [],
    "summary": "The Object-Oriented (OO) software system evolves over the time to meet the\nnew requirements. Based on the initial release of software, the continuous\nmodification of software code leads to software evolution. Software needs to\nevolve over the time to meet the new user's requirements. Software companies\noften develop variant software of the original one depends on customers' needs.\nThe main hypothesis of this paper states that the software when it evolves over\nthe time, its code continues to grow, change and become more complex. This\npaper proposes an automatic approach (Iris) to examine the proposed hypothesis.\nOriginality of this approach is the exploiting of the software variants to\nstudy the impact of software evolution on the software metrics. This paper\npresents the results of experiments conducted on three releases of drawing\nshapes software, sixteen releases of rhino software, eight releases of mobile\nmedia software and ten releases of ArgoUML software. Based on the extracted\nsoftware metrics, It has been found that Iris hypothesis is supported by the\ncomputed metrics.",
    "sourceUrl": "http://arxiv.org/abs/1803.09823v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The Object-Oriented (OO) software system evolves over the time to meet the\nnew requirements. Based on the initial release of software, the continuous\nmodification of software code leads to software evolution. Software needs to\nevolve over the time to meet the new user's requirements. Software companies\noften develop variant software of the original one depends on customers' needs.\nThe main hypothesis of this paper states that the software when it evolves over\nthe time, its code continues to grow, change and become more complex. This\npaper proposes an automatic approach (Iris) to examine the proposed hypothesis.\nOriginality of this approach is the exploiting of the software variants to\nstudy the impact of software evolution on the software metrics. This paper\npresents the results of experiments conducted on three releases of drawing\nshapes software, sixteen releases of rhino software, eight releases of mobile\nmedia software and ten releases of ArgoUML software. Based on the extracted\nsoftware metrics, It has been found that Iris hypothesis is supported by the\ncomputed metrics.</p>"
  },
  {
    "title": "Increase of Software Safety",
    "date": "2008-07-01",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Arkadiy Khandjian",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/0807.0161v1"
    },
    "publicTags": [],
    "summary": "New model of software safety is offered. Distribution of mistakes in program\non stages of life cycle is researched. Study of ways of increase of reliability\nof software at help simulation program is leaded.",
    "sourceUrl": "http://arxiv.org/abs/0807.0161v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>New model of software safety is offered. Distribution of mistakes in program\non stages of life cycle is researched. Study of ways of increase of reliability\nof software at help simulation program is leaded.</p>"
  },
  {
    "title": "The Study and Approach of Software Re-Engineering",
    "date": "2011-12-17",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Phuc V. Nguyen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1112.4016v1"
    },
    "publicTags": [],
    "summary": "The nature of software re-engineering is to improve or transform existing\nsoftware so it can be understood, controlled and reused as new software. Needs,\nthe necessity of re-engineering software has greatly increased. The system\nsoftware has become obsolete no longer used in architecture, platform they're\nrunning, stable and consistent they support the development and support needs\nchange. Software re-engineering is vital to restore and reuse the things\ninherent in the existing software, put the cost of software maintenance to the\nlowest in the control and establish a basis for the development of software in\nthe future.",
    "sourceUrl": "http://arxiv.org/abs/1112.4016v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The nature of software re-engineering is to improve or transform existing\nsoftware so it can be understood, controlled and reused as new software. Needs,\nthe necessity of re-engineering software has greatly increased. The system\nsoftware has become obsolete no longer used in architecture, platform they're\nrunning, stable and consistent they support the development and support needs\nchange. Software re-engineering is vital to restore and reuse the things\ninherent in the existing software, put the cost of software maintenance to the\nlowest in the control and establish a basis for the development of software in\nthe future.</p>"
  },
  {
    "title": "A Noble Methodology for Users Work Process Driven Software Requirements\n  for Smart Handheld Devices",
    "date": "2014-08-12",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Tamjid Rahman",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1408.2687v1"
    },
    "publicTags": [],
    "summary": "Requirement engineering is a key ingredient for software development to be\neffective. Apart from the traditional software requirement which is not much\nappropriate for new emerging software such as smart handheld device based\nsoftware. In many perspectives of requirement engineering, traditional and new\nemerging software are not similar. Whereas requirement engineering of\ntraditional software needs more research, it is obvious that new emerging\nsoftware needs methodically and in-depth research for improved productivity,\nquality, risk management and validity. In particular, the result of this paper\nshows that how effective requirement engineering can improve in project\nnegotiation, project planning, managing feature creep, testing, defect, rework\nand product quality. This paper also shows a new methodology which is focused\non users work process applicable for eliciting the requirement of traditional\nsoftware and any new type software of smart handheld device such as iPad. As an\nexample, the paper shows how the methodology will be applied as a software\nrequirement of iPad-based software for play-group students.",
    "sourceUrl": "http://arxiv.org/abs/1408.2687v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Requirement engineering is a key ingredient for software development to be\neffective. Apart from the traditional software requirement which is not much\nappropriate for new emerging software such as smart handheld device based\nsoftware. In many perspectives of requirement engineering, traditional and new\nemerging software are not similar. Whereas requirement engineering of\ntraditional software needs more research, it is obvious that new emerging\nsoftware needs methodically and in-depth research for improved productivity,\nquality, risk management and validity. In particular, the result of this paper\nshows that how effective requirement engineering can improve in project\nnegotiation, project planning, managing feature creep, testing, defect, rework\nand product quality. This paper also shows a new methodology which is focused\non users work process applicable for eliciting the requirement of traditional\nsoftware and any new type software of smart handheld device such as iPad. As an\nexample, the paper shows how the methodology will be applied as a software\nrequirement of iPad-based software for play-group students.</p>"
  },
  {
    "title": "Morescient GAI for Software Engineering (Extended Version)",
    "date": "2024-06-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Marcus Kessel",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2406.04710v2"
    },
    "publicTags": [],
    "summary": "The ability of Generative AI (GAI) technology to automatically check,\nsynthesize and modify software engineering artifacts promises to revolutionize\nall aspects of software engineering. Using GAI for software engineering tasks\nis consequently one of the most rapidly expanding fields of software\nengineering research, with over a hundred LLM-based code models having been\npublished since 2021. However, the overwhelming majority of existing code\nmodels share a major weakness - they are exclusively trained on the syntactic\nfacet of software, significantly lowering their trustworthiness in tasks\ndependent on software semantics. To address this problem, a new class of\n\"Morescient\" GAI is needed that is \"aware\" of (i.e., trained on) both the\nsemantic and static facets of software. This, in turn, will require a new\ngeneration of software observation platforms capable of generating large\nquantities of execution observations in a structured and readily analyzable\nway. In this paper, we present a vision and roadmap for how such \"Morescient\"\nGAI models can be engineered, evolved and disseminated according to the\nprinciples of open science.",
    "sourceUrl": "http://arxiv.org/abs/2406.04710v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The ability of Generative AI (GAI) technology to automatically check,\nsynthesize and modify software engineering artifacts promises to revolutionize\nall aspects of software engineering. Using GAI for software engineering tasks\nis consequently one of the most rapidly expanding fields of software\nengineering research, with over a hundred LLM-based code models having been\npublished since 2021. However, the overwhelming majority of existing code\nmodels share a major weakness - they are exclusively trained on the syntactic\nfacet of software, significantly lowering their trustworthiness in tasks\ndependent on software semantics. To address this problem, a new class of\n\"Morescient\" GAI is needed that is \"aware\" of (i.e., trained on) both the\nsemantic and static facets of software. This, in turn, will require a new\ngeneration of software observation platforms capable of generating large\nquantities of execution observations in a structured and readily analyzable\nway. In this paper, we present a vision and roadmap for how such \"Morescient\"\nGAI models can be engineered, evolved and disseminated according to the\nprinciples of open science.</p>"
  },
  {
    "title": "Software Evolution Understanding: Automatic Extraction of Software\n  Identifiers Map for Object-Oriented Software Systems",
    "date": "2021-10-03",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ra'Fat AL-msie'deen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2110.00980v1"
    },
    "publicTags": [],
    "summary": "Software companies usually develop a set of product variants within the same\nfamily that share certain functions and differ in others. Variations across\nsoftware variants occur to meet different customer requirements. Thus, software\nproduct variants evolve overtime to cope with new requirements. A software\nengineer who deals with this family may find it difficult to understand the\nevolution scenarios that have taken place over time. In addition, software\nidentifier names are important resources to understand the evolution scenarios\nin this family. This paper introduces an automatic approach called Juana's\napproach to detect the evolution scenario across two product variants at the\nsource code level and identifies the common and unique software identifier\nnames across software variants source code. Juana's approach refers to common\nand unique identifier names as a software identifiers map and computes it by\ncomparing software variants to each other. Juana considers all software\nidentifier names such as package, class, attribute, and method. The novelty of\nthis approach is that it exploits common and unique identifier names across the\nsource code of software variants, to understand the evolution scenarios across\nsoftware family in an efficient way. For validity, Juana was applied on ArgoUML\nand Mobile Media software variants. The results of this evaluation validate the\nrelevance and the performance of the approach as all evolution scenarios were\ncorrectly detected via a software identifiers map.",
    "sourceUrl": "http://arxiv.org/abs/2110.00980v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software companies usually develop a set of product variants within the same\nfamily that share certain functions and differ in others. Variations across\nsoftware variants occur to meet different customer requirements. Thus, software\nproduct variants evolve overtime to cope with new requirements. A software\nengineer who deals with this family may find it difficult to understand the\nevolution scenarios that have taken place over time. In addition, software\nidentifier names are important resources to understand the evolution scenarios\nin this family. This paper introduces an automatic approach called Juana's\napproach to detect the evolution scenario across two product variants at the\nsource code level and identifies the common and unique software identifier\nnames across software variants source code. Juana's approach refers to common\nand unique identifier names as a software identifiers map and computes it by\ncomparing software variants to each other. Juana considers all software\nidentifier names such as package, class, attribute, and method. The novelty of\nthis approach is that it exploits common and unique identifier names across the\nsource code of software variants, to understand the evolution scenarios across\nsoftware family in an efficient way. For validity, Juana was applied on ArgoUML\nand Mobile Media software variants. The results of this evaluation validate the\nrelevance and the performance of the approach as all evolution scenarios were\ncorrectly detected via a software identifiers map.</p>"
  },
  {
    "title": "SRS BUILDER 1.0: An Upper Type CASE Tool For Requirement Specification",
    "date": "2011-09-08",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ardhendu Mandal",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1109.1651v1"
    },
    "publicTags": [],
    "summary": "Software (SW) development is a labor intensive activity. Modern software\nprojects generally have to deal with producing and managing large and complex\nsoftware products. Developing such software has become an extremely challenging\njob not only because of inherent complexity, but also mainly for economic\nconstraints unlike time, quality, maintainability concerns. Hence, developing\nmodern software within the budget still remains as one of the main software\ncrisis. The most significant way to reduce the software development cost is to\nuse the Computer-Aided Software Engineering (CASE) tools over the entire\nSoftware Development Life Cycle (SDLC) process as substitute to expensive human\nlabor cost. We think that automation of software development methods is a\nvaluable support for the software engineers in coping with this complexity and\nfor improving quality too. This paper demonstrates the newly developed CASE\ntools name \"SRS Builder 1.0\" for software requirement specification developed\nat our university laboratory, University of North Bengal, India. This paper\ndiscusses our new developed product with its functionalities and usages. We\nbelieve the tool has the potential to play an important role in the software\ndevelopment process.",
    "sourceUrl": "http://arxiv.org/abs/1109.1651v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software (SW) development is a labor intensive activity. Modern software\nprojects generally have to deal with producing and managing large and complex\nsoftware products. Developing such software has become an extremely challenging\njob not only because of inherent complexity, but also mainly for economic\nconstraints unlike time, quality, maintainability concerns. Hence, developing\nmodern software within the budget still remains as one of the main software\ncrisis. The most significant way to reduce the software development cost is to\nuse the Computer-Aided Software Engineering (CASE) tools over the entire\nSoftware Development Life Cycle (SDLC) process as substitute to expensive human\nlabor cost. We think that automation of software development methods is a\nvaluable support for the software engineers in coping with this complexity and\nfor improving quality too. This paper demonstrates the newly developed CASE\ntools name \"SRS Builder 1.0\" for software requirement specification developed\nat our university laboratory, University of North Bengal, India. This paper\ndiscusses our new developed product with its functionalities and usages. We\nbelieve the tool has the potential to play an important role in the software\ndevelopment process.</p>"
  },
  {
    "title": "What Practitioners Really Think About Continuous Software Engineering: A\n  Taxonomy of Challenges",
    "date": "2023-03-30",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Muhammad Zohaib",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2303.17271v1"
    },
    "publicTags": [],
    "summary": "The Continuous software engineering is a collaborative software development\nenvironment which offers the continues development and deployment of quality\nsoftware project within short time. The Continuous software engineering\npractices are not yet mature enough, and the software organizations hesitate to\nadopt it. This study aims: (1) to explore the Continuous software engineering\nchallenges by conducting systematic literature review (SLR) and to get the\ninsight of industry experts via questionnaire survey study; (2) to prioritize\nthe investigated challenges using fuzzy analytical hierarchy process (FAHP).\nThe study findings provides the set of critical challenges faced by the\nsoftware organizations while adopting Continuous software engineering and a\nprioritization based taxonomy of the Continuous software engineering\nchallenges. The application of FAHP is novel in this research area as it\nassists in addressing the vagueness of practitioners concerning the influencing\nfactors of Continuous software engineering. We believe that the finding of this\nstudy will serve as a body of knowledge for real world practitioners and\nresearchers to revise and develop the new strategies for the successful\nimplementation of Continuous software engineering practices in the software\nindustry.",
    "sourceUrl": "http://arxiv.org/abs/2303.17271v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The Continuous software engineering is a collaborative software development\nenvironment which offers the continues development and deployment of quality\nsoftware project within short time. The Continuous software engineering\npractices are not yet mature enough, and the software organizations hesitate to\nadopt it. This study aims: (1) to explore the Continuous software engineering\nchallenges by conducting systematic literature review (SLR) and to get the\ninsight of industry experts via questionnaire survey study; (2) to prioritize\nthe investigated challenges using fuzzy analytical hierarchy process (FAHP).\nThe study findings provides the set of critical challenges faced by the\nsoftware organizations while adopting Continuous software engineering and a\nprioritization based taxonomy of the Continuous software engineering\nchallenges. The application of FAHP is novel in this research area as it\nassists in addressing the vagueness of practitioners concerning the influencing\nfactors of Continuous software engineering. We believe that the finding of this\nstudy will serve as a body of knowledge for real world practitioners and\nresearchers to revise and develop the new strategies for the successful\nimplementation of Continuous software engineering practices in the software\nindustry.</p>"
  },
  {
    "title": "New Failure Rate Model for Iterative Software Development Life Cycle\n  Process",
    "date": "2019-10-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": " Sangeeta",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1910.00903v1"
    },
    "publicTags": [],
    "summary": "Software reliability models are one of the most generally used mathematical\ntool for estimation of reliability, failure rate and number of remaining faults\nin the software. Existing software reliability models are designed to follow\nwaterfall software development life cycle process. These existing models do not\ntake advantage of iterative software development process. In this paper, a new\nfailure rate model centered on iterative software development life cycle\nprocess has been developed. It aims to integrate a new modulation factor for\nincorporating varying needs in each phase of iterative software development\nprocess. It comprises imperfect debugging with the possibility of fault\nintroduction and removal of multiple faults in an interval as iterative\ndevelopment of the software proceeds. The proposed model has been validated on\ntwelve iterations of Eclipse software failure dataset and nine iterations of\nJava Development toolkit (JDT) software failure dataset. Parameter estimation\nfor the proposed model has been done by hybrid Particle Swarm Optimization and\nGravitational Search Algorithm. Experimental results in-terms of\ngoodness-of-fit shows that proposed model has outperformed Jelinski Moranda,\nShick Wolverton, Goel Okummotto Imperfect debugging, GS Mahapatra, Modified\nShick Wolverton in 83.33 % of iterations for eclipse dataset and 77.77% of\niterations for JDT dataset.",
    "sourceUrl": "http://arxiv.org/abs/1910.00903v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software reliability models are one of the most generally used mathematical\ntool for estimation of reliability, failure rate and number of remaining faults\nin the software. Existing software reliability models are designed to follow\nwaterfall software development life cycle process. These existing models do not\ntake advantage of iterative software development process. In this paper, a new\nfailure rate model centered on iterative software development life cycle\nprocess has been developed. It aims to integrate a new modulation factor for\nincorporating varying needs in each phase of iterative software development\nprocess. It comprises imperfect debugging with the possibility of fault\nintroduction and removal of multiple faults in an interval as iterative\ndevelopment of the software proceeds. The proposed model has been validated on\ntwelve iterations of Eclipse software failure dataset and nine iterations of\nJava Development toolkit (JDT) software failure dataset. Parameter estimation\nfor the proposed model has been done by hybrid Particle Swarm Optimization and\nGravitational Search Algorithm. Experimental results in-terms of\ngoodness-of-fit shows that proposed model has outperformed Jelinski Moranda,\nShick Wolverton, Goel Okummotto Imperfect debugging, GS Mahapatra, Modified\nShick Wolverton in 83.33 % of iterations for eclipse dataset and 77.77% of\niterations for JDT dataset.</p>"
  },
  {
    "title": "End-of-Life of Software How is it Defined and Managed?",
    "date": "2022-04-08",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Zena Assaad",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2204.03800v1"
    },
    "publicTags": [],
    "summary": "The rapid development of new software and algorithms, fueled by the immense\namount of data available, has made the shelf life of software products a lot\nshorter. With a rough estimate of more than 40,000 new software projects\ndeveloped every day, it is becoming quicker and cheaper to abandon old software\nand acquire new software that meets rapidly changing needs and demands. What\nhappens to software that is abandoned and what consequences may arise from\n'throwaway' culture (Cooper, 2005) are still open questions. This paper will\nexplore the systems engineering concept of end-of-life for software, it will\nhighlight the gaps in existing software engineering practices, it will bring\nforward examples of software that has been abandoned in an attempt to\ndecommission and it will explore the repercussions of abandoned software\nartefacts. A proposed way forward for addressing the identified research gaps\nis also detailed.",
    "sourceUrl": "http://arxiv.org/abs/2204.03800v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The rapid development of new software and algorithms, fueled by the immense\namount of data available, has made the shelf life of software products a lot\nshorter. With a rough estimate of more than 40,000 new software projects\ndeveloped every day, it is becoming quicker and cheaper to abandon old software\nand acquire new software that meets rapidly changing needs and demands. What\nhappens to software that is abandoned and what consequences may arise from\n'throwaway' culture (Cooper, 2005) are still open questions. This paper will\nexplore the systems engineering concept of end-of-life for software, it will\nhighlight the gaps in existing software engineering practices, it will bring\nforward examples of software that has been abandoned in an attempt to\ndecommission and it will explore the repercussions of abandoned software\nartefacts. A proposed way forward for addressing the identified research gaps\nis also detailed.</p>"
  },
  {
    "title": "Software Startup Practices -- Software Development in Startups through\n  the Lens of the Essence Theory of Software Engineering",
    "date": "2021-02-11",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Kai-Kristian Kemell",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2103.03648v1"
    },
    "publicTags": [],
    "summary": "Software startups continue to be important drivers of economy globally. As\nthe initial investment required to found a new software company becomes smaller\nand smaller resulting from technological advances such as cloud technology,\nincreasing numbers of new software startups are born. Typically, the main\nargument for studying software startups is that they differ from mature\nsoftware organizations in various ways, thus making the findings of many\nexisting studies not directly applicable to them. How, exactly, software\nstartups really differ from other types of software organizations as an\non-going debate. In this paper, we seek to better understand how software\nstartups differ from mature software organizations in terms of development\npractices. Past studies have primarily studied method use, and in comparison,\nwe take on a more atomic approach by focusing on practices. Utilizing the\nEssence Theory of Software Engineering as a framework, we split these practices\ninto categories for analysis while simultaneously evaluating the suitability of\nthe theory for the context of software startups. Based on the results, we\npropose changes to the Essence Theory of Software Engineering for it to better\nfit the startup context.",
    "sourceUrl": "http://arxiv.org/abs/2103.03648v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software startups continue to be important drivers of economy globally. As\nthe initial investment required to found a new software company becomes smaller\nand smaller resulting from technological advances such as cloud technology,\nincreasing numbers of new software startups are born. Typically, the main\nargument for studying software startups is that they differ from mature\nsoftware organizations in various ways, thus making the findings of many\nexisting studies not directly applicable to them. How, exactly, software\nstartups really differ from other types of software organizations as an\non-going debate. In this paper, we seek to better understand how software\nstartups differ from mature software organizations in terms of development\npractices. Past studies have primarily studied method use, and in comparison,\nwe take on a more atomic approach by focusing on practices. Utilizing the\nEssence Theory of Software Engineering as a framework, we split these practices\ninto categories for analysis while simultaneously evaluating the suitability of\nthe theory for the context of software startups. Based on the results, we\npropose changes to the Essence Theory of Software Engineering for it to better\nfit the startup context.</p>"
  },
  {
    "title": "Quantum Software Engineering: A New Genre of Computing",
    "date": "2022-11-25",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Muhammad Azeem Akbar",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2211.13990v1"
    },
    "publicTags": [],
    "summary": "Quantum computing (QC) is no longer only a scientific interest but is rapidly\nbecoming an industrially available technology that can potentially tackle the\nlimitations of classical computing. Over the last few years, major technology\ngiants have invested in developing hardware and programming frameworks to\ndevelop quantum-specific applications. QC hardware technologies are gaining\nmomentum, however, operationalizing the QC technologies trigger the need for\nsoftware-intensive methodologies, techniques, processes, tools, roles, and\nresponsibilities for developing industrial-centric quantum software\napplications. This paper presents the vision of the quantum software\nengineering (QSE) life cycle consisting of quantum requirements engineering,\nquantum software design, quantum software implementation, quantum software\ntesting, and quantum software maintenance. This paper particularly calls for\njoint contributions of software engineering research and industrial community\nto present real-world solutions to support the entire quantum software\ndevelopment activities. The proposed vision facilitates the researchers and\npractitioners to propose new processes, reference architectures, novel tools,\nand practices to leverage quantum computers and develop emerging and next\ngenerations of quantum software.",
    "sourceUrl": "http://arxiv.org/abs/2211.13990v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Quantum computing (QC) is no longer only a scientific interest but is rapidly\nbecoming an industrially available technology that can potentially tackle the\nlimitations of classical computing. Over the last few years, major technology\ngiants have invested in developing hardware and programming frameworks to\ndevelop quantum-specific applications. QC hardware technologies are gaining\nmomentum, however, operationalizing the QC technologies trigger the need for\nsoftware-intensive methodologies, techniques, processes, tools, roles, and\nresponsibilities for developing industrial-centric quantum software\napplications. This paper presents the vision of the quantum software\nengineering (QSE) life cycle consisting of quantum requirements engineering,\nquantum software design, quantum software implementation, quantum software\ntesting, and quantum software maintenance. This paper particularly calls for\njoint contributions of software engineering research and industrial community\nto present real-world solutions to support the entire quantum software\ndevelopment activities. The proposed vision facilitates the researchers and\npractitioners to propose new processes, reference architectures, novel tools,\nand practices to leverage quantum computers and develop emerging and next\ngenerations of quantum software.</p>"
  },
  {
    "title": "Systematic Innovation Mounted Software Development Process and Intuitive\n  Project Management Framework for Lean Startups",
    "date": "2017-08-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Song-Kyoo Kim",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1708.06900v2"
    },
    "publicTags": [],
    "summary": "This paper provides a new process which integrates an inventive problem\nsolving method into one modern software development program, making it part of\nthe software development process. The research question is how to improve\nsoftware development process which tech startups could adopt with minor project\nmanagement skills. The Systematic Innovation Mounted Software Development\nProcess, a combination of Agile and Systematic Innovation, provides an\nalternative development process which is targeted to adapt idea generation into\nsoftware products. The intuitive project management framework helps technology\ndriven companies to manage their software projects more effectively. The\nimplication and aim of this research are providing the guideline to help the\nentrepreneurs for managing their project properly. The Systematic Innovation\nmodel helps to generate new ideas and innovative ways to solve problems with\nthe collaboration with the existing Agile model. The new software development\nprocess and associated techniques could impact the current software development\nindustry significantly, especially software startup companies, because these\npowerful tools can help reduce managerial workloads of the companies and give\nthem more time to remain focused on their key technologies.",
    "sourceUrl": "http://arxiv.org/abs/1708.06900v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>This paper provides a new process which integrates an inventive problem\nsolving method into one modern software development program, making it part of\nthe software development process. The research question is how to improve\nsoftware development process which tech startups could adopt with minor project\nmanagement skills. The Systematic Innovation Mounted Software Development\nProcess, a combination of Agile and Systematic Innovation, provides an\nalternative development process which is targeted to adapt idea generation into\nsoftware products. The intuitive project management framework helps technology\ndriven companies to manage their software projects more effectively. The\nimplication and aim of this research are providing the guideline to help the\nentrepreneurs for managing their project properly. The Systematic Innovation\nmodel helps to generate new ideas and innovative ways to solve problems with\nthe collaboration with the existing Agile model. The new software development\nprocess and associated techniques could impact the current software development\nindustry significantly, especially software startup companies, because these\npowerful tools can help reduce managerial workloads of the companies and give\nthem more time to remain focused on their key technologies.</p>"
  },
  {
    "title": "On Assessing the Complexity of Software Architectures",
    "date": "2001-05-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jianjun Zhao",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/cs/0105010v1"
    },
    "publicTags": [],
    "summary": "This paper proposes some new architectural metrics which are appropriate for\nevaluating the architectural attributes of a software system. The main feature\nof our approach is to assess the complexity of a software architecture by\nanalyzing various types of architectural dependences in the architecture.",
    "sourceUrl": "http://arxiv.org/abs/cs/0105010v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>This paper proposes some new architectural metrics which are appropriate for\nevaluating the architectural attributes of a software system. The main feature\nof our approach is to assess the complexity of a software architecture by\nanalyzing various types of architectural dependences in the architecture.</p>"
  },
  {
    "title": "swMATH - a new information service for mathematical software",
    "date": "2013-06-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Sebastian BÃ¶nisch",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1306.1036v1"
    },
    "publicTags": [],
    "summary": "An information service for mathematical software is presented. Publications\nand software are two closely connected facets of mathematical knowledge. This\nrelation can be used to identify mathematical software and find relevant\ninformation about it. The approach and the state of the art of the information\nservice are described here.",
    "sourceUrl": "http://arxiv.org/abs/1306.1036v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>An information service for mathematical software is presented. Publications\nand software are two closely connected facets of mathematical knowledge. This\nrelation can be used to identify mathematical software and find relevant\ninformation about it. The approach and the state of the art of the information\nservice are described here.</p>"
  },
  {
    "title": "Software Dependability Measurement at the Age Of 36",
    "date": "2023-11-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Robert V. Binder",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2311.10039v1"
    },
    "publicTags": [],
    "summary": "Thirty-six years after the first edition of IEEE standard 982.1, Measures of\nthe Software Aspects of Dependability, the third edition focuses on the\nmeasurement of in-service software dependability. This article explains how\nthis new point of view evolved and shaped the third edition's guidance for\nsoftware dependability measurement.",
    "sourceUrl": "http://arxiv.org/abs/2311.10039v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Thirty-six years after the first edition of IEEE standard 982.1, Measures of\nthe Software Aspects of Dependability, the third edition focuses on the\nmeasurement of in-service software dependability. This article explains how\nthis new point of view evolved and shaped the third edition's guidance for\nsoftware dependability measurement.</p>"
  },
  {
    "title": "BRIDGE: A Model for Modern Software Development Process to Cater the\n  Present Software Crisis",
    "date": "2011-09-08",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ardhendu Mandal",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1109.1648v1"
    },
    "publicTags": [],
    "summary": "As hardware components are becoming cheaper and powerful day by day, the\nexpected services from modern software are increasing like any thing.\nDeveloping such software has become extremely challenging. Not only the\ncomplexity, but also the developing of such software within the time\nconstraints and budget has become the real challenge. Quality concern and\nmaintainability are added flavour to the challenge. On stream, the requirements\nof the clients are changing so frequently that it has become extremely tough to\nmanage these changes. More often, the clients are unhappy with the end product.\nLarge, complex software projects are notoriously late to market, often exhibit\nquality problems, and don't always deliver on promised functionality. None of\nthe existing models are helpful to cater the modern software crisis. Hence, a\nbetter modern software development process model to handle with the present\nsoftware crisis is badly needed. This paper suggests a new software development\nprocess model, BRIDGE, to tackle present software crisis.",
    "sourceUrl": "http://arxiv.org/abs/1109.1648v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>As hardware components are becoming cheaper and powerful day by day, the\nexpected services from modern software are increasing like any thing.\nDeveloping such software has become extremely challenging. Not only the\ncomplexity, but also the developing of such software within the time\nconstraints and budget has become the real challenge. Quality concern and\nmaintainability are added flavour to the challenge. On stream, the requirements\nof the clients are changing so frequently that it has become extremely tough to\nmanage these changes. More often, the clients are unhappy with the end product.\nLarge, complex software projects are notoriously late to market, often exhibit\nquality problems, and don't always deliver on promised functionality. None of\nthe existing models are helpful to cater the modern software crisis. Hence, a\nbetter modern software development process model to handle with the present\nsoftware crisis is badly needed. This paper suggests a new software development\nprocess model, BRIDGE, to tackle present software crisis.</p>"
  },
  {
    "title": "Software Process Models and Analysis on Failure of Software Development\n  Projects",
    "date": "2013-06-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Rupinder Kaur",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1306.1068v1"
    },
    "publicTags": [],
    "summary": "The software process model consists of a set of activities undertaken to\ndesign, develop and maintain software systems. A variety of software process\nmodels have been designed to structure, describe and prescribe the software\ndevelopment process. The software process models play a very important role in\nsoftware development, so it forms the core of the software product. Software\nproject failure is often devastating to an organization. Schedule slips, buggy\nreleases and missing features can mean the end of the project or even financial\nruin for a company. Oddly, there is disagreement over what it means for a\nproject to fail. In this paper, discussion is done on current process models\nand analysis on failure of software development, which shows the need of new\nresearch.",
    "sourceUrl": "http://arxiv.org/abs/1306.1068v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The software process model consists of a set of activities undertaken to\ndesign, develop and maintain software systems. A variety of software process\nmodels have been designed to structure, describe and prescribe the software\ndevelopment process. The software process models play a very important role in\nsoftware development, so it forms the core of the software product. Software\nproject failure is often devastating to an organization. Schedule slips, buggy\nreleases and missing features can mean the end of the project or even financial\nruin for a company. Oddly, there is disagreement over what it means for a\nproject to fail. In this paper, discussion is done on current process models\nand analysis on failure of software development, which shows the need of new\nresearch.</p>"
  },
  {
    "title": "Software Metrics in Boa Large-Scale Software Mining Infrastructure:\n  Challenges and Solutions",
    "date": "2017-03-18",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Agnieszka Patalas",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1703.06293v1"
    },
    "publicTags": [],
    "summary": "In this paper, we describe our experience implementing some of classic\nsoftware engineering metrics using Boa - a large-scale software repository\nmining platform - and its dedicated language. We also aim to take an advantage\nof the Boa infrastructure to propose new software metrics and to characterize\nopen source projects by software metrics to provide reference values of\nsoftware metrics based on large number of open source projects. Presented\nsoftware metrics, well known and proposed in this paper, can be used to build\nlarge-scale software defect prediction models. Additionally, we present the\nobstacles we met while developing metrics, and our analysis can be used to\nimprove Boa in its future releases. The implemented metrics can also be used as\na foundation for more complex explorations of open source projects and serve as\na guide how to implement software metrics using Boa as the source code of the\nmetrics is freely available to support reproducible research.",
    "sourceUrl": "http://arxiv.org/abs/1703.06293v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In this paper, we describe our experience implementing some of classic\nsoftware engineering metrics using Boa - a large-scale software repository\nmining platform - and its dedicated language. We also aim to take an advantage\nof the Boa infrastructure to propose new software metrics and to characterize\nopen source projects by software metrics to provide reference values of\nsoftware metrics based on large number of open source projects. Presented\nsoftware metrics, well known and proposed in this paper, can be used to build\nlarge-scale software defect prediction models. Additionally, we present the\nobstacles we met while developing metrics, and our analysis can be used to\nimprove Boa in its future releases. The implemented metrics can also be used as\na foundation for more complex explorations of open source projects and serve as\na guide how to implement software metrics using Boa as the source code of the\nmetrics is freely available to support reproducible research.</p>"
  },
  {
    "title": "Would You Like to Motivate Software Testers? Ask Them How",
    "date": "2017-11-21",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ronnie Santos",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1711.07890v1"
    },
    "publicTags": [],
    "summary": "Context. Considering the importance of software testing to the development of\nhigh quality and reliable software systems, this paper aims to investigate how\ncan work-related factors influence the motivation of software testers. Method.\nWe applied a questionnaire that was developed using a previous theory of\nmotivation and satisfaction of software engineers to conduct a survey-based\nstudy to explore and understand how professional software testers perceive and\nvalue work-related factors that could influence their motivation at work.\nResults. With a sample of 80 software testers we observed that software testers\nare strongly motivated by variety of work, creative tasks, recognition for\ntheir work, and activities that allow them to acquire new knowledge, but in\ngeneral the social impact of this activity has low influence on their\nmotivation. Conclusion. This study discusses the difference of opinions among\nsoftware testers, regarding work-related factors that could impact their\nmotivation, which can be relevant for managers and leaders in software\nengineering practice.",
    "sourceUrl": "http://arxiv.org/abs/1711.07890v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Context. Considering the importance of software testing to the development of\nhigh quality and reliable software systems, this paper aims to investigate how\ncan work-related factors influence the motivation of software testers. Method.\nWe applied a questionnaire that was developed using a previous theory of\nmotivation and satisfaction of software engineers to conduct a survey-based\nstudy to explore and understand how professional software testers perceive and\nvalue work-related factors that could influence their motivation at work.\nResults. With a sample of 80 software testers we observed that software testers\nare strongly motivated by variety of work, creative tasks, recognition for\ntheir work, and activities that allow them to acquire new knowledge, but in\ngeneral the social impact of this activity has low influence on their\nmotivation. Conclusion. This study discusses the difference of opinions among\nsoftware testers, regarding work-related factors that could impact their\nmotivation, which can be relevant for managers and leaders in software\nengineering practice.</p>"
  },
  {
    "title": "Software Architecture Decision-Making Practices and Challenges: An\n  Industrial Case Study",
    "date": "2016-10-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Sandun Dasanayake",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1610.09240v1"
    },
    "publicTags": [],
    "summary": "Software architecture decision-making is critical to the success of a\nsoftware system as software architecture sets the structure of the system,\ndetermines its qualities, and has far-reaching consequences throughout the\nsystem life cycle. The complex nature of the software development context and\nthe importance of the problem has led the research community to develop several\ntechniques, tools, and processes to assist software architects in making better\ndecisions. Despite these effort, the adoption of such systematic approaches\nappears to be quite limited in practice. In addition, the practitioners are\nalso facing new challenges as different software development methods suggest\ndifferent approaches for architecture design. In this paper, we study the\ncurrent software architecture decision-making practices in the industry using a\ncase study conducted among professional software architects in three different\ncompanies in Europe. As a result, we identified different software architecture\ndecision-making practices followed by the software teams as well as their\nreasons for following them, the challenges associated with them, and the\npossible improvements from the software architects' point of view. Based on\nthat, we recognized that improving software architecture knowledge management\ncan address most of the identified challenges and would result in better\nsoftware architecture decision-making.",
    "sourceUrl": "http://arxiv.org/abs/1610.09240v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software architecture decision-making is critical to the success of a\nsoftware system as software architecture sets the structure of the system,\ndetermines its qualities, and has far-reaching consequences throughout the\nsystem life cycle. The complex nature of the software development context and\nthe importance of the problem has led the research community to develop several\ntechniques, tools, and processes to assist software architects in making better\ndecisions. Despite these effort, the adoption of such systematic approaches\nappears to be quite limited in practice. In addition, the practitioners are\nalso facing new challenges as different software development methods suggest\ndifferent approaches for architecture design. In this paper, we study the\ncurrent software architecture decision-making practices in the industry using a\ncase study conducted among professional software architects in three different\ncompanies in Europe. As a result, we identified different software architecture\ndecision-making practices followed by the software teams as well as their\nreasons for following them, the challenges associated with them, and the\npossible improvements from the software architects' point of view. Based on\nthat, we recognized that improving software architecture knowledge management\ncan address most of the identified challenges and would result in better\nsoftware architecture decision-making.</p>"
  },
  {
    "title": "Testing Research Software: An In-Depth Survey of Practices, Methods, and\n  Tools",
    "date": "2025-01-29",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Nasir U. Eisty",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2501.17739v1"
    },
    "publicTags": [],
    "summary": "Context: Research software is essential for developing advanced tools and\nmodels to solve complex research problems and drive innovation across domains.\nTherefore, it is essential to ensure its correctness. Software testing plays a\nvital role in this task. However, testing research software is challenging due\nto the software's complexity and to the unique culture of the research software\ncommunity. Aims: Building on previous research, this study provides an in-depth\ninvestigation of testing practices in research software, focusing on test case\ndesign, challenges with expected outputs, use of quality metrics, execution\nmethods, tools, and desired tool features. Additionally, we explore whether\ndemographic factors influence testing processes. Method: We survey research\nsoftware developers to understand how they design test cases, handle output\nchallenges, use metrics, execute tests, and select tools. Results: Research\nsoftware testing varies widely. The primary challenges are test case design,\nevaluating test quality, and evaluating the correctness of test outputs.\nOverall, research software developers are not familiar with existing testing\ntools and have a need for new tools to support their specific needs.\nConclusion: Allocating human resources to testing and providing developers with\nknowledge about effective testing techniques are important steps toward\nimproving the testing process of research software. While many industrial\ntesting tools exist, they are inadequate for testing research software due to\nits complexity, specialized algorithms, continuous updates, and need for\nflexible, custom testing approaches. Access to a standard set of testing tools\nthat address these special characteristics will increase level of testing in\nresearch software development and reduce the overhead of distributing knowledge\nabout software testing.",
    "sourceUrl": "http://arxiv.org/abs/2501.17739v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Context: Research software is essential for developing advanced tools and\nmodels to solve complex research problems and drive innovation across domains.\nTherefore, it is essential to ensure its correctness. Software testing plays a\nvital role in this task. However, testing research software is challenging due\nto the software's complexity and to the unique culture of the research software\ncommunity. Aims: Building on previous research, this study provides an in-depth\ninvestigation of testing practices in research software, focusing on test case\ndesign, challenges with expected outputs, use of quality metrics, execution\nmethods, tools, and desired tool features. Additionally, we explore whether\ndemographic factors influence testing processes. Method: We survey research\nsoftware developers to understand how they design test cases, handle output\nchallenges, use metrics, execute tests, and select tools. Results: Research\nsoftware testing varies widely. The primary challenges are test case design,\nevaluating test quality, and evaluating the correctness of test outputs.\nOverall, research software developers are not familiar with existing testing\ntools and have a need for new tools to support their specific needs.\nConclusion: Allocating human resources to testing and providing developers with\nknowledge about effective testing techniques are important steps toward\nimproving the testing process of research software. While many industrial\ntesting tools exist, they are inadequate for testing research software due to\nits complexity, specialized algorithms, continuous updates, and need for\nflexible, custom testing approaches. Access to a standard set of testing tools\nthat address these special characteristics will increase level of testing in\nresearch software development and reduce the overhead of distributing knowledge\nabout software testing.</p>"
  },
  {
    "title": "Current practice in software development for computational neuroscience\n  and how to improve it",
    "date": "2012-05-14",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Marc-Oliver Gewaltig",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1205.3025v2"
    },
    "publicTags": [],
    "summary": "Almost all research work in computational neuroscience involves software. As\nresearchers try to understand ever more complex systems, there is a continual\nneed for software with new capabilities. Because of the wide range of questions\nbeing investigated, new software is often developed rapidly by individuals or\nsmall groups. In these cases, it can be hard to demonstrate that the software\ngives the right results. Software developers are often open about the code they\nproduce and willing to share it, but there is little appreciation among\npotential users of the great diversity of software development practices and\nend results, and how this affects the suitability of software tools for use in\nresearch projects. To help clarify these issues, we have reviewed a range of\nsoftware tools and asked how the culture and practice of software development\naffects their validity and trustworthiness. We identified four key questions\nthat can be used to categorize software projects and correlate them with the\ntype of product that results. The first question addresses what is being\nproduced. The other three concern why, how, and by whom the work is done. The\nanswers to these questions show strong correlations with the nature of the\nsoftware being produced, and its suitability for particular purposes. Based on\nour findings, we suggest ways in which current software development practice in\ncomputational neuroscience can be improved and propose checklists to help\ndevelopers, reviewers and scientists to assess the quality whether particular\npieces of software are ready for use in research.",
    "sourceUrl": "http://arxiv.org/abs/1205.3025v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Almost all research work in computational neuroscience involves software. As\nresearchers try to understand ever more complex systems, there is a continual\nneed for software with new capabilities. Because of the wide range of questions\nbeing investigated, new software is often developed rapidly by individuals or\nsmall groups. In these cases, it can be hard to demonstrate that the software\ngives the right results. Software developers are often open about the code they\nproduce and willing to share it, but there is little appreciation among\npotential users of the great diversity of software development practices and\nend results, and how this affects the suitability of software tools for use in\nresearch projects. To help clarify these issues, we have reviewed a range of\nsoftware tools and asked how the culture and practice of software development\naffects their validity and trustworthiness. We identified four key questions\nthat can be used to categorize software projects and correlate them with the\ntype of product that results. The first question addresses what is being\nproduced. The other three concern why, how, and by whom the work is done. The\nanswers to these questions show strong correlations with the nature of the\nsoftware being produced, and its suitability for particular purposes. Based on\nour findings, we suggest ways in which current software development practice in\ncomputational neuroscience can be improved and propose checklists to help\ndevelopers, reviewers and scientists to assess the quality whether particular\npieces of software are ready for use in research.</p>"
  },
  {
    "title": "E-SC4R: Explaining Software Clustering for Remodularisation",
    "date": "2021-07-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Alvin Jian Jia Tan",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2107.01766v2"
    },
    "publicTags": [],
    "summary": "Maintenance of existing software requires a large amount of time for\ncomprehending the source code. The architecture of a software, however, may not\nbe clear to maintainers if up to date documentations are not available.\nSoftware clustering is often used as a remodularisation and architecture\nrecovery technique to help recover a semantic representation of the software\ndesign. Due to the diverse domains, structure, and behaviour of software\nsystems, the suitability of different clustering algorithms for different\nsoftware systems are not investigated thoroughly. Research that introduce new\nclustering techniques usually validate their approaches on a specific domain,\nwhich might limit its generalisability. If the chosen test subjects could only\nrepresent a narrow perspective of the whole picture, researchers might risk not\nbeing able to address the external validity of their findings. This work aims\nto fill this gap by introducing a new approach, Explaining Software Clustering\nfor Remodularisation, to evaluate the effectiveness of different software\nclustering approaches. This work focuses on hierarchical clustering and Bunch\nclustering algorithms and provides information about their suitability\naccording to the features of the software, which as a consequence, enables the\nselection of the most optimum algorithm and configuration from our existing\npool of choices for a particular software system. The proposed framework is\ntested on 30 open source software systems with varying sizes and domains, and\ndemonstrates that it can characterise both the strengths and weaknesses of the\nanalysed software clustering algorithms using software features extracted from\nthe code. The proposed approach also provides a better understanding of the\nalgorithms behaviour through the application of dimensionality reduction\ntechniques.",
    "sourceUrl": "http://arxiv.org/abs/2107.01766v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Maintenance of existing software requires a large amount of time for\ncomprehending the source code. The architecture of a software, however, may not\nbe clear to maintainers if up to date documentations are not available.\nSoftware clustering is often used as a remodularisation and architecture\nrecovery technique to help recover a semantic representation of the software\ndesign. Due to the diverse domains, structure, and behaviour of software\nsystems, the suitability of different clustering algorithms for different\nsoftware systems are not investigated thoroughly. Research that introduce new\nclustering techniques usually validate their approaches on a specific domain,\nwhich might limit its generalisability. If the chosen test subjects could only\nrepresent a narrow perspective of the whole picture, researchers might risk not\nbeing able to address the external validity of their findings. This work aims\nto fill this gap by introducing a new approach, Explaining Software Clustering\nfor Remodularisation, to evaluate the effectiveness of different software\nclustering approaches. This work focuses on hierarchical clustering and Bunch\nclustering algorithms and provides information about their suitability\naccording to the features of the software, which as a consequence, enables the\nselection of the most optimum algorithm and configuration from our existing\npool of choices for a particular software system. The proposed framework is\ntested on 30 open source software systems with varying sizes and domains, and\ndemonstrates that it can characterise both the strengths and weaknesses of the\nanalysed software clustering algorithms using software features extracted from\nthe code. The proposed approach also provides a better understanding of the\nalgorithms behaviour through the application of dimensionality reduction\ntechniques.</p>"
  },
  {
    "title": "Combining Agile with Traditional V Model for Enhancement of Maturity in\n  Software Development",
    "date": "2017-02-01",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ahmed Mateen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1702.00126v1"
    },
    "publicTags": [],
    "summary": "In the field of software engineering there are many new archetypes are\nintroducing day to day Improve the efficiency and effectiveness of software\ndevelopment. Due to dynamic environment organizations are frequently exchanging\ntheir software constraint to meet their objectives. The propose research is a\nnew approach by integrating the traditional V model and agile methodology to\ncombining the strength of these models while minimizing their individual\nweakness.The fluctuating requirements of emerging a carried software system and\naccumulative cost of operational software are imposing researchers and experts\nto determine innovative and superior means for emerging software application at\nslight business or at enterprise level are viewing for. Agile methodology has\nits own benefits but there are deficiency several of the features of\ntraditional software development methodologies that are essential for success.\nThats why an embedded approach will be the right answer for software industry\nrather than a pure agile approach. This research shows how agile embedded\ntraditional can play a vital role in development of software. A survey\nconducted to find the impact of this approach in industry. Both qualitative and\nquantitative analysis performed.",
    "sourceUrl": "http://arxiv.org/abs/1702.00126v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In the field of software engineering there are many new archetypes are\nintroducing day to day Improve the efficiency and effectiveness of software\ndevelopment. Due to dynamic environment organizations are frequently exchanging\ntheir software constraint to meet their objectives. The propose research is a\nnew approach by integrating the traditional V model and agile methodology to\ncombining the strength of these models while minimizing their individual\nweakness.The fluctuating requirements of emerging a carried software system and\naccumulative cost of operational software are imposing researchers and experts\nto determine innovative and superior means for emerging software application at\nslight business or at enterprise level are viewing for. Agile methodology has\nits own benefits but there are deficiency several of the features of\ntraditional software development methodologies that are essential for success.\nThats why an embedded approach will be the right answer for software industry\nrather than a pure agile approach. This research shows how agile embedded\ntraditional can play a vital role in development of software. A survey\nconducted to find the impact of this approach in industry. Both qualitative and\nquantitative analysis performed.</p>"
  },
  {
    "title": "Experience on Re-engineering Applying with Software Product Line",
    "date": "2012-06-19",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Waraporn Jirapanthong",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1206.4120v1"
    },
    "publicTags": [],
    "summary": "In this paper, we present our experience based on a reengineering project.\nThe software project is to re-engineer the original system of a company to\nanswer the new requirements and changed business functions. Reengineering is a\nprocess that involves not only the software system, but also underlying\nbusiness model. Particularly, the new business model is designed along with new\ntechnologies to support the new system. This paper presents our experience that\napplies with software product line approach to develop the new system\nsupporting original business functions and new ones.",
    "sourceUrl": "http://arxiv.org/abs/1206.4120v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In this paper, we present our experience based on a reengineering project.\nThe software project is to re-engineer the original system of a company to\nanswer the new requirements and changed business functions. Reengineering is a\nprocess that involves not only the software system, but also underlying\nbusiness model. Particularly, the new business model is designed along with new\ntechnologies to support the new system. This paper presents our experience that\napplies with software product line approach to develop the new system\nsupporting original business functions and new ones.</p>"
  },
  {
    "title": "Problems in Systematic Application of Software Metrics and Possible\n  Solution",
    "date": "2013-11-15",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Gordana Rakic",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1311.3852v1"
    },
    "publicTags": [],
    "summary": "Systematic application of software metric techniques can lead to significant\nimprovements of the quality of a final software product. However, there is\nstill the evident lack of wider utilization of software metrics techniques and\ntools due to many reasons. In this paper we investigate some limitations of\ncontemporary software metrics tools and then propose construction of a new tool\nthat would solve some of the problems. We describe the promising prototype, its\ninternal structure, and then focus on its independency of the input language.",
    "sourceUrl": "http://arxiv.org/abs/1311.3852v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Systematic application of software metric techniques can lead to significant\nimprovements of the quality of a final software product. However, there is\nstill the evident lack of wider utilization of software metrics techniques and\ntools due to many reasons. In this paper we investigate some limitations of\ncontemporary software metrics tools and then propose construction of a new tool\nthat would solve some of the problems. We describe the promising prototype, its\ninternal structure, and then focus on its independency of the input language.</p>"
  },
  {
    "title": "On Testing Quantum Programs",
    "date": "2018-12-21",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Andriy Miranskyy",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1812.09261v1"
    },
    "publicTags": [],
    "summary": "A quantum computer (QC) can solve many computational problems more\nefficiently than a classic one. The field of QCs is growing: companies (such as\nDWave, IBM, Google, and Microsoft) are building QC offerings. We position that\nsoftware engineers should look into defining a set of software engineering\npractices that apply to QC's software. To start this process, we give examples\nof challenges associated with testing such software and sketch potential\nsolutions to some of these challenges.",
    "sourceUrl": "http://arxiv.org/abs/1812.09261v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>A quantum computer (QC) can solve many computational problems more\nefficiently than a classic one. The field of QCs is growing: companies (such as\nDWave, IBM, Google, and Microsoft) are building QC offerings. We position that\nsoftware engineers should look into defining a set of software engineering\npractices that apply to QC's software. To start this process, we give examples\nof challenges associated with testing such software and sketch potential\nsolutions to some of these challenges.</p>"
  },
  {
    "title": "Software Engineering und Software Engineering Forschung im Zeitalter der\n  Digitalisierung",
    "date": "2020-02-25",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Michael Felderer",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2002.10835v1"
    },
    "publicTags": [],
    "summary": "Digitization not only affects society, it also requires a redefinition of the\nlocation of computer science and computer scientists, as the science journalist\nYogeshwar suggests. Since all official aspects of digitalization are based on\nsoftware, this article is intended to attempt to redefine the role of software\nengineering and its research. Software-based products, systems or services are\ninfluencing all areas of life and are a critical component and central\ninnovation driver of digitization in all areas of life. Scientifically, there\nare new opportunities and challenges for software engineering as a driving\ndiscipline in the development of any technical innovation. However, the chances\nmust not be sacrificed to the competition for bibliometric numbers as an end in\nthemselves.",
    "sourceUrl": "http://arxiv.org/abs/2002.10835v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Digitization not only affects society, it also requires a redefinition of the\nlocation of computer science and computer scientists, as the science journalist\nYogeshwar suggests. Since all official aspects of digitalization are based on\nsoftware, this article is intended to attempt to redefine the role of software\nengineering and its research. Software-based products, systems or services are\ninfluencing all areas of life and are a critical component and central\ninnovation driver of digitization in all areas of life. Scientifically, there\nare new opportunities and challenges for software engineering as a driving\ndiscipline in the development of any technical innovation. However, the chances\nmust not be sacrificed to the competition for bibliometric numbers as an end in\nthemselves.</p>"
  },
  {
    "title": "Software Effort Estimation from Use Case Diagrams Using Nonlinear\n  Regression Analysis",
    "date": "2020-03-22",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ali Bou Nassif",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2003.09873v1"
    },
    "publicTags": [],
    "summary": "Software effort estimation in the early stages of the software life cycle is\none of the most essential and daunting tasks for project managers. In this\nresearch, a new model based on non-linear regression analysis is proposed to\npredict software effort from use case diagrams. It is concluded that, where\nsoftware size is classified from small to very large, one linear or non-linear\nequation for effort estimation cannot be applied. Our model with three\ndifferent non-linear regression equations can incorporate the different ranges\nin software size.",
    "sourceUrl": "http://arxiv.org/abs/2003.09873v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software effort estimation in the early stages of the software life cycle is\none of the most essential and daunting tasks for project managers. In this\nresearch, a new model based on non-linear regression analysis is proposed to\npredict software effort from use case diagrams. It is concluded that, where\nsoftware size is classified from small to very large, one linear or non-linear\nequation for effort estimation cannot be applied. Our model with three\ndifferent non-linear regression equations can incorporate the different ranges\nin software size.</p>"
  },
  {
    "title": "Enabling Reusability in Agile Software Development",
    "date": "2012-10-09",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Sukhpal Singh",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1210.2506v2"
    },
    "publicTags": [],
    "summary": "Software Engineering Discipline is constantly achieving momentum from past\ntwo decades. In last decade, remarkable progress has been observed. New process\nmodels that are introduced from time to time in order to keep pace with\nmultidimensional demands of the industry. New software development paradigms\nare finding its place in industry such as Agile Software Development, Reuse\nbased Development and Component based Development. But different software\ndevelopment models fail to satisfy many needs of software industry. As aim of\nall the process models is same, i.e., to get quality product, reduce time of\ndevelopment, productivity improvement and reduction in cost. Still, no single\nprocess model is complete in itself. Software industry is moving towards Agile\nSoftware Development. Agile development does not obviously fit well for\nbuilding reusable artifacts. However, with careful attention, and important\nmodifications made to agile processes, it may be possible to successfully adapt\nand put on agile methods to development of reusable objects. The model being\nproposed here combines the features of Agile Software Development and\nreusability.",
    "sourceUrl": "http://arxiv.org/abs/1210.2506v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software Engineering Discipline is constantly achieving momentum from past\ntwo decades. In last decade, remarkable progress has been observed. New process\nmodels that are introduced from time to time in order to keep pace with\nmultidimensional demands of the industry. New software development paradigms\nare finding its place in industry such as Agile Software Development, Reuse\nbased Development and Component based Development. But different software\ndevelopment models fail to satisfy many needs of software industry. As aim of\nall the process models is same, i.e., to get quality product, reduce time of\ndevelopment, productivity improvement and reduction in cost. Still, no single\nprocess model is complete in itself. Software industry is moving towards Agile\nSoftware Development. Agile development does not obviously fit well for\nbuilding reusable artifacts. However, with careful attention, and important\nmodifications made to agile processes, it may be possible to successfully adapt\nand put on agile methods to development of reusable objects. The model being\nproposed here combines the features of Agile Software Development and\nreusability.</p>"
  },
  {
    "title": "Probabilistic Software Modeling",
    "date": "2018-06-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Hannes Thaller",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1806.08942v2"
    },
    "publicTags": [],
    "summary": "Software Engineering and the implementation of software has become a\nchallenging task as many tools, frameworks and languages must be orchestrated\ninto one functioning piece. This complexity increases the need for testing and\nanalysis methodologies that aid the developers and engineers as the software\ngrows and evolves. The amount of resources that companies budget for testing\nand analysis is limited, highlighting the importance of automation for economic\nsoftware development. We propose Probabilistic Software Modeling, a new\nparadigm for software modeling that builds on the fact that software is an\neasy-to-monitor environment from which statistical models can be built.\nProbabilistic Software Modeling provides increased comprehension for engineers\nwithout changing the level of abstraction. The approach relies on the recursive\ndecomposition principle of object-oriented programming to build hierarchies of\nprobabilistic models that are fitted via observations collected at runtime of a\nsoftware system. This leads to a network of models that mirror the static\nstructure of the software system while modeling its dynamic runtime behavior.\nThe resulting models can be used in applications such as test-case generation,\nanomaly and outlier detection, probabilistic program simulation, or state\npredictions. Ideally, probabilistic software modeling allows the use of the\nentire spectrum of statistical modeling and inference for software, enabling\nin-depth analysis and generative procedures for software.",
    "sourceUrl": "http://arxiv.org/abs/1806.08942v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software Engineering and the implementation of software has become a\nchallenging task as many tools, frameworks and languages must be orchestrated\ninto one functioning piece. This complexity increases the need for testing and\nanalysis methodologies that aid the developers and engineers as the software\ngrows and evolves. The amount of resources that companies budget for testing\nand analysis is limited, highlighting the importance of automation for economic\nsoftware development. We propose Probabilistic Software Modeling, a new\nparadigm for software modeling that builds on the fact that software is an\neasy-to-monitor environment from which statistical models can be built.\nProbabilistic Software Modeling provides increased comprehension for engineers\nwithout changing the level of abstraction. The approach relies on the recursive\ndecomposition principle of object-oriented programming to build hierarchies of\nprobabilistic models that are fitted via observations collected at runtime of a\nsoftware system. This leads to a network of models that mirror the static\nstructure of the software system while modeling its dynamic runtime behavior.\nThe resulting models can be used in applications such as test-case generation,\nanomaly and outlier detection, probabilistic program simulation, or state\npredictions. Ideally, probabilistic software modeling allows the use of the\nentire spectrum of statistical modeling and inference for software, enabling\nin-depth analysis and generative procedures for software.</p>"
  },
  {
    "title": "Toward Inclusion of Children as Software Engineering Stakeholders",
    "date": "2021-01-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Letizia Jaccheri",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2101.02704v1"
    },
    "publicTags": [],
    "summary": "Background: A growing amount of software is available to children today.\nChildren use both software that has been explicitly developed for them and\nsoftware for general users. While they obtain clear benefits from software,\nsuch as access to creativity tools and learning resources, children are also\nexposed to several risks and disadvantages, such as privacy violation,\ninactivity, or safety risks that can even lead to death. The research and\ndevelopment community is addressing and investigating positive and negative\nimpacts of software for children one by one, but no comprehensive model exists\nthat relates software engineering and children as stakeholders in their own\nright. Aims: The final objective of this line of research is to propose\neffective ways in which children can be involved in Software Engineering\nactivities as stakeholders. Specifically, in this paper, we investigate the\nquality aspects that are of interest for children, as quality is a crucial\naspect in the development of any kind of software, especially for stakeholders\nlike children. Method: Our contribution is based mainly on an analysis of\nstudies at the intersection between Software Engineering (especially software\nquality) and Child Computer Interaction. Results: We identify a set of\nqualities and a preliminary set of guidelines that can be used by researchers\nand practitioners in understanding the complex interrelations between Software\nEngineering and children. Based on the qualities and the guidelines,\nresearchers can design empirical investigations to obtain deeper insights into\nthe phenomenon and propose new Software Engineering knowledge specific for this\ntype of stakeholders. Conclusions: This conceptualization is a first step\ntowards a framework to support children as stakeholders in software\nengineering.",
    "sourceUrl": "http://arxiv.org/abs/2101.02704v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Background: A growing amount of software is available to children today.\nChildren use both software that has been explicitly developed for them and\nsoftware for general users. While they obtain clear benefits from software,\nsuch as access to creativity tools and learning resources, children are also\nexposed to several risks and disadvantages, such as privacy violation,\ninactivity, or safety risks that can even lead to death. The research and\ndevelopment community is addressing and investigating positive and negative\nimpacts of software for children one by one, but no comprehensive model exists\nthat relates software engineering and children as stakeholders in their own\nright. Aims: The final objective of this line of research is to propose\neffective ways in which children can be involved in Software Engineering\nactivities as stakeholders. Specifically, in this paper, we investigate the\nquality aspects that are of interest for children, as quality is a crucial\naspect in the development of any kind of software, especially for stakeholders\nlike children. Method: Our contribution is based mainly on an analysis of\nstudies at the intersection between Software Engineering (especially software\nquality) and Child Computer Interaction. Results: We identify a set of\nqualities and a preliminary set of guidelines that can be used by researchers\nand practitioners in understanding the complex interrelations between Software\nEngineering and children. Based on the qualities and the guidelines,\nresearchers can design empirical investigations to obtain deeper insights into\nthe phenomenon and propose new Software Engineering knowledge specific for this\ntype of stakeholders. Conclusions: This conceptualization is a first step\ntowards a framework to support children as stakeholders in software\nengineering.</p>"
  },
  {
    "title": "A Comprehensive Study on Challenges in Deploying Deep Learning Based\n  Software",
    "date": "2020-05-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Zhenpeng Chen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2005.00760v4"
    },
    "publicTags": [],
    "summary": "Deep learning (DL) becomes increasingly pervasive, being used in a wide range\nof software applications. These software applications, named as DL based\nsoftware (in short as DL software), integrate DL models trained using a large\ndata corpus with DL programs written based on DL frameworks such as TensorFlow\nand Keras. A DL program encodes the network structure of a desirable DL model\nand the process by which the model is trained using the training data. To help\ndevelopers of DL software meet the new challenges posed by DL, enormous\nresearch efforts in software engineering have been devoted. Existing studies\nfocus on the development of DL software and extensively analyze faults in DL\nprograms. However, the deployment of DL software has not been comprehensively\nstudied. To fill this knowledge gap, this paper presents a comprehensive study\non understanding challenges in deploying DL software. We mine and analyze 3,023\nrelevant posts from Stack Overflow, a popular Q&A website for developers, and\nshow the increasing popularity and high difficulty of DL software deployment\namong developers. We build a taxonomy of specific challenges encountered by\ndevelopers in the process of DL software deployment through manual inspection\nof 769 sampled posts and report a series of actionable implications for\nresearchers, developers, and DL framework vendors.",
    "sourceUrl": "http://arxiv.org/abs/2005.00760v4",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Deep learning (DL) becomes increasingly pervasive, being used in a wide range\nof software applications. These software applications, named as DL based\nsoftware (in short as DL software), integrate DL models trained using a large\ndata corpus with DL programs written based on DL frameworks such as TensorFlow\nand Keras. A DL program encodes the network structure of a desirable DL model\nand the process by which the model is trained using the training data. To help\ndevelopers of DL software meet the new challenges posed by DL, enormous\nresearch efforts in software engineering have been devoted. Existing studies\nfocus on the development of DL software and extensively analyze faults in DL\nprograms. However, the deployment of DL software has not been comprehensively\nstudied. To fill this knowledge gap, this paper presents a comprehensive study\non understanding challenges in deploying DL software. We mine and analyze 3,023\nrelevant posts from Stack Overflow, a popular Q&A website for developers, and\nshow the increasing popularity and high difficulty of DL software deployment\namong developers. We build a taxonomy of specific challenges encountered by\ndevelopers in the process of DL software deployment through manual inspection\nof 769 sampled posts and report a series of actionable implications for\nresearchers, developers, and DL framework vendors.</p>"
  },
  {
    "title": "Distributed Software Evolution: a Survey",
    "date": "2022-04-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Mohammad Reza Besharati",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2204.14036v1"
    },
    "publicTags": [],
    "summary": "Distribution can be a feature of the software evolution process. In other\nwords, temporally and spatially distributed teams and organizations can develop\nand work on a software application. The simplest case is to outsource\nproduction and employ workforce at distributed sites so that multiple\ndistributed teams can work on a project within a parallel framework. If this\ndistribution is global, it will be called the global software evolution or\ndevelopment. A higher level of distribution is defined as decentralization and\ndecentralized software evolution, which means that software development can be\nindependent of the initial provider. It also means that software execution is\nindependent of the initial provider and the initial system so that the software\napplication can easily be reused in different and new projects. However, the\nhigh level architecture is managed within a practically centralized framework\nin the decentralized software evolution. Most of the large scale open-source\nprojects are exemplars of this level. In terms of distribution, there is a\nhigher level of decentralized software evolution called \"distributed cognition\nand leadership\". At this level of distribution, all system levels evolve within\na distributed framework, and there are no centralized points in the project\nnetwork and its evolution process. Some open-source software applications are\nthe exemplars of this last level. Not only is the distributed software\nevolution faced with certain challenges and opportunities to reach its goals,\nbut it has also caused some challenges and opportunities in other fields. This\npaper conducts a general review of the distributed software evolution. For this\npurpose, the paper first addresses the importance of the distributed software\nevolution, and then introduces its noteworthy paradigms.",
    "sourceUrl": "http://arxiv.org/abs/2204.14036v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Distribution can be a feature of the software evolution process. In other\nwords, temporally and spatially distributed teams and organizations can develop\nand work on a software application. The simplest case is to outsource\nproduction and employ workforce at distributed sites so that multiple\ndistributed teams can work on a project within a parallel framework. If this\ndistribution is global, it will be called the global software evolution or\ndevelopment. A higher level of distribution is defined as decentralization and\ndecentralized software evolution, which means that software development can be\nindependent of the initial provider. It also means that software execution is\nindependent of the initial provider and the initial system so that the software\napplication can easily be reused in different and new projects. However, the\nhigh level architecture is managed within a practically centralized framework\nin the decentralized software evolution. Most of the large scale open-source\nprojects are exemplars of this level. In terms of distribution, there is a\nhigher level of decentralized software evolution called \"distributed cognition\nand leadership\". At this level of distribution, all system levels evolve within\na distributed framework, and there are no centralized points in the project\nnetwork and its evolution process. Some open-source software applications are\nthe exemplars of this last level. Not only is the distributed software\nevolution faced with certain challenges and opportunities to reach its goals,\nbut it has also caused some challenges and opportunities in other fields. This\npaper conducts a general review of the distributed software evolution. For this\npurpose, the paper first addresses the importance of the distributed software\nevolution, and then introduces its noteworthy paradigms.</p>"
  },
  {
    "title": "Modelling Open-Source Software Reliability Incorporating Swarm\n  Intelligence-Based Techniques",
    "date": "2024-01-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Omar Shatnawi",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2401.02664v1"
    },
    "publicTags": [],
    "summary": "In the software industry, two software engineering development best practices\ncoexist: open-source and closed-source software. The former has a shared code\nthat anyone can contribute, whereas the latter has a proprietary code that only\nthe owner can access. Software reliability is crucial in the industry when a\nnew product or update is released. Applying meta-heuristic optimization\nalgorithms for closed-source software reliability prediction has produced\nsignificant and accurate results. Now, open-source software dominates the\nlandscape of cloud-based systems. Therefore, providing results on open-source\nsoftware reliability - as a quality indicator - would greatly help solve the\nopen-source software reliability growth-modelling problem. The reliability is\npredicted by estimating the parameters of the software reliability models. As\nsoftware reliability models are inherently nonlinear, traditional approaches\nmake estimating the appropriate parameters difficult and ineffective.\nConsequently, software reliability models necessitate a high-quality parameter\nestimation technique. These objectives dictate the exploration of potential\napplications of meta-heuristic swarm intelligence optimization algorithms for\noptimizing the parameter estimation of nonhomogeneous Poisson process-based\nopen-source software reliability modelling. The optimization algorithms are\nfirefly, social spider, artificial bee colony, grey wolf, particle swarm, moth\nflame, and whale. The applicability and performance evaluation of the\noptimization modelling approach is demonstrated through two real open-source\nsoftware reliability datasets. The results are promising.",
    "sourceUrl": "http://arxiv.org/abs/2401.02664v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In the software industry, two software engineering development best practices\ncoexist: open-source and closed-source software. The former has a shared code\nthat anyone can contribute, whereas the latter has a proprietary code that only\nthe owner can access. Software reliability is crucial in the industry when a\nnew product or update is released. Applying meta-heuristic optimization\nalgorithms for closed-source software reliability prediction has produced\nsignificant and accurate results. Now, open-source software dominates the\nlandscape of cloud-based systems. Therefore, providing results on open-source\nsoftware reliability - as a quality indicator - would greatly help solve the\nopen-source software reliability growth-modelling problem. The reliability is\npredicted by estimating the parameters of the software reliability models. As\nsoftware reliability models are inherently nonlinear, traditional approaches\nmake estimating the appropriate parameters difficult and ineffective.\nConsequently, software reliability models necessitate a high-quality parameter\nestimation technique. These objectives dictate the exploration of potential\napplications of meta-heuristic swarm intelligence optimization algorithms for\noptimizing the parameter estimation of nonhomogeneous Poisson process-based\nopen-source software reliability modelling. The optimization algorithms are\nfirefly, social spider, artificial bee colony, grey wolf, particle swarm, moth\nflame, and whale. The applicability and performance evaluation of the\noptimization modelling approach is demonstrated through two real open-source\nsoftware reliability datasets. The results are promising.</p>"
  },
  {
    "title": "How Does API Migration Impact Software Quality and Comprehension? An\n  Empirical Study",
    "date": "2019-07-18",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Hussein Alrubaye",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1907.07724v2"
    },
    "publicTags": [],
    "summary": "The migration process between different third-party software libraries is\nhard, complex and error-prone. Typically, during a library migration process,\ndevelopers opt to replace methods from the retired library with other methods\nfrom a new library without altering the software behavior. However, the extent\nto which such a migration process to new libraries will be rewarded with an\nimproved software quality is still unknown. In this paper, we aim at studying\nand analyzing the impact of library API migration on software quality. We\nconduct a large-scale empirical study on 9 popular API migrations, collected\nfrom a corpus of 57,447 open-source Java projects. We compute the values of\ncommonly-used software quality metrics before and after a migration occurs. The\nstatistical analysis of the obtained results provides evidence that library\nmigrations are likely to improve different software quality attributes\nincluding significantly reduced coupling, increased cohesion, and improved code\nreadability. Furthermore, we release an online portal that helps software\ndevelopers to understand the pre-impact of a library migration on software\nquality and recommend migration examples that adopt the best design and\nimplementation practices to improve software quality. Finally, we provide the\nsoftware engineering community with a large scale dataset to foster research in\nsoftware library migration.",
    "sourceUrl": "http://arxiv.org/abs/1907.07724v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The migration process between different third-party software libraries is\nhard, complex and error-prone. Typically, during a library migration process,\ndevelopers opt to replace methods from the retired library with other methods\nfrom a new library without altering the software behavior. However, the extent\nto which such a migration process to new libraries will be rewarded with an\nimproved software quality is still unknown. In this paper, we aim at studying\nand analyzing the impact of library API migration on software quality. We\nconduct a large-scale empirical study on 9 popular API migrations, collected\nfrom a corpus of 57,447 open-source Java projects. We compute the values of\ncommonly-used software quality metrics before and after a migration occurs. The\nstatistical analysis of the obtained results provides evidence that library\nmigrations are likely to improve different software quality attributes\nincluding significantly reduced coupling, increased cohesion, and improved code\nreadability. Furthermore, we release an online portal that helps software\ndevelopers to understand the pre-impact of a library migration on software\nquality and recommend migration examples that adopt the best design and\nimplementation practices to improve software quality. Finally, we provide the\nsoftware engineering community with a large scale dataset to foster research in\nsoftware library migration.</p>"
  },
  {
    "title": "Designing a Serious Game: Teaching Developers to Embed Privacy into\n  Software Systems",
    "date": "2020-09-12",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Nalin Asanka Gamagedara Arachchilage",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2009.05714v1"
    },
    "publicTags": [],
    "summary": "Software applications continue to challenge user privacy when users interact\nwith them. Privacy practices (e.g. Data Minimisation (DM), Privacy by Design\n(PbD) or General Data Protection Regulation (GDPR)) and related \"privacy\nengineering\" methodologies exist and provide clear instructions for developers\nto implement privacy into software systems they develop that preserve user\nprivacy. However, those practices and methodologies are not yet a common\npractice in the software development community. There has been no previous\nresearch focused on developing \"educational\" interventions such as serious\ngames to enhance software developers' coding behaviour. Therefore, this\nresearch proposes a game design framework as an educational tool for software\ndevelopers to improve (secure) coding behaviour, so they can develop\nprivacy-preserving software applications that people can use. The elements of\nthe proposed framework were incorporated into a gaming application scenario\nthat enhances the software developers' coding behaviour through their\nmotivation. The proposed work not only enables the development of\nprivacy-preserving software systems but also helping the software development\ncommunity to put privacy guidelines and engineering methodologies into\npractice.",
    "sourceUrl": "http://arxiv.org/abs/2009.05714v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software applications continue to challenge user privacy when users interact\nwith them. Privacy practices (e.g. Data Minimisation (DM), Privacy by Design\n(PbD) or General Data Protection Regulation (GDPR)) and related \"privacy\nengineering\" methodologies exist and provide clear instructions for developers\nto implement privacy into software systems they develop that preserve user\nprivacy. However, those practices and methodologies are not yet a common\npractice in the software development community. There has been no previous\nresearch focused on developing \"educational\" interventions such as serious\ngames to enhance software developers' coding behaviour. Therefore, this\nresearch proposes a game design framework as an educational tool for software\ndevelopers to improve (secure) coding behaviour, so they can develop\nprivacy-preserving software applications that people can use. The elements of\nthe proposed framework were incorporated into a gaming application scenario\nthat enhances the software developers' coding behaviour through their\nmotivation. The proposed work not only enables the development of\nprivacy-preserving software systems but also helping the software development\ncommunity to put privacy guidelines and engineering methodologies into\npractice.</p>"
  },
  {
    "title": "SoftCloud: A Tool for Visualizing Software Artifacts as Tag Clouds",
    "date": "2021-09-27",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ra'Fat Al-Msie'deen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2109.12881v1"
    },
    "publicTags": [],
    "summary": "Software artifacts visualization helps software developers to manage the size\nand complexity of the software system. The tag cloud technique visualizes tags\nwithin the cloud according to their frequencies in software artifacts. A font\nsize of the tag within the cloud indicates its frequency within a software\nartifact, while the color of a tag within the cloud uses just for aesthetic\npurposes. This paper suggests a new approach (SoftCloud) to visualize software\nartifacts as a tag cloud. The originality of SoftCloud is visualizing all the\nartifacts available to the software program as a tag cloud. Experiments have\nconducted on different software artifacts to validate SoftCloud and demonstrate\nits strengths. The results showed the ability of SoftCloud to correctly\nretrieve all tags and their frequencies from available software artifacts.",
    "sourceUrl": "http://arxiv.org/abs/2109.12881v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software artifacts visualization helps software developers to manage the size\nand complexity of the software system. The tag cloud technique visualizes tags\nwithin the cloud according to their frequencies in software artifacts. A font\nsize of the tag within the cloud indicates its frequency within a software\nartifact, while the color of a tag within the cloud uses just for aesthetic\npurposes. This paper suggests a new approach (SoftCloud) to visualize software\nartifacts as a tag cloud. The originality of SoftCloud is visualizing all the\nartifacts available to the software program as a tag cloud. Experiments have\nconducted on different software artifacts to validate SoftCloud and demonstrate\nits strengths. The results showed the ability of SoftCloud to correctly\nretrieve all tags and their frequencies from available software artifacts.</p>"
  },
  {
    "title": "Software Must Move! A Description of the Software Assembly Line",
    "date": "2010-06-10",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Martin J. McGowan III",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1006.2155v1"
    },
    "publicTags": [],
    "summary": "This paper describes a set of tools for automating and controlling the\ndevelopment and maintenance of software systems. The mental model is a software\nassembly line. Program design and construction take place at individual\nprogrammer workstations. Integration of individual software components takes\nplace at subsequent stations on the assembly line. Software is moved\nautomatically along the assembly line toward final packaging. Software under\nconstruction or maintenance is divided into packages. Each package of software\nis composed of a recipe and ingredients. Some new terms are introduced to\ndescribe the ingredients. The recipe specifies how ingredients are transformed\ninto products. The benefits of the Software Assembly Line for development,\nmaintenance, and management of large-scale computer systems are explained.",
    "sourceUrl": "http://arxiv.org/abs/1006.2155v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>This paper describes a set of tools for automating and controlling the\ndevelopment and maintenance of software systems. The mental model is a software\nassembly line. Program design and construction take place at individual\nprogrammer workstations. Integration of individual software components takes\nplace at subsequent stations on the assembly line. Software is moved\nautomatically along the assembly line toward final packaging. Software under\nconstruction or maintenance is divided into packages. Each package of software\nis composed of a recipe and ingredients. Some new terms are introduced to\ndescribe the ingredients. The recipe specifies how ingredients are transformed\ninto products. The benefits of the Software Assembly Line for development,\nmaintenance, and management of large-scale computer systems are explained.</p>"
  },
  {
    "title": "Antipatterns in Software Classification Taxonomies",
    "date": "2022-04-19",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Cezar Sas",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2204.08880v1"
    },
    "publicTags": [],
    "summary": "Empirical results in software engineering have long started to show that\nfindings are unlikely to be applicable to all software systems, or any domain:\nresults need to be evaluated in specified contexts, and limited to the type of\nsystems that they were extracted from. This is a known issue, and requires the\nestablishment of a classification of software types.\n  This paper makes two contributions: the first is to evaluate the quality of\nthe current software classifications landscape. The second is to perform a case\nstudy showing how to create a classification of software types using a curated\nset of software systems.\n  Our contributions show that existing, and very likely even new,\nclassification attempts are deemed to fail for one or more issues, that we\nnamed as the `antipatterns' of software classification tasks. We collected 7 of\nthese antipatterns that emerge from both our case study, and the existing\nclassifications.\n  These antipatterns represent recurring issues in a classification, so we\ndiscuss practical ways to help researchers avoid these pitfalls. It becomes\nclear that classification attempts must also face the daunting task of\nformulating a taxonomy of software types, with the objective of establishing a\nhierarchy of categories in a classification.",
    "sourceUrl": "http://arxiv.org/abs/2204.08880v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Empirical results in software engineering have long started to show that\nfindings are unlikely to be applicable to all software systems, or any domain:\nresults need to be evaluated in specified contexts, and limited to the type of\nsystems that they were extracted from. This is a known issue, and requires the\nestablishment of a classification of software types.\n  This paper makes two contributions: the first is to evaluate the quality of\nthe current software classifications landscape. The second is to perform a case\nstudy showing how to create a classification of software types using a curated\nset of software systems.\n  Our contributions show that existing, and very likely even new,\nclassification attempts are deemed to fail for one or more issues, that we\nnamed as the `antipatterns' of software classification tasks. We collected 7 of\nthese antipatterns that emerge from both our case study, and the existing\nclassifications.\n  These antipatterns represent recurring issues in a classification, so we\ndiscuss practical ways to help researchers avoid these pitfalls. It becomes\nclear that classification attempts must also face the daunting task of\nformulating a taxonomy of software types, with the objective of establishing a\nhierarchy of categories in a classification.</p>"
  },
  {
    "title": "6G Software Engineering: A Systematic Mapping Study",
    "date": "2024-05-08",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ruoyu Su",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2405.05017v1"
    },
    "publicTags": [],
    "summary": "6G will revolutionize the software world allowing faster cellular\ncommunications and a massive number of connected devices. 6G will enable a\nshift towards a continuous edge-to-cloud architecture. Current cloud solutions,\nwhere all the data is transferred and computed in the cloud, are not\nsustainable in such a large network of devices. Current technologies, including\ndevelopment methods, software architectures, and orchestration and offloading\nsystems, still need to be prepared to cope with such requirements. In this\npaper, we conduct a Systematic Mapping Study to investigate the current\nresearch status of 6G Software Engineering. Results show that 18 research\npapers have been proposed in software process, software architecture,\norchestration and offloading methods. Of these, software architecture and\nsoftware-defined networks are respectively areas and topics that have received\nthe most attention in 6G Software Engineering. In addition, the main types of\nresults of these papers are methods, architectures, platforms, frameworks and\nalgorithms. For the five tools/frameworks proposed, they are new and not\ncurrently studied by other researchers. The authors of these findings are\nmainly from China, India and Saudi Arabia. The results will enable researchers\nand practitioners to further research and extend for 6G Software Engineering.",
    "sourceUrl": "http://arxiv.org/abs/2405.05017v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>6G will revolutionize the software world allowing faster cellular\ncommunications and a massive number of connected devices. 6G will enable a\nshift towards a continuous edge-to-cloud architecture. Current cloud solutions,\nwhere all the data is transferred and computed in the cloud, are not\nsustainable in such a large network of devices. Current technologies, including\ndevelopment methods, software architectures, and orchestration and offloading\nsystems, still need to be prepared to cope with such requirements. In this\npaper, we conduct a Systematic Mapping Study to investigate the current\nresearch status of 6G Software Engineering. Results show that 18 research\npapers have been proposed in software process, software architecture,\norchestration and offloading methods. Of these, software architecture and\nsoftware-defined networks are respectively areas and topics that have received\nthe most attention in 6G Software Engineering. In addition, the main types of\nresults of these papers are methods, architectures, platforms, frameworks and\nalgorithms. For the five tools/frameworks proposed, they are new and not\ncurrently studied by other researchers. The authors of these findings are\nmainly from China, India and Saudi Arabia. The results will enable researchers\nand practitioners to further research and extend for 6G Software Engineering.</p>"
  },
  {
    "title": "Requirements Engineering for Research Software: A Vision",
    "date": "2024-05-13",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Adrian Bajraktari",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2405.07781v2"
    },
    "publicTags": [],
    "summary": "Modern science is relying on software more than ever. The behavior and\noutcomes of this software shape the scientific and public discourse on\nimportant topics like climate change, economic growth, or the spread of\ninfections. Most researchers creating software for scientific purposes are not\ntrained in Software Engineering. As a consequence, research software is often\ndeveloped ad hoc without following stringent processes. With this paper, we\nwant to characterize research software as a new application domain that needs\nattention from the Requirements Engineering community. We conducted an\nexploratory study based on 8 interviews with 12 researchers who develop\nsoftware. We describe how researchers elicit, document, and analyze\nrequirements for research software and what processes they follow. From this,\nwe derive specific challenges and describe a vision of Requirements Engineering\nfor research software.",
    "sourceUrl": "http://arxiv.org/abs/2405.07781v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Modern science is relying on software more than ever. The behavior and\noutcomes of this software shape the scientific and public discourse on\nimportant topics like climate change, economic growth, or the spread of\ninfections. Most researchers creating software for scientific purposes are not\ntrained in Software Engineering. As a consequence, research software is often\ndeveloped ad hoc without following stringent processes. With this paper, we\nwant to characterize research software as a new application domain that needs\nattention from the Requirements Engineering community. We conducted an\nexploratory study based on 8 interviews with 12 researchers who develop\nsoftware. We describe how researchers elicit, document, and analyze\nrequirements for research software and what processes they follow. From this,\nwe derive specific challenges and describe a vision of Requirements Engineering\nfor research software.</p>"
  },
  {
    "title": "Tackling Erosion in Variant-Rich Software Systems: Challenges and\n  Approaches",
    "date": "2024-07-04",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Johannes StÃ¼mpfle",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2407.03914v1"
    },
    "publicTags": [],
    "summary": "Software product lines (SPL) have emerged as a pivotal paradigm in software\nengineering, enabling the efficient development of variant-rich software\nsystems. Consistently updating these systems, often through over-the-air\nupdates, enables the continuous integration of new features and bug fixes,\nensuring the system remains up to date throughout its entire lifecycle.\nHowever, evolving such complex systems is an error prone task, leading to a\nphenomenon known as erosion. This phenomenon significantly impacts the\nefficiency and longevity of software systems, presenting a formidable challenge\nfor manufacturers of variant-rich software systems, such as in the automotive\ndomain. While existing studies concentrate on the evolutionary planning of\nvariant-rich software systems, there is a noticeable lack of research\naddressing the problem of erosion. In this paper, we conduct an in-depth\nexploration of the erosion phenomena within variant-rich software systems. We\nbegin by highlighting the significance of controlling erosion in extensive\nvariant-rich software systems. Subsequently, we address the current challenges\nregarding tackling erosion, including issues such as the lack of a consensus on\nunderstanding and defining erosion, as well as the early detection and\nelimination. Finally, we outline a first approach aimed at tackling erosion in\nvariant-rich software systems.",
    "sourceUrl": "http://arxiv.org/abs/2407.03914v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software product lines (SPL) have emerged as a pivotal paradigm in software\nengineering, enabling the efficient development of variant-rich software\nsystems. Consistently updating these systems, often through over-the-air\nupdates, enables the continuous integration of new features and bug fixes,\nensuring the system remains up to date throughout its entire lifecycle.\nHowever, evolving such complex systems is an error prone task, leading to a\nphenomenon known as erosion. This phenomenon significantly impacts the\nefficiency and longevity of software systems, presenting a formidable challenge\nfor manufacturers of variant-rich software systems, such as in the automotive\ndomain. While existing studies concentrate on the evolutionary planning of\nvariant-rich software systems, there is a noticeable lack of research\naddressing the problem of erosion. In this paper, we conduct an in-depth\nexploration of the erosion phenomena within variant-rich software systems. We\nbegin by highlighting the significance of controlling erosion in extensive\nvariant-rich software systems. Subsequently, we address the current challenges\nregarding tackling erosion, including issues such as the lack of a consensus on\nunderstanding and defining erosion, as well as the early detection and\nelimination. Finally, we outline a first approach aimed at tackling erosion in\nvariant-rich software systems.</p>"
  },
  {
    "title": "A New P2N Approach to Software Development Under the Clustering",
    "date": "2012-07-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Gang Liao",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1207.5345v1"
    },
    "publicTags": [],
    "summary": "In this computer era of rapid development, software development can be seen\neverywhere, but a lot of softwares are dead in modern development of software.\nJust as The Mythical Man-Month said, it exists a problem in the software\ndevelopment, and the problem is interflow.A lock of interflow can be said great\ncalamity. Clustering is a environment to breed new life. In this thesis, we\nelaborate how P2N can be used to thinking, planning, developing, collaborating,\nreleasing. And the approach that make your team and organization more perfect.",
    "sourceUrl": "http://arxiv.org/abs/1207.5345v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In this computer era of rapid development, software development can be seen\neverywhere, but a lot of softwares are dead in modern development of software.\nJust as The Mythical Man-Month said, it exists a problem in the software\ndevelopment, and the problem is interflow.A lock of interflow can be said great\ncalamity. Clustering is a environment to breed new life. In this thesis, we\nelaborate how P2N can be used to thinking, planning, developing, collaborating,\nreleasing. And the approach that make your team and organization more perfect.</p>"
  },
  {
    "title": "A New Framework for software Library Investment Metrics",
    "date": "2019-06-03",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Anas Shatnawi",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1906.08592v1"
    },
    "publicTags": [],
    "summary": "Software quality is considered as one of the most important challenges in\nsoftware engineering. It has many dimensions which differ from users' point of\nview that depend on their requirements. Therefore, those dimensions lead to\ndifficulty in measuring and defining the software quality properly. Software\nquality measurement is the main core of the software quality. Thus, it is\nnecessary to study and develop the software measurements to meet the better\nquality. The use of libraries increases software quality more than that of\nusing generic programming because these libraries are prepared and tested in\nadvance. In addition, these libraries reduce the effort that is spent in\ndesigning, testing, and maintenance processes. In this research, we presented a\nnew model to calculate the saved effort that results from using libraries\ninstead of generic programming in the coding, testing, and productivity\nprocesses. The proposed model consists of three metrics that are Library\nInvestment Ratio, Library Investment Level, and Program Simplicity. An\nempirical analyzes has been applied into ten projects to compare the results of\nthe model with Reuse Percent. The results show that the model has better\nindication of the improvement of software quality and productivity rather than\nReuse Percent.",
    "sourceUrl": "http://arxiv.org/abs/1906.08592v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software quality is considered as one of the most important challenges in\nsoftware engineering. It has many dimensions which differ from users' point of\nview that depend on their requirements. Therefore, those dimensions lead to\ndifficulty in measuring and defining the software quality properly. Software\nquality measurement is the main core of the software quality. Thus, it is\nnecessary to study and develop the software measurements to meet the better\nquality. The use of libraries increases software quality more than that of\nusing generic programming because these libraries are prepared and tested in\nadvance. In addition, these libraries reduce the effort that is spent in\ndesigning, testing, and maintenance processes. In this research, we presented a\nnew model to calculate the saved effort that results from using libraries\ninstead of generic programming in the coding, testing, and productivity\nprocesses. The proposed model consists of three metrics that are Library\nInvestment Ratio, Library Investment Level, and Program Simplicity. An\nempirical analyzes has been applied into ten projects to compare the results of\nthe model with Reuse Percent. The results show that the model has better\nindication of the improvement of software quality and productivity rather than\nReuse Percent.</p>"
  },
  {
    "title": "Modernizing the ESRF beamline application software architecture with\n  generic Python modules",
    "date": "2002-10-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jorg Klora",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/cond-mat/0210344v1"
    },
    "publicTags": [],
    "summary": "We report on the modernization of the ESRF beamline application software with\nPython modules. The current building blocks used around the SPEC data\nacquisition software together with the new elements are presented.",
    "sourceUrl": "http://arxiv.org/abs/cond-mat/0210344v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>We report on the modernization of the ESRF beamline application software with\nPython modules. The current building blocks used around the SPEC data\nacquisition software together with the new elements are presented.</p>"
  },
  {
    "title": "Low-Modeling of Software Systems",
    "date": "2024-02-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jordi Cabot",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2402.18375v1"
    },
    "publicTags": [],
    "summary": "There is a growing need for better development methods and tools to keep up\nwith the increasing complexity of new software systems. New types of user\ninterfaces, the need for intelligent components, sustainability concerns, ...\nbring new challenges that we need to handle. In the last years, model-driven\nengineering has been key to improving the quality and productivity of software\ndevelopment, but models themselves are becoming increasingly complex to specify\nand manage. In this paper, we present the concept of low-modeling as a solution\nto enhance current model-driven engineering techniques and get them ready for\nthis new generation of software systems.",
    "sourceUrl": "http://arxiv.org/abs/2402.18375v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>There is a growing need for better development methods and tools to keep up\nwith the increasing complexity of new software systems. New types of user\ninterfaces, the need for intelligent components, sustainability concerns, ...\nbring new challenges that we need to handle. In the last years, model-driven\nengineering has been key to improving the quality and productivity of software\ndevelopment, but models themselves are becoming increasingly complex to specify\nand manage. In this paper, we present the concept of low-modeling as a solution\nto enhance current model-driven engineering techniques and get them ready for\nthis new generation of software systems.</p>"
  },
  {
    "title": "Test Case Generation using Mutation Operators and Fault Classification",
    "date": "2010-02-10",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Mrs. R. Jeevarathinam",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1002.2197v1"
    },
    "publicTags": [],
    "summary": "Software testing is the important phase of software development process. But,\nthis phase can be easily missed by software developers because of their limited\ntime to complete the project. Since, software developers finish their software\nnearer to the delivery time; they dont get enough time to test their program by\ncreating effective test cases. . One of the major difficulties in software\ntesting is the generation of test cases that satisfy the given adequacy\ncriterion Moreover, creating manual test cases is a tedious work for software\ndevelopers in the final rush hours. A new approach which generates test cases\ncan help the software developers to create test cases from software\nspecifications in early stage of software development (before coding) and as\nwell as from program execution traces from after software development (after\ncoding). Heuristic techniques can be applied for creating quality test cases.\nMutation testing is a technique for testing software units that has great\npotential for improving the quality of testing, and to assure the high\nreliability of software. In this paper, a mutation testing based test cases\ngeneration technique has been proposed to generate test cases from program\nexecution trace, so that the test cases can be generated after coding. The\npaper details about the mutation testing implementation to generate test cases.\nThe proposed algorithm has been demonstrated for an example.",
    "sourceUrl": "http://arxiv.org/abs/1002.2197v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software testing is the important phase of software development process. But,\nthis phase can be easily missed by software developers because of their limited\ntime to complete the project. Since, software developers finish their software\nnearer to the delivery time; they dont get enough time to test their program by\ncreating effective test cases. . One of the major difficulties in software\ntesting is the generation of test cases that satisfy the given adequacy\ncriterion Moreover, creating manual test cases is a tedious work for software\ndevelopers in the final rush hours. A new approach which generates test cases\ncan help the software developers to create test cases from software\nspecifications in early stage of software development (before coding) and as\nwell as from program execution traces from after software development (after\ncoding). Heuristic techniques can be applied for creating quality test cases.\nMutation testing is a technique for testing software units that has great\npotential for improving the quality of testing, and to assure the high\nreliability of software. In this paper, a mutation testing based test cases\ngeneration technique has been proposed to generate test cases from program\nexecution trace, so that the test cases can be generated after coding. The\npaper details about the mutation testing implementation to generate test cases.\nThe proposed algorithm has been demonstrated for an example.</p>"
  },
  {
    "title": "An analysis of open source software licensing questions in Stack\n  Exchange sites",
    "date": "2021-10-01",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Maria Papoutsoglou",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2110.00361v1"
    },
    "publicTags": [],
    "summary": "Free and open source software is widely used in the creation of software\nsystems, whereas many organisations choose to provide their systems as open\nsource. Open source software carries licenses that determine the conditions\nunder which the original software can be used. Appropriate use of licenses\nrequires relevant expertise by the practitioners, and has an important legal\nangle. Educators and employers need to ensure that developers have the\nnecessary training to understand licensing risks and how they can be addressed.\nAt the same time, it is important to understand which issues practitioners face\nwhen they are using a specific open source license, when they are developing\nnew open source software products or when they are reusing open source\nsoftware. In this work, we examine questions posed about open source software\nlicensing using data from the following Stack Exchange sites: Stack Overflow,\nSoftware Engineering, Open Source and Law. We analyse the indication of\nspecific licenses and topics in the questions, investigate the attention the\nposts receive and trends over time, whether appropriate answers are provided\nand which type of questions are asked. Our results indicate that practitioners\nneed, among other, clarifications about licensing specific software when other\nlicenses are used, and for understanding license content. The results of the\nstudy can be useful for educators and employers, organisations that are\nauthoring open source software licenses and developers for understanding the\nissues faced when using licenses, whereas they are relevant to other software\nengineering research areas, such as software reusability.",
    "sourceUrl": "http://arxiv.org/abs/2110.00361v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Free and open source software is widely used in the creation of software\nsystems, whereas many organisations choose to provide their systems as open\nsource. Open source software carries licenses that determine the conditions\nunder which the original software can be used. Appropriate use of licenses\nrequires relevant expertise by the practitioners, and has an important legal\nangle. Educators and employers need to ensure that developers have the\nnecessary training to understand licensing risks and how they can be addressed.\nAt the same time, it is important to understand which issues practitioners face\nwhen they are using a specific open source license, when they are developing\nnew open source software products or when they are reusing open source\nsoftware. In this work, we examine questions posed about open source software\nlicensing using data from the following Stack Exchange sites: Stack Overflow,\nSoftware Engineering, Open Source and Law. We analyse the indication of\nspecific licenses and topics in the questions, investigate the attention the\nposts receive and trends over time, whether appropriate answers are provided\nand which type of questions are asked. Our results indicate that practitioners\nneed, among other, clarifications about licensing specific software when other\nlicenses are used, and for understanding license content. The results of the\nstudy can be useful for educators and employers, organisations that are\nauthoring open source software licenses and developers for understanding the\nissues faced when using licenses, whereas they are relevant to other software\nengineering research areas, such as software reusability.</p>"
  },
  {
    "title": "Quantum Software Engineering and Potential of Quantum Computing in\n  Software Engineering Research: A Review",
    "date": "2025-02-13",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ashis Kumar Mandal",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2502.08925v1"
    },
    "publicTags": [],
    "summary": "Research in software engineering is essential for improving development\npractices, leading to reliable and secure software. Leveraging the principles\nof quantum physics, quantum computing has emerged as a new computational\nparadigm that offers significant advantages over classical computing. As\nquantum computing progresses rapidly, its potential applications across various\nfields are becoming apparent. In software engineering, many tasks involve\ncomplex computations where quantum computers can greatly speed up the\ndevelopment process, leading to faster and more efficient solutions. With the\ngrowing use of quantum-based applications in different fields, quantum software\nengineering (QSE) has emerged as a discipline focused on designing, developing,\nand optimizing quantum software for diverse applications. This paper aims to\nreview the role of quantum computing in software engineering research and the\nlatest developments in QSE. To our knowledge, this is the first comprehensive\nreview on this topic. We begin by introducing quantum computing, exploring its\nfundamental concepts, and discussing its potential applications in software\nengineering. We also examine various QSE techniques that expedite software\ndevelopment. Finally, we discuss the opportunities and challenges in\nquantum-driven software engineering and QSE. Our study reveals that quantum\nmachine learning (QML) and quantum optimization have substantial potential to\naddress classical software engineering tasks, though this area is still\nlimited. Current QSE tools and techniques lack robustness and maturity,\nindicating a need for more focus. One of the main challenges is that quantum\ncomputing has yet to reach its full potential.",
    "sourceUrl": "http://arxiv.org/abs/2502.08925v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Research in software engineering is essential for improving development\npractices, leading to reliable and secure software. Leveraging the principles\nof quantum physics, quantum computing has emerged as a new computational\nparadigm that offers significant advantages over classical computing. As\nquantum computing progresses rapidly, its potential applications across various\nfields are becoming apparent. In software engineering, many tasks involve\ncomplex computations where quantum computers can greatly speed up the\ndevelopment process, leading to faster and more efficient solutions. With the\ngrowing use of quantum-based applications in different fields, quantum software\nengineering (QSE) has emerged as a discipline focused on designing, developing,\nand optimizing quantum software for diverse applications. This paper aims to\nreview the role of quantum computing in software engineering research and the\nlatest developments in QSE. To our knowledge, this is the first comprehensive\nreview on this topic. We begin by introducing quantum computing, exploring its\nfundamental concepts, and discussing its potential applications in software\nengineering. We also examine various QSE techniques that expedite software\ndevelopment. Finally, we discuss the opportunities and challenges in\nquantum-driven software engineering and QSE. Our study reveals that quantum\nmachine learning (QML) and quantum optimization have substantial potential to\naddress classical software engineering tasks, though this area is still\nlimited. Current QSE tools and techniques lack robustness and maturity,\nindicating a need for more focus. One of the main challenges is that quantum\ncomputing has yet to reach its full potential.</p>"
  },
  {
    "title": "Exploring the extent of similarities in software failures across\n  industries using LLMs",
    "date": "2024-08-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Martin Detloff",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2408.03528v2"
    },
    "publicTags": [],
    "summary": "The rapid evolution of software development necessitates enhanced safety\nmeasures. Extracting information about software failures from companies is\nbecoming increasingly more available through news articles.\n  This research utilizes the Failure Analysis Investigation with LLMs (FAIL)\nmodel to extract industry-specific information. Although the FAIL model's\ndatabase is rich in information, it could benefit from further categorization\nand industry-specific insights to further assist software engineers.\n  In previous work news articles were collected from reputable sources and\ncategorized by incidents inside a database. Prompt engineering and Large\nLanguage Models (LLMs) were then applied to extract relevant information\nregarding the software failure. This research extends these methods by\ncategorizing articles into specific domains and types of software failures. The\nresults are visually represented through graphs.\n  The analysis shows that throughout the database some software failures occur\nsignificantly more often in specific industries. This categorization provides a\nvaluable resource for software engineers and companies to identify and address\ncommon failures.\n  This research highlights the synergy between software engineering and Large\nLanguage Models (LLMs) to automate and enhance the analysis of software\nfailures. By transforming data from the database into an industry specific\nmodel, we provide a valuable resource that can be used to identify common\nvulnerabilities, predict potential risks, and implement proactive measures for\npreventing software failures. Leveraging the power of the current FAIL database\nand data visualization, we aim to provide an avenue for safer and more secure\nsoftware in the future.",
    "sourceUrl": "http://arxiv.org/abs/2408.03528v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The rapid evolution of software development necessitates enhanced safety\nmeasures. Extracting information about software failures from companies is\nbecoming increasingly more available through news articles.\n  This research utilizes the Failure Analysis Investigation with LLMs (FAIL)\nmodel to extract industry-specific information. Although the FAIL model's\ndatabase is rich in information, it could benefit from further categorization\nand industry-specific insights to further assist software engineers.\n  In previous work news articles were collected from reputable sources and\ncategorized by incidents inside a database. Prompt engineering and Large\nLanguage Models (LLMs) were then applied to extract relevant information\nregarding the software failure. This research extends these methods by\ncategorizing articles into specific domains and types of software failures. The\nresults are visually represented through graphs.\n  The analysis shows that throughout the database some software failures occur\nsignificantly more often in specific industries. This categorization provides a\nvaluable resource for software engineers and companies to identify and address\ncommon failures.\n  This research highlights the synergy between software engineering and Large\nLanguage Models (LLMs) to automate and enhance the analysis of software\nfailures. By transforming data from the database into an industry specific\nmodel, we provide a valuable resource that can be used to identify common\nvulnerabilities, predict potential risks, and implement proactive measures for\npreventing software failures. Leveraging the power of the current FAIL database\nand data visualization, we aim to provide an avenue for safer and more secure\nsoftware in the future.</p>"
  },
  {
    "title": "Generative AI and Empirical Software Engineering: A Paradigm Shift",
    "date": "2025-02-12",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Christoph Treude",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2502.08108v1"
    },
    "publicTags": [],
    "summary": "The widespread adoption of generative AI in software engineering marks a\nparadigm shift, offering new opportunities to design and utilize software\nengineering tools while influencing both developers and the artifacts they\ncreate. Traditional empirical methods in software engineering, including\nquantitative, qualitative, and mixed-method approaches, are well established.\nHowever, this paradigm shift introduces novel data types and redefines many\nconcepts in the software engineering process. The roles of developers, users,\nagents, and researchers increasingly overlap, blurring the distinctions between\nthese social and technical actors within the field.\n  This paper examines how integrating AI into software engineering challenges\ntraditional research paradigms. It focuses on the research phenomena that we\ninvestigate, the methods and theories that we employ, the data we analyze, and\nthe threats to validity that emerge in this new context. Through this\nexploration, our goal is to understand how AI adoption disrupts established\nsoftware development practices that creates new opportunities for empirical\nsoftware engineering research.",
    "sourceUrl": "http://arxiv.org/abs/2502.08108v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The widespread adoption of generative AI in software engineering marks a\nparadigm shift, offering new opportunities to design and utilize software\nengineering tools while influencing both developers and the artifacts they\ncreate. Traditional empirical methods in software engineering, including\nquantitative, qualitative, and mixed-method approaches, are well established.\nHowever, this paradigm shift introduces novel data types and redefines many\nconcepts in the software engineering process. The roles of developers, users,\nagents, and researchers increasingly overlap, blurring the distinctions between\nthese social and technical actors within the field.\n  This paper examines how integrating AI into software engineering challenges\ntraditional research paradigms. It focuses on the research phenomena that we\ninvestigate, the methods and theories that we employ, the data we analyze, and\nthe threats to validity that emerge in this new context. Through this\nexploration, our goal is to understand how AI adoption disrupts established\nsoftware development practices that creates new opportunities for empirical\nsoftware engineering research.</p>"
  },
  {
    "title": "Software Sustainability & High Energy Physics",
    "date": "2020-10-10",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Daniel S. Katz",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2010.05102v2"
    },
    "publicTags": [],
    "summary": "New facilities of the 2020s, such as the High Luminosity Large Hadron\nCollider (HL-LHC), will be relevant through at least the 2030s. This means that\ntheir software efforts and those that are used to analyze their data need to\nconsider sustainability to enable their adaptability to new challenges,\nlongevity, and efficiency, over at least this period. This will help ensure\nthat this software will be easier to develop and maintain, that it remains\navailable in the future on new platforms, that it meets new needs, and that it\nis as reusable as possible. This report discusses a virtual half-day workshop\non \"Software Sustainability and High Energy Physics\" that aimed 1) to bring\ntogether experts from HEP as well as those from outside to share their\nexperiences and practices, and 2) to articulate a vision that helps the\nInstitute for Research and Innovation in Software for High Energy Physics\n(IRIS-HEP) to create a work plan to implement elements of software\nsustainability. Software sustainability practices could lead to new\ncollaborations, including elements of HEP software being directly used outside\nthe field, and, as has happened more frequently in recent years, to HEP\ndevelopers contributing to software developed outside the field rather than\nreinventing it. A focus on and skills related to sustainable software will give\nHEP software developers an important skill that is essential to careers in the\nrealm of software, inside or outside HEP. The report closes with\nrecommendations to improve software sustainability in HEP, aimed at the HEP\ncommunity via IRIS-HEP and the HEP Software Foundation (HSF).",
    "sourceUrl": "http://arxiv.org/abs/2010.05102v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>New facilities of the 2020s, such as the High Luminosity Large Hadron\nCollider (HL-LHC), will be relevant through at least the 2030s. This means that\ntheir software efforts and those that are used to analyze their data need to\nconsider sustainability to enable their adaptability to new challenges,\nlongevity, and efficiency, over at least this period. This will help ensure\nthat this software will be easier to develop and maintain, that it remains\navailable in the future on new platforms, that it meets new needs, and that it\nis as reusable as possible. This report discusses a virtual half-day workshop\non \"Software Sustainability and High Energy Physics\" that aimed 1) to bring\ntogether experts from HEP as well as those from outside to share their\nexperiences and practices, and 2) to articulate a vision that helps the\nInstitute for Research and Innovation in Software for High Energy Physics\n(IRIS-HEP) to create a work plan to implement elements of software\nsustainability. Software sustainability practices could lead to new\ncollaborations, including elements of HEP software being directly used outside\nthe field, and, as has happened more frequently in recent years, to HEP\ndevelopers contributing to software developed outside the field rather than\nreinventing it. A focus on and skills related to sustainable software will give\nHEP software developers an important skill that is essential to careers in the\nrealm of software, inside or outside HEP. The report closes with\nrecommendations to improve software sustainability in HEP, aimed at the HEP\ncommunity via IRIS-HEP and the HEP Software Foundation (HSF).</p>"
  },
  {
    "title": "DolNet: A Division Of Labour Based Distributed Object Oriented Software\n  Process Model",
    "date": "2012-09-20",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Sachin Lakra",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1209.4635v1"
    },
    "publicTags": [],
    "summary": "Distributed Software Development today is in its childhood and not too\nwidespread as a method of developing software in the global IT Industry. In\nthis context, Petrinets are a mathematical model for describing distributed\nsystems theoretically, whereas AttNets are one of their offshoots. But\ndevelopment of true distributed software is limited to network operating\nsystems majorly. Software that runs on many machines with separate programs for\neach machine, are very few. This paper introduces and defines Distributed\nObject Oriented Software Engineering DOOSE as a new field in software\nengineering. The paper further gives a Distributed Object Oriented Software\nProcess Model DOOSPM, called the DolNet, which describes how work may be done\nby a software development organization while working on Distributed Object\nOriented DOO Projects.",
    "sourceUrl": "http://arxiv.org/abs/1209.4635v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Distributed Software Development today is in its childhood and not too\nwidespread as a method of developing software in the global IT Industry. In\nthis context, Petrinets are a mathematical model for describing distributed\nsystems theoretically, whereas AttNets are one of their offshoots. But\ndevelopment of true distributed software is limited to network operating\nsystems majorly. Software that runs on many machines with separate programs for\neach machine, are very few. This paper introduces and defines Distributed\nObject Oriented Software Engineering DOOSE as a new field in software\nengineering. The paper further gives a Distributed Object Oriented Software\nProcess Model DOOSPM, called the DolNet, which describes how work may be done\nby a software development organization while working on Distributed Object\nOriented DOO Projects.</p>"
  },
  {
    "title": "Metrics for Evolution of Aspect Oriented Software",
    "date": "2020-10-12",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Senthil Velan S",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2010.05479v2"
    },
    "publicTags": [],
    "summary": "Aspect Oriented Software Development (AOSD) is a promising methodology which\nprovides powerful techniques to improve the modularity of the software by\nseparating the cross-cutting concerns from the core functionality. Since\nevolution is a major requirement for the sustainability of any software, it is\nnecessary to quantitatively measure its impact. In order to quantify, it is\nessential to define metrics that will capture the evolution of Aspect Oriented\n(AO) software. It is also necessary to compare the metric values of various\nversions of software to draw inferences on the evolution dynamics of AO\nsoftware. This needs identification of artifacts that were added, deleted or\nmodified across versions and study the consequence of these types of changes.\nThis paper defines a new set of metrics for measuring the evolution of Aspect\nOriented software. As a case study, an aspect refactored software, AJHotDraw\nhas been chosen and its four versions have been analyzed for their capability\nto evolve over time.",
    "sourceUrl": "http://arxiv.org/abs/2010.05479v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Aspect Oriented Software Development (AOSD) is a promising methodology which\nprovides powerful techniques to improve the modularity of the software by\nseparating the cross-cutting concerns from the core functionality. Since\nevolution is a major requirement for the sustainability of any software, it is\nnecessary to quantitatively measure its impact. In order to quantify, it is\nessential to define metrics that will capture the evolution of Aspect Oriented\n(AO) software. It is also necessary to compare the metric values of various\nversions of software to draw inferences on the evolution dynamics of AO\nsoftware. This needs identification of artifacts that were added, deleted or\nmodified across versions and study the consequence of these types of changes.\nThis paper defines a new set of metrics for measuring the evolution of Aspect\nOriented software. As a case study, an aspect refactored software, AJHotDraw\nhas been chosen and its four versions have been analyzed for their capability\nto evolve over time.</p>"
  },
  {
    "title": "Enhancing the OPEN Process Framework with Service-Oriented Method\n  Fragments",
    "date": "2020-04-17",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Mahdi Fahmideh",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2004.10136v1"
    },
    "publicTags": [],
    "summary": "Service-orientation is a promising paradigm that enables the engineering of\nlarge-scale distributed software systems using rigorous software development\nprocesses. The existing problem is that every service-oriented software\ndevelopment project often requires a customized development process that\nprovides specific service-oriented software engineering tasks in support of\nrequirements unique to that project. To resolve this problem and allow\nsituational method engineering, we have defined a set of method fragments in\nsupport of the engineering of the project-specific service-oriented software\ndevelopment processes. We have derived the proposed method fragments from the\nrecurring features of eleven prominent service-oriented software development\nmethodologies using a systematic mining approach. We have added these new\nfragments to the repository of OPEN Process Framework to make them available to\nsoftware engineers as reusable fragments using this well-known method\nrepository.\n  Keyword. Service-Oriented Software Development, OPEN Process Framework, OPF\nRepository, Method Fragment, Situational Method Engineering",
    "sourceUrl": "http://arxiv.org/abs/2004.10136v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Service-orientation is a promising paradigm that enables the engineering of\nlarge-scale distributed software systems using rigorous software development\nprocesses. The existing problem is that every service-oriented software\ndevelopment project often requires a customized development process that\nprovides specific service-oriented software engineering tasks in support of\nrequirements unique to that project. To resolve this problem and allow\nsituational method engineering, we have defined a set of method fragments in\nsupport of the engineering of the project-specific service-oriented software\ndevelopment processes. We have derived the proposed method fragments from the\nrecurring features of eleven prominent service-oriented software development\nmethodologies using a systematic mining approach. We have added these new\nfragments to the repository of OPEN Process Framework to make them available to\nsoftware engineers as reusable fragments using this well-known method\nrepository.\n  Keyword. Service-Oriented Software Development, OPEN Process Framework, OPF\nRepository, Method Fragment, Situational Method Engineering</p>"
  },
  {
    "title": "Research Software Publication Policy Case Study",
    "date": "2022-06-10",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Nic Weber",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2206.05367v1"
    },
    "publicTags": [],
    "summary": "Research software is increasingly recognized as a vital component of the\nscholarly record. Journals offer authors the opportunity to publish research\nsoftware papers, but often have different requirements for how these\npublications should be structured and how code should be verified. In this\nshort case study we gather data from 20 Physical Science journals to trace the\nfrequency, quality control, and publishing criteria for software papers. Our\ngoal with the case study is to provide a proof-of-concept for doing descriptive\nempirical work with software publication policies across numerous domains of\nscience and engineering. In the narrative we therefore provide descriptive\nstatistics showing how these journals differ in criteria required for\narchiving, linking, verifying, and documenting software as part of a formal\npublication. The contribution of this preliminary work is twofold: 1. We\nprovide case study of Physical Science research software publications over\ntime; 2. We demonstrate the use of a new survey method for analyzing research\nsoftware publication policies. In our conclusion, we describe how comparative\nresearch into software publication policies can provide better criteria and\nrequirements for an emerging software publication landscape.",
    "sourceUrl": "http://arxiv.org/abs/2206.05367v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Research software is increasingly recognized as a vital component of the\nscholarly record. Journals offer authors the opportunity to publish research\nsoftware papers, but often have different requirements for how these\npublications should be structured and how code should be verified. In this\nshort case study we gather data from 20 Physical Science journals to trace the\nfrequency, quality control, and publishing criteria for software papers. Our\ngoal with the case study is to provide a proof-of-concept for doing descriptive\nempirical work with software publication policies across numerous domains of\nscience and engineering. In the narrative we therefore provide descriptive\nstatistics showing how these journals differ in criteria required for\narchiving, linking, verifying, and documenting software as part of a formal\npublication. The contribution of this preliminary work is twofold: 1. We\nprovide case study of Physical Science research software publications over\ntime; 2. We demonstrate the use of a new survey method for analyzing research\nsoftware publication policies. In our conclusion, we describe how comparative\nresearch into software publication policies can provide better criteria and\nrequirements for an emerging software publication landscape.</p>"
  },
  {
    "title": "Agile Practices for Quantum Software Development: Practitioners\n  Perspectives",
    "date": "2022-10-18",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Arif Ali Khan",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2210.09825v1"
    },
    "publicTags": [],
    "summary": "Quantum software systems are emerging software engineering (SE) genre that\nexploit principles of quantum bits (Qubit) and quantum gates (Qgates) to solve\ncomplex computing problems that today classic computers can not effectively do\nin a reasonable time. According to its proponents, agile software development\npractices have the potential to address many of the problems endemic to the\ndevelopment of quantum software. However, there is a dearth of evidence\nconfirming if agile practices suit and can be adopted by software teams as they\nare in the context of quantum software development. To address this lack, we\nconducted an empirical study to investigate the needs and challenges of using\nagile practices to develop quantum software. While our semi-structured\ninterviews with 26 practitioners across 10 countries highlighted the\napplicability of agile practices in this domain, the interview findings also\nrevealed new challenges impeding the effective incorporation of these\npractices. Our research findings provide a springboard for further\ncontextualization and seamless integration of agile practices with developing\nthe next generation of quantum software.",
    "sourceUrl": "http://arxiv.org/abs/2210.09825v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Quantum software systems are emerging software engineering (SE) genre that\nexploit principles of quantum bits (Qubit) and quantum gates (Qgates) to solve\ncomplex computing problems that today classic computers can not effectively do\nin a reasonable time. According to its proponents, agile software development\npractices have the potential to address many of the problems endemic to the\ndevelopment of quantum software. However, there is a dearth of evidence\nconfirming if agile practices suit and can be adopted by software teams as they\nare in the context of quantum software development. To address this lack, we\nconducted an empirical study to investigate the needs and challenges of using\nagile practices to develop quantum software. While our semi-structured\ninterviews with 26 practitioners across 10 countries highlighted the\napplicability of agile practices in this domain, the interview findings also\nrevealed new challenges impeding the effective incorporation of these\npractices. Our research findings provide a springboard for further\ncontextualization and seamless integration of agile practices with developing\nthe next generation of quantum software.</p>"
  },
  {
    "title": "Software Reuse in the Generative AI Era: From Cargo Cult Towards AI\n  Native Software Engineering",
    "date": "2025-06-22",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Tommi Mikkonen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2506.17937v1"
    },
    "publicTags": [],
    "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Consequently, earlier software reuse practices and methods\nare rapidly being replaced by AI-assisted approaches in which developers place\ntheir trust on code that has been generated by artificial intelligence. This is\nleading to a new form of software reuse that is conceptually not all that\ndifferent from cargo cult development. In this paper we discuss the\nimplications of AI-assisted generative software reuse in the context of\nemerging \"AI native\" software engineering, bring forth relevant questions, and\ndefine a tentative research agenda and call to action for tackling some of the\ncentral issues associated with this approach.",
    "sourceUrl": "http://arxiv.org/abs/2506.17937v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Consequently, earlier software reuse practices and methods\nare rapidly being replaced by AI-assisted approaches in which developers place\ntheir trust on code that has been generated by artificial intelligence. This is\nleading to a new form of software reuse that is conceptually not all that\ndifferent from cargo cult development. In this paper we discuss the\nimplications of AI-assisted generative software reuse in the context of\nemerging \"AI native\" software engineering, bring forth relevant questions, and\ndefine a tentative research agenda and call to action for tackling some of the\ncentral issues associated with this approach.</p>"
  },
  {
    "title": "Software Components for Web Services",
    "date": "2010-01-21",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Muthu Ramachandran",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1001.3734v1"
    },
    "publicTags": [],
    "summary": "Service-oriented computing has emerged as the new area to address software as\na service. This paper proposes a model for component based development for\nservice-oriented systems and have created best practice guidelines on software\ncomponent design.",
    "sourceUrl": "http://arxiv.org/abs/1001.3734v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Service-oriented computing has emerged as the new area to address software as\na service. This paper proposes a model for component based development for\nservice-oriented systems and have created best practice guidelines on software\ncomponent design.</p>"
  },
  {
    "title": "DAQ meta-software for HEP experimental setups",
    "date": "2020-05-14",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "S. Ryzhikov",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2005.06710v1"
    },
    "publicTags": [],
    "summary": "Meta-software for data acquisition (DAQ) is a new approach to design the DAQ\nsystems for experimental setups in experiments in high energy physics (HEP). It\nabstracts from experiment-specific data processing logic, but reflects it\nthrough configuration. It is also intended to substitute highly integrated DAQ\nsoftware for a swarm of single-functional components, orchestrated by universal\nmeta-software.",
    "sourceUrl": "http://arxiv.org/abs/2005.06710v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Meta-software for data acquisition (DAQ) is a new approach to design the DAQ\nsystems for experimental setups in experiments in high energy physics (HEP). It\nabstracts from experiment-specific data processing logic, but reflects it\nthrough configuration. It is also intended to substitute highly integrated DAQ\nsoftware for a swarm of single-functional components, orchestrated by universal\nmeta-software.</p>"
  },
  {
    "title": "Experiments in Sustainable Software Practices for Future Architectures",
    "date": "2013-09-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Charles R. Ferenbaugh",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1309.1428v1"
    },
    "publicTags": [],
    "summary": "In the process of rewriting large physics codes at Los Alamos National\nLaboratory to perform well on new architectures such as many-core, GPU, and\nIntel MIC, we have found a number of areas in which sustainable software\npractices can provide significant advantages. We describe several specific\nadvantages of sustainable practices for future architectures, and report on two\nsmall experimental projects at LANL intended to raise awareness of new software\npractices and programming approaches for new architectures.",
    "sourceUrl": "http://arxiv.org/abs/1309.1428v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In the process of rewriting large physics codes at Los Alamos National\nLaboratory to perform well on new architectures such as many-core, GPU, and\nIntel MIC, we have found a number of areas in which sustainable software\npractices can provide significant advantages. We describe several specific\nadvantages of sustainable practices for future architectures, and report on two\nsmall experimental projects at LANL intended to raise awareness of new software\npractices and programming approaches for new architectures.</p>"
  },
  {
    "title": "Building Reusable Software Component For Optimization Check in ABAP\n  Coding",
    "date": "2010-07-29",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "P. Shireesha",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1007.5123v1"
    },
    "publicTags": [],
    "summary": "Software component reuse is the software engineering practice of developing\nnew software products from existing components. A reuse library or component\nreuse repository organizes stores and manages reusable components. This paper\ndescribes how a reusable component is created, how it reuses the function and\nchecking if optimized code is being used in building programs and applications.\nFinally providing coding guidelines, standards and best practices used for\ncreating reusable components and guidelines and best practices for making\nconfigurable and easy to use.",
    "sourceUrl": "http://arxiv.org/abs/1007.5123v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software component reuse is the software engineering practice of developing\nnew software products from existing components. A reuse library or component\nreuse repository organizes stores and manages reusable components. This paper\ndescribes how a reusable component is created, how it reuses the function and\nchecking if optimized code is being used in building programs and applications.\nFinally providing coding guidelines, standards and best practices used for\ncreating reusable components and guidelines and best practices for making\nconfigurable and easy to use.</p>"
  },
  {
    "title": "CBSE CASE Environment",
    "date": "2015-08-25",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Luiz Fernando Capretz",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1508.06208v1"
    },
    "publicTags": [],
    "summary": "With the need to produce ever larger and more complex software systems, the\nuse of reusable components has become increasingly imperative. Of the many\nexisting and proposed techniques for software development, it seems clear that\ncomponents-based software engineering (CBSE) will be at the forefront of new\napproaches to the production of software systems, and holds the promise of\nsubstantially enhancing the software development and maintenance process. The\nrequired features of a CASE environment suitable for component reuse will be\nput forward in this paper.",
    "sourceUrl": "http://arxiv.org/abs/1508.06208v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>With the need to produce ever larger and more complex software systems, the\nuse of reusable components has become increasingly imperative. Of the many\nexisting and proposed techniques for software development, it seems clear that\ncomponents-based software engineering (CBSE) will be at the forefront of new\napproaches to the production of software systems, and holds the promise of\nsubstantially enhancing the software development and maintenance process. The\nrequired features of a CASE environment suitable for component reuse will be\nput forward in this paper.</p>"
  },
  {
    "title": "Software Autotuning for Sustainable Performance Portability",
    "date": "2013-09-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Azamat Mametjanov",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1309.1894v1"
    },
    "publicTags": [],
    "summary": "Scientific software applications are increasingly developed by large\ninterdiscplinary teams operating on functional modules organized around a\ncommon software framework, which is capable of integrating new functional\ncapabilities without modifying the core of the framework. In such environment,\nsoftware correctness and modularity take precedence at the expense of code\nperformance, which is an important concern during execution on supercomputing\nfacilities, where the allocation of core-hours is a valuable resource. To\nalleviate the performance problems, we propose automated performance tuning\n(autotuning) of software to extract the maximum performance on a given hardware\nplatform and to enable performance portability across heterogeneous hardware\nplatforms. The resulting code remains generic without committing to a\nparticular software stack and yet is compile-time specializable for maximal\nsustained performance.",
    "sourceUrl": "http://arxiv.org/abs/1309.1894v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Scientific software applications are increasingly developed by large\ninterdiscplinary teams operating on functional modules organized around a\ncommon software framework, which is capable of integrating new functional\ncapabilities without modifying the core of the framework. In such environment,\nsoftware correctness and modularity take precedence at the expense of code\nperformance, which is an important concern during execution on supercomputing\nfacilities, where the allocation of core-hours is a valuable resource. To\nalleviate the performance problems, we propose automated performance tuning\n(autotuning) of software to extract the maximum performance on a given hardware\nplatform and to enable performance portability across heterogeneous hardware\nplatforms. The resulting code remains generic without committing to a\nparticular software stack and yet is compile-time specializable for maximal\nsustained performance.</p>"
  },
  {
    "title": "FQL: An Extensible Feature Query Language and Toolkit on Searching\n  Software Characteristics for HPC Applications",
    "date": "2019-05-22",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Weijian Zheng",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1905.09364v1"
    },
    "publicTags": [],
    "summary": "The amount of large-scale scientific computing software is dramatically\nincreasing. In this work, we designed a new language, named feature query\nlanguage (FQL), to collect and extract software features from a quick static\ncode analysis. We designed and implemented an FQL toolkit to automatically\ndetect and present the software features using an extensible query repository.\nSeveral large-scale, high performance computing (HPC) scientific codes have\nbeen used in the paper to demonstrate the HPC-related feature extraction and\ninformation collection. Although we emphasized the HPC features in the study,\nthe toolkit can be easily extended to answer general software feature\nquestions, such as coding pattern and hardware dependency.",
    "sourceUrl": "http://arxiv.org/abs/1905.09364v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The amount of large-scale scientific computing software is dramatically\nincreasing. In this work, we designed a new language, named feature query\nlanguage (FQL), to collect and extract software features from a quick static\ncode analysis. We designed and implemented an FQL toolkit to automatically\ndetect and present the software features using an extensible query repository.\nSeveral large-scale, high performance computing (HPC) scientific codes have\nbeen used in the paper to demonstrate the HPC-related feature extraction and\ninformation collection. Although we emphasized the HPC features in the study,\nthe toolkit can be easily extended to answer general software feature\nquestions, such as coding pattern and hardware dependency.</p>"
  },
  {
    "title": "Technical Debt in Data-Intensive Software Systems",
    "date": "2019-05-31",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Harald Foidl",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1905.13455v1"
    },
    "publicTags": [],
    "summary": "The ever-increasing amount, variety as well as generation and processing\nspeed of today's data pose a variety of new challenges for developing\nData-Intensive Software Systems (DISS). As with developing other kinds of\nsoftware systems, developing DISS is often done under severe pressure and\nstrict schedules. Thus, developers of DISS often have to make technical\ncompromises to meet business concerns. This position paper proposes a\nconceptual model that outlines where Technical Debt (TD) can emerge and\nproliferate within such data-centric systems by separating a DISS into three\nparts (Software Systems, Data Storage Systems and Data). Further, the paper\nillustrates the proliferation of Database Schema Smells as TD items within a\nrelational database-centric software system based on two examples.",
    "sourceUrl": "http://arxiv.org/abs/1905.13455v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The ever-increasing amount, variety as well as generation and processing\nspeed of today's data pose a variety of new challenges for developing\nData-Intensive Software Systems (DISS). As with developing other kinds of\nsoftware systems, developing DISS is often done under severe pressure and\nstrict schedules. Thus, developers of DISS often have to make technical\ncompromises to meet business concerns. This position paper proposes a\nconceptual model that outlines where Technical Debt (TD) can emerge and\nproliferate within such data-centric systems by separating a DISS into three\nparts (Software Systems, Data Storage Systems and Data). Further, the paper\nillustrates the proliferation of Database Schema Smells as TD items within a\nrelational database-centric software system based on two examples.</p>"
  },
  {
    "title": "Systematic Mapping Protocol: Variability Management in Dynamic Software\n  Product Lines for Self-Adaptive Systems",
    "date": "2022-05-17",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Oscar Aguayo",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2205.08487v2"
    },
    "publicTags": [],
    "summary": "Context: The Importance of Dynamic Variability Management in Dynamic Software\nProduct Lines. Objective: Define a protocol for conducting a systematic mapping\nstudy to summarize and synthesize evidence on dynamic variability management\nfor Dynamic Software Product Lines in self-adaptive systems. Method:\nApplication the protocol to conduct a systematic mapping study according the\nguidelines of K. Petersen. Results: A validated protocol to conduct a\nsystematic mapping study. Conclusions: First findings show that it is necessary\nto visualize new ways to manage variability in dynamic software product lines.",
    "sourceUrl": "http://arxiv.org/abs/2205.08487v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Context: The Importance of Dynamic Variability Management in Dynamic Software\nProduct Lines. Objective: Define a protocol for conducting a systematic mapping\nstudy to summarize and synthesize evidence on dynamic variability management\nfor Dynamic Software Product Lines in self-adaptive systems. Method:\nApplication the protocol to conduct a systematic mapping study according the\nguidelines of K. Petersen. Results: A validated protocol to conduct a\nsystematic mapping study. Conclusions: First findings show that it is necessary\nto visualize new ways to manage variability in dynamic software product lines.</p>"
  },
  {
    "title": "Architectural Approaches to Overcome Challenges in the Development of\n  Data-Intensive Systems",
    "date": "2023-12-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Aleksandar Dimov",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2312.03049v1"
    },
    "publicTags": [],
    "summary": "Orientation of modern software systems towards data-intensive processing\nraises new difficulties in software engineering on how to build and maintain\nsuch systems. Some of the important challenges concern the design of software\narchitecture. In this article, we survey the fundamental challenges when\ndesigning data-intensive computing systems and present some of the most popular\nsoftware architectural styles together with their potential to tackle these\nchallenges.",
    "sourceUrl": "http://arxiv.org/abs/2312.03049v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Orientation of modern software systems towards data-intensive processing\nraises new difficulties in software engineering on how to build and maintain\nsuch systems. Some of the important challenges concern the design of software\narchitecture. In this article, we survey the fundamental challenges when\ndesigning data-intensive computing systems and present some of the most popular\nsoftware architectural styles together with their potential to tackle these\nchallenges.</p>"
  },
  {
    "title": "Stability prediction of the software requirements specification",
    "date": "2024-01-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "J. del Sagrado",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2401.12636v1"
    },
    "publicTags": [],
    "summary": "Complex decision-making is a prominent aspect of Requirements Engineering.\nThis work presents the Bayesian network Requisites that predicts whether the\nrequirements specification documents have to be revised. We show how to\nvalidate Requisites by means of metrics obtained from a large complex software\nproject. Besides, this Bayesian network has been integrated into a software\ntool by defining a communication interface inside a multilayer architecture to\nadd this a new decision making functionality. It provides requirements\nengineers a way of exploring the software requirement specification by\ncombining requirement metrics and the probability values estimated by the\nBayesian network.",
    "sourceUrl": "http://arxiv.org/abs/2401.12636v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Complex decision-making is a prominent aspect of Requirements Engineering.\nThis work presents the Bayesian network Requisites that predicts whether the\nrequirements specification documents have to be revised. We show how to\nvalidate Requisites by means of metrics obtained from a large complex software\nproject. Besides, this Bayesian network has been integrated into a software\ntool by defining a communication interface inside a multilayer architecture to\nadd this a new decision making functionality. It provides requirements\nengineers a way of exploring the software requirement specification by\ncombining requirement metrics and the probability values estimated by the\nBayesian network.</p>"
  },
  {
    "title": "With Great Power Comes Great Responsibility: The Role of Software\n  Engineers",
    "date": "2024-07-11",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Stefanie Betz",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2407.08823v1"
    },
    "publicTags": [],
    "summary": "The landscape of software engineering is evolving rapidly amidst the digital\ntransformation and the ascendancy of AI, leading to profound shifts in the role\nand responsibilities of software engineers. This evolution encompasses both\nimmediate changes, such as the adoption of Language Model-based approaches in\ncoding, and deeper shifts driven by the profound societal and environmental\nimpacts of technology. Despite the urgency, there persists a lag in adapting to\nthese evolving roles. By fostering ongoing discourse and reflection on Software\nEngineers role and responsibilities, this vision paper seeks to cultivate a new\ngeneration of software engineers equipped to navigate the complexities and\nethical considerations inherent in their evolving profession.",
    "sourceUrl": "http://arxiv.org/abs/2407.08823v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The landscape of software engineering is evolving rapidly amidst the digital\ntransformation and the ascendancy of AI, leading to profound shifts in the role\nand responsibilities of software engineers. This evolution encompasses both\nimmediate changes, such as the adoption of Language Model-based approaches in\ncoding, and deeper shifts driven by the profound societal and environmental\nimpacts of technology. Despite the urgency, there persists a lag in adapting to\nthese evolving roles. By fostering ongoing discourse and reflection on Software\nEngineers role and responsibilities, this vision paper seeks to cultivate a new\ngeneration of software engineers equipped to navigate the complexities and\nethical considerations inherent in their evolving profession.</p>"
  },
  {
    "title": "Software Reuse in Medical Database for Cardiac Patients using Pearson\n  Family Equations",
    "date": "2012-12-03",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "M. Bhanu Sridhar",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1212.0312v1"
    },
    "publicTags": [],
    "summary": "Software reuse is a subfield of software engineering that is used to adopt\nthe existing software for similar purposes. Reuse Metrics determine the extent\nto which an existing software component is reused in new software with an\nobjective to minimize the errors and cost of the new project. In this paper,\nmedical database related to cardiology is considered. The Pearson Type I\nDistribution is used to calculate the probability density function (pdf) and\nthereby utilizing it for clustering the data. Further, coupling methodology is\nused to bring out the similarity of the new patient data by comparing it with\nthe existing data. By this, the concerned treatment to be followed for the new\npatient is deduced by comparing with that of the previous patients case\nhistory. The metrics proposed by Chidamber and Kemerer are utilized for this\npurpose. This model will be useful for the medical field through software,\nparticularly in remote areas.",
    "sourceUrl": "http://arxiv.org/abs/1212.0312v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software reuse is a subfield of software engineering that is used to adopt\nthe existing software for similar purposes. Reuse Metrics determine the extent\nto which an existing software component is reused in new software with an\nobjective to minimize the errors and cost of the new project. In this paper,\nmedical database related to cardiology is considered. The Pearson Type I\nDistribution is used to calculate the probability density function (pdf) and\nthereby utilizing it for clustering the data. Further, coupling methodology is\nused to bring out the similarity of the new patient data by comparing it with\nthe existing data. By this, the concerned treatment to be followed for the new\npatient is deduced by comparing with that of the previous patients case\nhistory. The metrics proposed by Chidamber and Kemerer are utilized for this\npurpose. This model will be useful for the medical field through software,\nparticularly in remote areas.</p>"
  },
  {
    "title": "A Drift Handling Approach for Self-Adaptive ML Software in Scalable\n  Industrial Processes",
    "date": "2022-08-15",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Firas Bayram",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2208.07037v1"
    },
    "publicTags": [],
    "summary": "Most industrial processes in real-world manufacturing applications are\ncharacterized by the scalability property, which requires an automated strategy\nto self-adapt machine learning (ML) software systems to the new conditions. In\nthis paper, we investigate an Electroslag Remelting (ESR) use case process from\nthe Uddeholms AB steel company. The use case involves predicting the minimum\npressure value for a vacuum pumping event. Taking into account the long time\nrequired to collect new records and efficiently integrate the new machines with\nthe built ML software system. Additionally, to accommodate the changes and\nsatisfy the non-functional requirement of the software system, namely\nadaptability, we propose an automated and adaptive approach based on a drift\nhandling technique called importance weighting. The aim is to address the\nproblem of adding a new furnace to production and enable the adaptability\nattribute of the ML software. The overall results demonstrate the improvements\nin ML software performance achieved by implementing the proposed approach over\nthe classical non-adaptive approach.",
    "sourceUrl": "http://arxiv.org/abs/2208.07037v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Most industrial processes in real-world manufacturing applications are\ncharacterized by the scalability property, which requires an automated strategy\nto self-adapt machine learning (ML) software systems to the new conditions. In\nthis paper, we investigate an Electroslag Remelting (ESR) use case process from\nthe Uddeholms AB steel company. The use case involves predicting the minimum\npressure value for a vacuum pumping event. Taking into account the long time\nrequired to collect new records and efficiently integrate the new machines with\nthe built ML software system. Additionally, to accommodate the changes and\nsatisfy the non-functional requirement of the software system, namely\nadaptability, we propose an automated and adaptive approach based on a drift\nhandling technique called importance weighting. The aim is to address the\nproblem of adding a new furnace to production and enable the adaptability\nattribute of the ML software. The overall results demonstrate the improvements\nin ML software performance achieved by implementing the proposed approach over\nthe classical non-adaptive approach.</p>"
  },
  {
    "title": "Myriad People Open Source Software for New Media Arts",
    "date": "2025-01-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Benoit Baudry",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2501.13644v1"
    },
    "publicTags": [],
    "summary": "New media art builds on top of rich software stacks. Blending multiple media\nsuch as code, light or sound , new media artists integrate various types of\nsoftware to draw, animate, control or synchronize different parts of an\nartwork. Yet, the artworks rarely credit software and all the developers\ninvolved.\n  In this work, we present Myriad People, an original dataset of open source\nprojects and their contributors, which span various software layers used in new\nmedia art installations. To collect this dataset, we released an open call for\nartists and eventually curated 9 artworks, which use a variety of software and\nmedia. In October 2024, we organized a collective exhibition in Stockholm,\nentitled Myriad, which showcased the 9 artworks. The Myriad People dataset\nincludes the 124 open source projects used in one or more of the Myriad's\nartworks, as well as all the contributors to these projects. In this paper, we\npresent the dataset, as well as the possible usages of this dataset for\nsoftware and art research.",
    "sourceUrl": "http://arxiv.org/abs/2501.13644v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>New media art builds on top of rich software stacks. Blending multiple media\nsuch as code, light or sound , new media artists integrate various types of\nsoftware to draw, animate, control or synchronize different parts of an\nartwork. Yet, the artworks rarely credit software and all the developers\ninvolved.\n  In this work, we present Myriad People, an original dataset of open source\nprojects and their contributors, which span various software layers used in new\nmedia art installations. To collect this dataset, we released an open call for\nartists and eventually curated 9 artworks, which use a variety of software and\nmedia. In October 2024, we organized a collective exhibition in Stockholm,\nentitled Myriad, which showcased the 9 artworks. The Myriad People dataset\nincludes the 124 open source projects used in one or more of the Myriad's\nartworks, as well as all the contributors to these projects. In this paper, we\npresent the dataset, as well as the possible usages of this dataset for\nsoftware and art research.</p>"
  },
  {
    "title": "Bayesian propensity score matching in automotive embedded software\n  engineering",
    "date": "2021-09-26",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Yuchu Liu",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2109.12563v1"
    },
    "publicTags": [],
    "summary": "Randomised field experiments, such as A/B testing, have long been the gold\nstandard for evaluating the value that new software brings to customers.\nHowever, running randomised field experiments is not always desired, possible\nor even ethical in the development of automotive embedded software. In the face\nof such restrictions, we propose the use of the Bayesian propensity score\nmatching technique for causal inference of observational studies in the\nautomotive domain. In this paper, we present a method based on the Bayesian\npropensity score matching framework, applied in the unique setting of\nautomotive software engineering. This method is used to generate balanced\ncontrol and treatment groups from an observational online evaluation and\nestimate causal treatment effects from the software changes, even with limited\nsamples in the treatment group. We exemplify the method with a proof-of-concept\nin the automotive domain. In the example, we have a larger control ($N_c=1100$)\nfleet of cars using the current software and a small treatment fleet\n($N_t=38$), in which we introduce a new software variant. We demonstrate a\nscenario that shipping of a new software to all users is restricted, as a\nresult, a fully randomised experiment could not be conducted. Therefore, we\nutilised the Bayesian propensity score matching method with 14 observed\ncovariates as inputs. The results show more balanced groups, suitable for\nestimating causal treatment effects from the collected observational data. We\ndescribe the method in detail and share our configuration. Furthermore, we\ndiscuss how can such a method be used for online evaluation of new software\nutilising small groups of samples.",
    "sourceUrl": "http://arxiv.org/abs/2109.12563v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Randomised field experiments, such as A/B testing, have long been the gold\nstandard for evaluating the value that new software brings to customers.\nHowever, running randomised field experiments is not always desired, possible\nor even ethical in the development of automotive embedded software. In the face\nof such restrictions, we propose the use of the Bayesian propensity score\nmatching technique for causal inference of observational studies in the\nautomotive domain. In this paper, we present a method based on the Bayesian\npropensity score matching framework, applied in the unique setting of\nautomotive software engineering. This method is used to generate balanced\ncontrol and treatment groups from an observational online evaluation and\nestimate causal treatment effects from the software changes, even with limited\nsamples in the treatment group. We exemplify the method with a proof-of-concept\nin the automotive domain. In the example, we have a larger control ($N_c=1100$)\nfleet of cars using the current software and a small treatment fleet\n($N_t=38$), in which we introduce a new software variant. We demonstrate a\nscenario that shipping of a new software to all users is restricted, as a\nresult, a fully randomised experiment could not be conducted. Therefore, we\nutilised the Bayesian propensity score matching method with 14 observed\ncovariates as inputs. The results show more balanced groups, suitable for\nestimating causal treatment effects from the collected observational data. We\ndescribe the method in detail and share our configuration. Furthermore, we\ndiscuss how can such a method be used for online evaluation of new software\nutilising small groups of samples.</p>"
  },
  {
    "title": "Using Dependence Analysis to Support Software Architecture Understanding",
    "date": "2001-05-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jianjun Zhao",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/cs/0105009v1"
    },
    "publicTags": [],
    "summary": "Software architecture is receiving increasingly attention as a critical\ndesign level for software systems. As software architecture design resources\n(in the form of architectural descriptions) are going to be accumulated, the\ndevelopment of techniques and tools to support architectural understanding,\ntesting, reengineering, maintaining, and reusing will become an important\nissue. In this paper we introduce a new dependence analysis technique, named\narchitectural dependence analysis to support software architecture development.\nIn contrast to traditional dependence analysis, architectural dependence\nanalysis is designed to operate on an architectural description of a software\nsystem, rather than the source code of a conventional program. Architectural\ndependence analysis provides knowledge of dependences for the high-level\narchitecture of a software system, rather than the low-level implementation\ndetails of a conventional program.",
    "sourceUrl": "http://arxiv.org/abs/cs/0105009v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software architecture is receiving increasingly attention as a critical\ndesign level for software systems. As software architecture design resources\n(in the form of architectural descriptions) are going to be accumulated, the\ndevelopment of techniques and tools to support architectural understanding,\ntesting, reengineering, maintaining, and reusing will become an important\nissue. In this paper we introduce a new dependence analysis technique, named\narchitectural dependence analysis to support software architecture development.\nIn contrast to traditional dependence analysis, architectural dependence\nanalysis is designed to operate on an architectural description of a software\nsystem, rather than the source code of a conventional program. Architectural\ndependence analysis provides knowledge of dependences for the high-level\narchitecture of a software system, rather than the low-level implementation\ndetails of a conventional program.</p>"
  },
  {
    "title": "Software Reuse in Cardiology Related Medical Database Using K-Means\n  Clustering Technique",
    "date": "2013-11-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "M. Bhanu Sridhar",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1311.1197v1"
    },
    "publicTags": [],
    "summary": "Software technology based on reuse is identified as a process of designing\nsoftware for the reuse purpose. The software reuse is a process in which the\nexisting software is used to build new software. A metric is a quantitative\nindicator of an attribute of an item or thing. Reusability is the likelihood\nfor a segment of source code that can be used again to add new functionalities\nwith slight or no modification. A lot of research has been projected using\nreusability in reducing code, domain, requirements, design etc., but very\nlittle work is reported using software reuse in medical domain. An attempt is\nmade to bridge the gap in this direction, using the concepts of clustering and\nclassifying the data based on the distance measures. In this paper cardiologic\ndatabase is considered for study. The developed model will be useful for\nDoctors or Paramedics to find out the patients level in the cardiologic\ndisease, deduce the medicines required in seconds and propose them to the\npatient. In order to measure the reusability K means clustering algorithm is\nused.",
    "sourceUrl": "http://arxiv.org/abs/1311.1197v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software technology based on reuse is identified as a process of designing\nsoftware for the reuse purpose. The software reuse is a process in which the\nexisting software is used to build new software. A metric is a quantitative\nindicator of an attribute of an item or thing. Reusability is the likelihood\nfor a segment of source code that can be used again to add new functionalities\nwith slight or no modification. A lot of research has been projected using\nreusability in reducing code, domain, requirements, design etc., but very\nlittle work is reported using software reuse in medical domain. An attempt is\nmade to bridge the gap in this direction, using the concepts of clustering and\nclassifying the data based on the distance measures. In this paper cardiologic\ndatabase is considered for study. The developed model will be useful for\nDoctors or Paramedics to find out the patients level in the cardiologic\ndisease, deduce the medicines required in seconds and propose them to the\npatient. In order to measure the reusability K means clustering algorithm is\nused.</p>"
  },
  {
    "title": "A Review of Software Quality Models for the Evaluation of Software\n  Products",
    "date": "2014-12-09",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jose P. Miguel",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1412.2977v1"
    },
    "publicTags": [],
    "summary": "Actually, software products are increasing in a fast way and are used in\nalmost all activities of human life. Consequently measuring and evaluating the\nquality of a software product has become a critical task for many companies.\nSeveral models have been proposed to help diverse types of users with quality\nissues. The development of techniques for building software has influenced the\ncreation of models to assess the quality. Since 2000 the construction of\nsoftware started to depend on generated or manufactured components and gave\nrise to new challenges for assessing quality. These components introduce new\nconcepts such as configurability, reusability, availability, better quality and\nlower cost. Consequently the models are classified in basic models which were\ndeveloped until 2000, and those based on components called tailored quality\nmodels. The purpose of this article is to describe the main models with their\nstrengths and point out some deficiencies. In this work, we conclude that in\nthe present age, aspects of communications play an important factor in the\nquality of software",
    "sourceUrl": "http://arxiv.org/abs/1412.2977v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Actually, software products are increasing in a fast way and are used in\nalmost all activities of human life. Consequently measuring and evaluating the\nquality of a software product has become a critical task for many companies.\nSeveral models have been proposed to help diverse types of users with quality\nissues. The development of techniques for building software has influenced the\ncreation of models to assess the quality. Since 2000 the construction of\nsoftware started to depend on generated or manufactured components and gave\nrise to new challenges for assessing quality. These components introduce new\nconcepts such as configurability, reusability, availability, better quality and\nlower cost. Consequently the models are classified in basic models which were\ndeveloped until 2000, and those based on components called tailored quality\nmodels. The purpose of this article is to describe the main models with their\nstrengths and point out some deficiencies. In this work, we conclude that in\nthe present age, aspects of communications play an important factor in the\nquality of software</p>"
  },
  {
    "title": "Architectural Consistency Checking in Plugin-Based Software Systems",
    "date": "2015-10-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Timo Greifenberg",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1510.08510v1"
    },
    "publicTags": [],
    "summary": "Manually ensuring that the implementation of a software system is consistent\nwith the software architecture is a laborious and error-prone task. Thus, a\nvariety of approaches towards automated consistency checking have been\ndeveloped to counteract architecture erosion. However, these approaches lack\nmeans to define and check architectural restrictions concerning plugin\ndependencies, which is required for plugin-based software systems. In this\npaper, we propose a domain-specific language called Dependency Constraint\nLanguage (DepCoL) to facilitate the definition of constraints concerning plugin\ndependencies. Using DepCoL, it is possible to define constraints affecting\ngroups of plugins, reducing the required specification effort, to formulate\nconstraints for specific plugins only and to refine constraints. Moreover, we\nprovide an Eclipse plugin, which checks whether the software system under\ndevelopment is consistent with the modeled constraints. This enables a seamless\nintegration into the development process to effortless check consistency during\ndevelopment of the software system. In this way, developers are informed about\ndependency violations immediately and this supports developers in counteracting\narchitecture erosion.",
    "sourceUrl": "http://arxiv.org/abs/1510.08510v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Manually ensuring that the implementation of a software system is consistent\nwith the software architecture is a laborious and error-prone task. Thus, a\nvariety of approaches towards automated consistency checking have been\ndeveloped to counteract architecture erosion. However, these approaches lack\nmeans to define and check architectural restrictions concerning plugin\ndependencies, which is required for plugin-based software systems. In this\npaper, we propose a domain-specific language called Dependency Constraint\nLanguage (DepCoL) to facilitate the definition of constraints concerning plugin\ndependencies. Using DepCoL, it is possible to define constraints affecting\ngroups of plugins, reducing the required specification effort, to formulate\nconstraints for specific plugins only and to refine constraints. Moreover, we\nprovide an Eclipse plugin, which checks whether the software system under\ndevelopment is consistent with the modeled constraints. This enables a seamless\nintegration into the development process to effortless check consistency during\ndevelopment of the software system. In this way, developers are informed about\ndependency violations immediately and this supports developers in counteracting\narchitecture erosion.</p>"
  },
  {
    "title": "Challenges for Inclusion in Software Engineering: The Case of the\n  Emerging Papua New Guinean Society",
    "date": "2019-10-31",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Raula Gaikovina Kula",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1911.04016v2"
    },
    "publicTags": [],
    "summary": "Software plays a central role in modern societies, with its high economic\nvalue and potential for advancing societal change. In this paper, we\ncharacterise challenges and opportunities for a country progressing towards\nentering the global software industry, focusing on Papua New Guinea (PNG). By\nhosting a Software Engineering workshop, we conducted a qualitative study by\nrecording talks (n=3), employing a questionnaire (n=52), and administering an\nin-depth focus group session with local actors (n=5). Based on a thematic\nanalysis, we identified challenges as barriers and opportunities for the PNG\nsoftware engineering community. We also discuss the state of practices and how\nto make it inclusive for practitioners, researchers, and educators from both\nthe local and global software engineering community.",
    "sourceUrl": "http://arxiv.org/abs/1911.04016v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software plays a central role in modern societies, with its high economic\nvalue and potential for advancing societal change. In this paper, we\ncharacterise challenges and opportunities for a country progressing towards\nentering the global software industry, focusing on Papua New Guinea (PNG). By\nhosting a Software Engineering workshop, we conducted a qualitative study by\nrecording talks (n=3), employing a questionnaire (n=52), and administering an\nin-depth focus group session with local actors (n=5). Based on a thematic\nanalysis, we identified challenges as barriers and opportunities for the PNG\nsoftware engineering community. We also discuss the state of practices and how\nto make it inclusive for practitioners, researchers, and educators from both\nthe local and global software engineering community.</p>"
  },
  {
    "title": "LabelGit: A Dataset for Software Repositories Classification using\n  Attributed Dependency Graphs",
    "date": "2021-03-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Cezar Sas",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2103.08890v1"
    },
    "publicTags": [],
    "summary": "Software repository hosting services contain large amounts of open-source\nsoftware, with GitHub hosting more than 100 million repositories, from new to\nestablished ones. Given this vast amount of projects, there is a pressing need\nfor a search based on the software's content and features. However, even though\nGitHub offers various solutions to aid software discovery, most repositories do\nnot have any labels, reducing the utility of search and topic-based analysis.\nMoreover, classifying software modules is also getting more importance given\nthe increase in Component-Based Software Development. However, previous work\nfocused on software classification using keyword-based approaches or proxies\nfor the project (e.g., README), which is not always available. In this work, we\ncreate a new annotated dataset of GitHub Java projects called LabelGit. Our\ndataset uses direct information from the source code, like the dependency graph\nand source code neural representations from the identifiers. Using this\ndataset, we hope to aid the development of solutions that do not rely on\nproxies but use the entire source code to perform classification.",
    "sourceUrl": "http://arxiv.org/abs/2103.08890v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software repository hosting services contain large amounts of open-source\nsoftware, with GitHub hosting more than 100 million repositories, from new to\nestablished ones. Given this vast amount of projects, there is a pressing need\nfor a search based on the software's content and features. However, even though\nGitHub offers various solutions to aid software discovery, most repositories do\nnot have any labels, reducing the utility of search and topic-based analysis.\nMoreover, classifying software modules is also getting more importance given\nthe increase in Component-Based Software Development. However, previous work\nfocused on software classification using keyword-based approaches or proxies\nfor the project (e.g., README), which is not always available. In this work, we\ncreate a new annotated dataset of GitHub Java projects called LabelGit. Our\ndataset uses direct information from the source code, like the dependency graph\nand source code neural representations from the identifiers. Using this\ndataset, we hope to aid the development of solutions that do not rely on\nproxies but use the entire source code to perform classification.</p>"
  },
  {
    "title": "SCENARIOTCHECK: A Checklist-based Reading Technique for the Verification\n  of IoT Scenarios",
    "date": "2021-07-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Bruno Pedraca de Souza",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2107.13597v1"
    },
    "publicTags": [],
    "summary": "Software systems on the Internet of Things have driven the world into a new\nindustrial revolution, bringing with it new features and concerns such as\nautonomy, continuous device connectivity, and interaction among systems, users,\nand things. Nevertheless, building these types of systems is still a\nproblematic activity due to their specific features. Empirical studies show the\nlack of technologies to support the construction of IoT software systems, in\nwhich different software artifacts should be created to ensure their quality.\nThus, software inspection has emerged as an alternative evidence-based method\nto support the quality assurance of artifacts produced during the software\ndevelopment cycle. However, there is no knowledge of inspection techniques\napplicable to IoT software systems. Therefore, this research presents\nSCENARIOTCHECK, a Checklist-based Reading Technique for the Verification of IoT\nScenarios. The checklist has been evaluated with experimental studies. This\nresearch shows that the technique has good results regarding cost-efficiency,\nefficiency, and IoT software system development effectiveness.",
    "sourceUrl": "http://arxiv.org/abs/2107.13597v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software systems on the Internet of Things have driven the world into a new\nindustrial revolution, bringing with it new features and concerns such as\nautonomy, continuous device connectivity, and interaction among systems, users,\nand things. Nevertheless, building these types of systems is still a\nproblematic activity due to their specific features. Empirical studies show the\nlack of technologies to support the construction of IoT software systems, in\nwhich different software artifacts should be created to ensure their quality.\nThus, software inspection has emerged as an alternative evidence-based method\nto support the quality assurance of artifacts produced during the software\ndevelopment cycle. However, there is no knowledge of inspection techniques\napplicable to IoT software systems. Therefore, this research presents\nSCENARIOTCHECK, a Checklist-based Reading Technique for the Verification of IoT\nScenarios. The checklist has been evaluated with experimental studies. This\nresearch shows that the technique has good results regarding cost-efficiency,\nefficiency, and IoT software system development effectiveness.</p>"
  },
  {
    "title": "Software Supply Chain Map: How Reuse Networks Expand",
    "date": "2022-04-13",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Hideaki Hata",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2204.06531v1"
    },
    "publicTags": [],
    "summary": "Clone-and-own is a typical code reuse approach because of its simplicity and\nefficiency. Cloned software components are maintained independently by a new\nowner. These clone-and-own operations can be occurred sequentially, that is,\ncloned components can be cloned again and owned by other new owners on the\nsupply chain. In general, code reuse is not documented well, consequently,\nappropriate changes like security patches cannot be propagated to descendant\nsoftware projects. However, the OpenChain Project defined identifying and\ntracking source code reuses as responsibilities of FLOSS software staffs. Hence\nsupporting source code reuse awareness is in a real need. This paper studies\nsoftware reuse relations in FLOSS ecosystem. Technically, clone-and-own reuses\nof source code can be identified by file-level clone set detection. Since\nchange histories are associated with files, we can determine origins and\ndestinations in reusing across multiple software by considering times. By\nbuilding software supply chain maps, we find that clone-and-own is prevalent in\nFLOSS development, and set of files are reused widely and repeatedly. These\nobservations open up future challenges of maintaining and tracking global\nsoftware genealogies.",
    "sourceUrl": "http://arxiv.org/abs/2204.06531v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Clone-and-own is a typical code reuse approach because of its simplicity and\nefficiency. Cloned software components are maintained independently by a new\nowner. These clone-and-own operations can be occurred sequentially, that is,\ncloned components can be cloned again and owned by other new owners on the\nsupply chain. In general, code reuse is not documented well, consequently,\nappropriate changes like security patches cannot be propagated to descendant\nsoftware projects. However, the OpenChain Project defined identifying and\ntracking source code reuses as responsibilities of FLOSS software staffs. Hence\nsupporting source code reuse awareness is in a real need. This paper studies\nsoftware reuse relations in FLOSS ecosystem. Technically, clone-and-own reuses\nof source code can be identified by file-level clone set detection. Since\nchange histories are associated with files, we can determine origins and\ndestinations in reusing across multiple software by considering times. By\nbuilding software supply chain maps, we find that clone-and-own is prevalent in\nFLOSS development, and set of files are reused widely and repeatedly. These\nobservations open up future challenges of maintaining and tracking global\nsoftware genealogies.</p>"
  },
  {
    "title": "Self-sustaining Software Systems (S4): Towards Improved Interpretability\n  and Adaptation",
    "date": "2024-01-21",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Christian Cabrera",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2401.11370v1"
    },
    "publicTags": [],
    "summary": "Software systems impact society at different levels as they pervasively solve\nreal-world problems. Modern software systems are often so sophisticated that\ntheir complexity exceeds the limits of human comprehension. These systems must\nrespond to changing goals, dynamic data, unexpected failures, and security\nthreats, among other variable factors in real-world environments. Systems'\ncomplexity challenges their interpretability and requires autonomous responses\nto dynamic changes. Two main research areas explore autonomous systems'\nresponses: evolutionary computing and autonomic computing. Evolutionary\ncomputing focuses on software improvement based on iterative modifications to\nthe source code. Autonomic computing focuses on optimising systems' performance\nby changing their structure, behaviour, or environment variables. Approaches\nfrom both areas rely on feedback loops that accumulate knowledge from the\nsystem interactions to inform autonomous decision-making. However, this\nknowledge is often limited, constraining the systems' interpretability and\nadaptability. This paper proposes a new concept for interpretable and adaptable\nsoftware systems: self-sustaining software systems (S4). S4 builds knowledge\nloops between all available knowledge sources that define modern software\nsystems to improve their interpretability and adaptability. This paper\nintroduces and discusses the S4 concept.",
    "sourceUrl": "http://arxiv.org/abs/2401.11370v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software systems impact society at different levels as they pervasively solve\nreal-world problems. Modern software systems are often so sophisticated that\ntheir complexity exceeds the limits of human comprehension. These systems must\nrespond to changing goals, dynamic data, unexpected failures, and security\nthreats, among other variable factors in real-world environments. Systems'\ncomplexity challenges their interpretability and requires autonomous responses\nto dynamic changes. Two main research areas explore autonomous systems'\nresponses: evolutionary computing and autonomic computing. Evolutionary\ncomputing focuses on software improvement based on iterative modifications to\nthe source code. Autonomic computing focuses on optimising systems' performance\nby changing their structure, behaviour, or environment variables. Approaches\nfrom both areas rely on feedback loops that accumulate knowledge from the\nsystem interactions to inform autonomous decision-making. However, this\nknowledge is often limited, constraining the systems' interpretability and\nadaptability. This paper proposes a new concept for interpretable and adaptable\nsoftware systems: self-sustaining software systems (S4). S4 builds knowledge\nloops between all available knowledge sources that define modern software\nsystems to improve their interpretability and adaptability. This paper\nintroduces and discusses the S4 concept.</p>"
  },
  {
    "title": "Prediction of rate of improvement of software quality and development\n  effort on the basis of Degreeof excellence with respect to number of lines of\n  code",
    "date": "2014-04-19",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ekbal Rashid",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1404.4970v1"
    },
    "publicTags": [],
    "summary": "The objective of this research work is to improve the degree of excellence by\nremoving the number of exceptions from the software. The modern age is more\nconcerned with the quality of software. Extensive research is being carried out\nin this direction. The rate of improvement of quality of software largely\ndepends on the development time. This development time is chiefly calculated in\nclock hours. However development time does not reflect the effort put in by the\ndeveloper. A better parameter can be the rate of improvement of quality level\nor the rate of improvement of the degree of excellence with respect to time.\nNow this parameter needs the prediction of error level and degree of excellence\nat a particular stage of development of the software. This paper explores an\nattempt to develop a system to predict rate of improvement of the software\nquality at a particular point of time with respect to the number of lines of\ncode present in the software. Having calculated the error level and degree of\nexcellence at two points in time, we can move forward towards the estimation of\nthe rate of improvement of the software quality with respect to time. This\nparameter can estimate the effort put in while development of the software and\ncan add a new dimension to the understanding of software quality in software\nengineering domain. In order to obtain the results we have used an indigenous\ntool for software quality prediction and for graphical representation of data,\nwe have used Microsoft office 2007 graphical chart.",
    "sourceUrl": "http://arxiv.org/abs/1404.4970v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The objective of this research work is to improve the degree of excellence by\nremoving the number of exceptions from the software. The modern age is more\nconcerned with the quality of software. Extensive research is being carried out\nin this direction. The rate of improvement of quality of software largely\ndepends on the development time. This development time is chiefly calculated in\nclock hours. However development time does not reflect the effort put in by the\ndeveloper. A better parameter can be the rate of improvement of quality level\nor the rate of improvement of the degree of excellence with respect to time.\nNow this parameter needs the prediction of error level and degree of excellence\nat a particular stage of development of the software. This paper explores an\nattempt to develop a system to predict rate of improvement of the software\nquality at a particular point of time with respect to the number of lines of\ncode present in the software. Having calculated the error level and degree of\nexcellence at two points in time, we can move forward towards the estimation of\nthe rate of improvement of the software quality with respect to time. This\nparameter can estimate the effort put in while development of the software and\ncan add a new dimension to the understanding of software quality in software\nengineering domain. In order to obtain the results we have used an indigenous\ntool for software quality prediction and for graphical representation of data,\nwe have used Microsoft office 2007 graphical chart.</p>"
  },
  {
    "title": "Software Sustainability: A Systematic Literature Review and\n  Comprehensive Analysis",
    "date": "2019-10-11",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Asif Imran",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1910.06109v1"
    },
    "publicTags": [],
    "summary": "Software Engineering is a constantly evolving subject area that faces new\nchallenges every day as it tries to automate newer business processes. One of\nthe key challenges to the success of a software solution is attaining\nsustainability. The inability of numerous software to sustain for the desired\ntime-length is caused by limited consideration given towards sustainability\nduring the stages of software development. This review aims to present a\ndetailed and inclusive study covering both the technical and non-technical\nchallenges and approaches of software sustainability. A systematic and\ncomprehensive literature review was conducted based on 107 relevant studies\nthat were selected using the Evidence-Based Software Engineering (EBSE)\ntechnique. The study showed that sustainability can be achieved by conducting\nspecific activities at the technical and non-technical levels. The technical\nlevel consists of software design, coding, and user experience attributes. The\nnon-technical level consists of documentation, sustainability manifestos,\ntraining of software engineers, funding software projects, and leadership\nskills of project managers to achieve sustainability. This paper groups the\nexisting research efforts based on the above aspects. Next, how those aspects\naffect open and closed source software is tabulated. Based on the findings of\nthis review, it is seen that both technical and non-technical sustainability\naspects are equally important, taking one into contention and ignoring the\nother will threaten the sustenance of software products.",
    "sourceUrl": "http://arxiv.org/abs/1910.06109v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software Engineering is a constantly evolving subject area that faces new\nchallenges every day as it tries to automate newer business processes. One of\nthe key challenges to the success of a software solution is attaining\nsustainability. The inability of numerous software to sustain for the desired\ntime-length is caused by limited consideration given towards sustainability\nduring the stages of software development. This review aims to present a\ndetailed and inclusive study covering both the technical and non-technical\nchallenges and approaches of software sustainability. A systematic and\ncomprehensive literature review was conducted based on 107 relevant studies\nthat were selected using the Evidence-Based Software Engineering (EBSE)\ntechnique. The study showed that sustainability can be achieved by conducting\nspecific activities at the technical and non-technical levels. The technical\nlevel consists of software design, coding, and user experience attributes. The\nnon-technical level consists of documentation, sustainability manifestos,\ntraining of software engineers, funding software projects, and leadership\nskills of project managers to achieve sustainability. This paper groups the\nexisting research efforts based on the above aspects. Next, how those aspects\naffect open and closed source software is tabulated. Based on the findings of\nthis review, it is seen that both technical and non-technical sustainability\naspects are equally important, taking one into contention and ignoring the\nother will threaten the sustenance of software products.</p>"
  },
  {
    "title": "Software Engineering in Civic Tech: A Case Study about Code for Ireland",
    "date": "2019-04-08",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Antti Knutas",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1904.04104v1"
    },
    "publicTags": [],
    "summary": "Civic grassroots have proven their ability to create useful and scalable\nsoftware that addresses pressing social needs. Although software engineering\nplays a fundamental role in the process of creating civic technology, academic\nliterature that analyses the software development processes of civic tech\ngrassroots is scarce. This paper aims to advance the understanding of how civic\ngrassroots tackle the different activities in their software development\nprocesses. In this study, we followed the formation of two projects in a civic\ntech group (Code for Ireland) seeking to understand how their development\nprocesses evolved over time, and how the group carried out their work in\ncreating new technology. Our preliminary findings show that such groups are\ncapable of setting up systematic software engineering processes that address\nsoftware specification, development, validation, and evolution. While they were\nable to deliver software according to self-specified quality standards, the\ngroup has challenges in requirements specification, stakeholder engagement, and\nreorienting from development to product delivery. Software engineering methods\nand tools can effectively support the future of civic technologies and\npotentially improve their management, quality, and durability.",
    "sourceUrl": "http://arxiv.org/abs/1904.04104v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Civic grassroots have proven their ability to create useful and scalable\nsoftware that addresses pressing social needs. Although software engineering\nplays a fundamental role in the process of creating civic technology, academic\nliterature that analyses the software development processes of civic tech\ngrassroots is scarce. This paper aims to advance the understanding of how civic\ngrassroots tackle the different activities in their software development\nprocesses. In this study, we followed the formation of two projects in a civic\ntech group (Code for Ireland) seeking to understand how their development\nprocesses evolved over time, and how the group carried out their work in\ncreating new technology. Our preliminary findings show that such groups are\ncapable of setting up systematic software engineering processes that address\nsoftware specification, development, validation, and evolution. While they were\nable to deliver software according to self-specified quality standards, the\ngroup has challenges in requirements specification, stakeholder engagement, and\nreorienting from development to product delivery. Software engineering methods\nand tools can effectively support the future of civic technologies and\npotentially improve their management, quality, and durability.</p>"
  },
  {
    "title": "Software Training in High Energy Physics",
    "date": "2022-03-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Michel H. Villanueva",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2203.04775v1"
    },
    "publicTags": [],
    "summary": "Among the upgrades in current high energy physics (HEP) experiments and the\nnew facilities coming online, solving software challenges has become integral\nfor the success of the collaborations, The demand for human resources\nhighly-skilled in both HEP and software domains is increasing. With a highly\ndistributed environment in human resources, the sustainability of the HEP\necosystem requires a continuous effort in the equipment of physicists with the\nrequired abilities in software development. In this paper, the collective\nsoftware training program in HEP and its activities led by the HEP Software\nFoundation (HSF) and the Institute for Research and Innovation in Software in\nHEP (IRIS-HEP) are presented. Experiment-agnostic, open, and accessible modules\nfor training have been developed, focusing on common software material with\nranges from core software skills needed by everyone to advanced training\nrequired to produce high-quality sustainable software. A basic software\ncurriculum was built, and an introductory software training event has been\nprepared to serve HEP entrants. This program serves individuals with\ntransferable skills that are becoming increasingly important to careers in the\nrealm of software and computing, whether inside or outside HEP.",
    "sourceUrl": "http://arxiv.org/abs/2203.04775v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Among the upgrades in current high energy physics (HEP) experiments and the\nnew facilities coming online, solving software challenges has become integral\nfor the success of the collaborations, The demand for human resources\nhighly-skilled in both HEP and software domains is increasing. With a highly\ndistributed environment in human resources, the sustainability of the HEP\necosystem requires a continuous effort in the equipment of physicists with the\nrequired abilities in software development. In this paper, the collective\nsoftware training program in HEP and its activities led by the HEP Software\nFoundation (HSF) and the Institute for Research and Innovation in Software in\nHEP (IRIS-HEP) are presented. Experiment-agnostic, open, and accessible modules\nfor training have been developed, focusing on common software material with\nranges from core software skills needed by everyone to advanced training\nrequired to produce high-quality sustainable software. A basic software\ncurriculum was built, and an introductory software training event has been\nprepared to serve HEP entrants. This program serves individuals with\ntransferable skills that are becoming increasingly important to careers in the\nrealm of software and computing, whether inside or outside HEP.</p>"
  },
  {
    "title": "Practitioners Testimonials about Software Testing",
    "date": "2021-03-10",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Pradeep Waychal",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2103.06343v1"
    },
    "publicTags": [],
    "summary": "As software systems are becoming more pervasive, they are also becoming more\nsusceptible to failures, resulting in potentially lethal combinations. Software\ntesting is critical to preventing software failures but is, arguably, the least\nunderstood part of the software life cycle and the toughest to perform\ncorrectly. Adequate research has been carried out in both the process and\ntechnology dimensions of testing, but not in the human dimensions. This paper\nattempts to fill in the gap by exploring the human dimension, i.e., trying to\nunderstand the motivation of software professionals to take up and sustain\ntesting careers. Towards that end, a survey was conducted in four countries -\nIndia, Canada, Cuba, and China - to try to understand how professional software\ntesters perceive and value work-related factors that could influence their\nmotivation to take up and sustain testing careers. With a sample of 220\nsoftware professionals, we observed that very few professionals are keen to\ntake up testing careers. Some aspects of software testing, such as the learning\nopportunities, appear to be a common motivator across the four countries;\nwhereas the treatment meted out to testers as second-class citizens and the\ncomplexity of the job appeared to be the most important de-motivators. This\ncomparative study offers useful insights that can help global software industry\nleaders to come up with an action plan to put the software testing profession\nunder a new light. That could increase the number of software engineers\nchoosing testing careers, which would facilitate quality testing.",
    "sourceUrl": "http://arxiv.org/abs/2103.06343v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>As software systems are becoming more pervasive, they are also becoming more\nsusceptible to failures, resulting in potentially lethal combinations. Software\ntesting is critical to preventing software failures but is, arguably, the least\nunderstood part of the software life cycle and the toughest to perform\ncorrectly. Adequate research has been carried out in both the process and\ntechnology dimensions of testing, but not in the human dimensions. This paper\nattempts to fill in the gap by exploring the human dimension, i.e., trying to\nunderstand the motivation of software professionals to take up and sustain\ntesting careers. Towards that end, a survey was conducted in four countries -\nIndia, Canada, Cuba, and China - to try to understand how professional software\ntesters perceive and value work-related factors that could influence their\nmotivation to take up and sustain testing careers. With a sample of 220\nsoftware professionals, we observed that very few professionals are keen to\ntake up testing careers. Some aspects of software testing, such as the learning\nopportunities, appear to be a common motivator across the four countries;\nwhereas the treatment meted out to testers as second-class citizens and the\ncomplexity of the job appeared to be the most important de-motivators. This\ncomparative study offers useful insights that can help global software industry\nleaders to come up with an action plan to put the software testing profession\nunder a new light. That could increase the number of software engineers\nchoosing testing careers, which would facilitate quality testing.</p>"
  },
  {
    "title": "Software quality: A Historical and Synthetic Content Analysis",
    "date": "2021-06-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Peter Kokol",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2106.14598v1"
    },
    "publicTags": [],
    "summary": "Interconnected computers and software systems have become an indispensable\npart of people's lives, therefore software quality research is becoming more\nand more important. There have been multiple attempts to synthesize knowledge\ngained in software quality research, however, they were focused mainly on\nsingle aspects of software quality and not to structure the knowledge in a\nholistic way. The aim of our study was to close this gap. The software quality\npublications were harvested from the Scopus bibliographic database. The\nmetadata was exported first to CRexlporer, which was employed to identify\nhistorical roots, and next to VOSViewer, which was used as a part of the\nsynthetic content analysis. In our study we defined synthetic context analysis\nas a triangulation of bibliometrics and content analysis. Our search resulted\nin 14451 publications. The performance bibliometric study showed that the\nproduction of research publications relating to software quality is currently\nfollowing an exponential growth trend and that the software quality research\ncommunity is growing. The most productive country was the United States and the\nmost productive Institution The Florida Atlantic University. The synthetic\ncontent analysis revealed that the published knowledge can be structured into\n10 themes, the most important being the themes regarding software quality\nimprovement with enhancing software engineering, advanced software testing, and\nimproved defect and fault prediction with machine learning and data mining.\nAccording to the analysis of the hot topics, it seems that future research will\nbe directed into developing and using a full specter of new artificial\nintelligence tools (not just machine learning and data mining) and focusing on\nhow to assure software quality in agile development paradigms.",
    "sourceUrl": "http://arxiv.org/abs/2106.14598v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Interconnected computers and software systems have become an indispensable\npart of people's lives, therefore software quality research is becoming more\nand more important. There have been multiple attempts to synthesize knowledge\ngained in software quality research, however, they were focused mainly on\nsingle aspects of software quality and not to structure the knowledge in a\nholistic way. The aim of our study was to close this gap. The software quality\npublications were harvested from the Scopus bibliographic database. The\nmetadata was exported first to CRexlporer, which was employed to identify\nhistorical roots, and next to VOSViewer, which was used as a part of the\nsynthetic content analysis. In our study we defined synthetic context analysis\nas a triangulation of bibliometrics and content analysis. Our search resulted\nin 14451 publications. The performance bibliometric study showed that the\nproduction of research publications relating to software quality is currently\nfollowing an exponential growth trend and that the software quality research\ncommunity is growing. The most productive country was the United States and the\nmost productive Institution The Florida Atlantic University. The synthetic\ncontent analysis revealed that the published knowledge can be structured into\n10 themes, the most important being the themes regarding software quality\nimprovement with enhancing software engineering, advanced software testing, and\nimproved defect and fault prediction with machine learning and data mining.\nAccording to the analysis of the hot topics, it seems that future research will\nbe directed into developing and using a full specter of new artificial\nintelligence tools (not just machine learning and data mining) and focusing on\nhow to assure software quality in agile development paradigms.</p>"
  },
  {
    "title": "Unified Software Engineering agent as AI Software Engineer",
    "date": "2025-06-17",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Leonhard Applis",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2506.14683v1"
    },
    "publicTags": [],
    "summary": "The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future.",
    "sourceUrl": "http://arxiv.org/abs/2506.14683v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future.</p>"
  },
  {
    "title": "ScaMaha: A Tool for Parsing, Analyzing, and Visualizing Object-Oriented\n  Software Systems",
    "date": "2025-01-19",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ra'Fat Al-Msie'deen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2501.11001v1"
    },
    "publicTags": [],
    "summary": "Reverse engineering tools are required to handle the complexity of software\nproducts and the unique requirements of many different tasks, like software\nanalysis and visualization. Thus, reverse engineering tools should adapt to a\nvariety of cases. Static Code Analysis (SCA) is a technique for analyzing and\nexploring software source code without running it. Manual review of software\nsource code puts additional effort on software developers and is a tedious,\nerror-prone, and costly job. This paper proposes an original approach (called\nScaMaha) for Object-Oriented (OO) source code analysis and visualization based\non SCA. ScaMaha is a modular, flexible, and extensible reverse engineering\ntool. ScaMaha revolves around a new meta-model and a new code parser, analyzer,\nand visualizer. ScaMaha parser extracts software source code based on the\nAbstract Syntax Tree (AST) and stores this code as a code file. The code file\nincludes all software code identifiers, relations, and structural information.\nScaMaha analyzer studies and exploits the code files to generate useful\ninformation regarding software source code. The software metrics file gives\nunique metrics regarding software systems, such as the number of method access\nrelations. Software source code visualization plays an important role in\nsoftware comprehension. Thus, ScaMaha visualizer exploits code files to\nvisualize different aspects of software source code. The visualizer generates\nunique graphs about software source code, like the visualization of inheritance\nrelations. ScaMaha tool was applied to several case studies from small to large\nsoftware systems, such as drawing shapes, mobile photo, health watcher, rhino,\nand ArgoUML. Results show the scalability, performance, soundness, and accuracy\nof ScaMaha tool. Evaluation metrics, such as precision and recall, demonstrate\nthe accuracy of ScaMaha ...",
    "sourceUrl": "http://arxiv.org/abs/2501.11001v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Reverse engineering tools are required to handle the complexity of software\nproducts and the unique requirements of many different tasks, like software\nanalysis and visualization. Thus, reverse engineering tools should adapt to a\nvariety of cases. Static Code Analysis (SCA) is a technique for analyzing and\nexploring software source code without running it. Manual review of software\nsource code puts additional effort on software developers and is a tedious,\nerror-prone, and costly job. This paper proposes an original approach (called\nScaMaha) for Object-Oriented (OO) source code analysis and visualization based\non SCA. ScaMaha is a modular, flexible, and extensible reverse engineering\ntool. ScaMaha revolves around a new meta-model and a new code parser, analyzer,\nand visualizer. ScaMaha parser extracts software source code based on the\nAbstract Syntax Tree (AST) and stores this code as a code file. The code file\nincludes all software code identifiers, relations, and structural information.\nScaMaha analyzer studies and exploits the code files to generate useful\ninformation regarding software source code. The software metrics file gives\nunique metrics regarding software systems, such as the number of method access\nrelations. Software source code visualization plays an important role in\nsoftware comprehension. Thus, ScaMaha visualizer exploits code files to\nvisualize different aspects of software source code. The visualizer generates\nunique graphs about software source code, like the visualization of inheritance\nrelations. ScaMaha tool was applied to several case studies from small to large\nsoftware systems, such as drawing shapes, mobile photo, health watcher, rhino,\nand ArgoUML. Results show the scalability, performance, soundness, and accuracy\nof ScaMaha tool. Evaluation metrics, such as precision and recall, demonstrate\nthe accuracy of ScaMaha ...</p>"
  },
  {
    "title": "Constructing a software requirements specification and design for\n  electronic IT news magazine system",
    "date": "2021-11-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ra'Fat Al-Msie'deen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2111.01501v1"
    },
    "publicTags": [],
    "summary": "Requirements engineering process intends to obtain software services and\nconstraints. This process is essential to meet the customer's needs and\nexpectations. This process includes three main activities in general. These are\ndetecting requirements by interacting with software stakeholders, transferring\nthese requirements into a standard document, and examining that the\nrequirements really define the software that the client needs. Functional\nrequirements are services that the software should deliver to the end-user. In\naddition, functional requirements describe how the software should respond to\nspecific inputs, and how the software should behave in certain circumstances.\nThis paper aims to develop a software requirements specification document of\nthe electronic IT news magazine system. The electronic magazine provides users\nto post and view up-to-date IT news. Still, there is a lack in the literature\nof comprehensive studies about the construction of the electronic magazine\nsoftware specification and design in conformance with the contemporary software\ndevelopment processes. Moreover, there is a need for a suitable research\nframework to support the requirements engineering process. The novelty of this\npaper is the construction of software specification and design of the\nelectronic magazine by following the Al-Msie'deen research framework. All the\ndocuments of software requirements specification and design have been\nconstructed to conform to the agile usage-centered design technique and the\nproposed research framework. A requirements specification and design are\nsuggested and followed for the construction of the electronic magazine\nsoftware. This study proved that involving users extensively in the process of\nsoftware requirements specification and design will lead to the creation of\ndependable and acceptable software systems.",
    "sourceUrl": "http://arxiv.org/abs/2111.01501v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Requirements engineering process intends to obtain software services and\nconstraints. This process is essential to meet the customer's needs and\nexpectations. This process includes three main activities in general. These are\ndetecting requirements by interacting with software stakeholders, transferring\nthese requirements into a standard document, and examining that the\nrequirements really define the software that the client needs. Functional\nrequirements are services that the software should deliver to the end-user. In\naddition, functional requirements describe how the software should respond to\nspecific inputs, and how the software should behave in certain circumstances.\nThis paper aims to develop a software requirements specification document of\nthe electronic IT news magazine system. The electronic magazine provides users\nto post and view up-to-date IT news. Still, there is a lack in the literature\nof comprehensive studies about the construction of the electronic magazine\nsoftware specification and design in conformance with the contemporary software\ndevelopment processes. Moreover, there is a need for a suitable research\nframework to support the requirements engineering process. The novelty of this\npaper is the construction of software specification and design of the\nelectronic magazine by following the Al-Msie'deen research framework. All the\ndocuments of software requirements specification and design have been\nconstructed to conform to the agile usage-centered design technique and the\nproposed research framework. A requirements specification and design are\nsuggested and followed for the construction of the electronic magazine\nsoftware. This study proved that involving users extensively in the process of\nsoftware requirements specification and design will lead to the creation of\ndependable and acceptable software systems.</p>"
  },
  {
    "title": "Proactive Web Server Protocol for Complaint Assessment",
    "date": "2014-02-09",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "G. Vijay Kumar",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1402.1943v1"
    },
    "publicTags": [],
    "summary": "Vulnerability Discovery with attack Injection security threats are increasing\nfor the server software, when software is developed, the software tested for\nthe functionality. Due to unawareness of software vulnerabilities most of the\nsoftware before pre-Release the software should be thoroughly tested for not\nonly functionality reliability, but should be tested for the security flows\n(or) vulnerabilities. The approaches such as fuzzers, Fault injection,\nvulnerabilities scanners, static vulnerabilities analyzers, Run time prevention\nmechanisms and software Rejuvenation are identifying the un-patched software\nwhich is open for security threats address to solve the problem \"security\ntesting\". These techniques are useful for generating attacks but cannot be\nextendable for the new land of attacks. The system called proactive\nvulnerability attack injection tool is suitable for adding new attacks\ninjection vectors, methods to define new protocol states (or) Specification\nusing the interface of tool includes Network server protocol specification\nusing GUI, Attacks generator, Attack injector, monitoring module at the victim\ninjector, monitoring module at the victim machine and the attacks injection\nreport generation. This tool can address most of the vulnerabilities (or)\nsecurity flows.",
    "sourceUrl": "http://arxiv.org/abs/1402.1943v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Vulnerability Discovery with attack Injection security threats are increasing\nfor the server software, when software is developed, the software tested for\nthe functionality. Due to unawareness of software vulnerabilities most of the\nsoftware before pre-Release the software should be thoroughly tested for not\nonly functionality reliability, but should be tested for the security flows\n(or) vulnerabilities. The approaches such as fuzzers, Fault injection,\nvulnerabilities scanners, static vulnerabilities analyzers, Run time prevention\nmechanisms and software Rejuvenation are identifying the un-patched software\nwhich is open for security threats address to solve the problem \"security\ntesting\". These techniques are useful for generating attacks but cannot be\nextendable for the new land of attacks. The system called proactive\nvulnerability attack injection tool is suitable for adding new attacks\ninjection vectors, methods to define new protocol states (or) Specification\nusing the interface of tool includes Network server protocol specification\nusing GUI, Attacks generator, Attack injector, monitoring module at the victim\ninjector, monitoring module at the victim machine and the attacks injection\nreport generation. This tool can address most of the vulnerabilities (or)\nsecurity flows.</p>"
  },
  {
    "title": "Hybrid architecture for satellite data processing workflow management",
    "date": "2015-09-25",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Naresh Kumar Mallenahalli",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1509.08790v1"
    },
    "publicTags": [],
    "summary": "The ever growing demand for remote sensing data products by user community\nhas resulted in many Indian and foreign remote sensing satellites being\nlaunched. The diversity in the remote sensing sensors has resulted in\nheterogeneous software and hardware environments for generating geospatial data\nproducts. The workflow automation software knows as information management\nsystem is in place at National Remote Sensing Centre (NRSC) catering to the\nneeds of the data processing and data dissemination. The software components of\nworkflow are interfaced in different heterogeneous environments that get\nexecuted at data processing software in automated and semi automated modes. For\nevery new satellite being launched, the software is modified or upgraded if new\nbusiness processes are introduced. In this study, we propose a software\narchitecture that gives more flexible automation with very less manageable\ncode. The study also addresses utilization and extraction of useful information\nfrom historic production and customer details. A comparison of the current\nworkflow software architecture with existing practices in industry like Service\nOriented Architecture (SOA), Extensible Markup Languages (XML), and Event based\narchitectures has been made. A new hybrid approach based on the industry\npractices is proposed to improve the existing workflow.",
    "sourceUrl": "http://arxiv.org/abs/1509.08790v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The ever growing demand for remote sensing data products by user community\nhas resulted in many Indian and foreign remote sensing satellites being\nlaunched. The diversity in the remote sensing sensors has resulted in\nheterogeneous software and hardware environments for generating geospatial data\nproducts. The workflow automation software knows as information management\nsystem is in place at National Remote Sensing Centre (NRSC) catering to the\nneeds of the data processing and data dissemination. The software components of\nworkflow are interfaced in different heterogeneous environments that get\nexecuted at data processing software in automated and semi automated modes. For\nevery new satellite being launched, the software is modified or upgraded if new\nbusiness processes are introduced. In this study, we propose a software\narchitecture that gives more flexible automation with very less manageable\ncode. The study also addresses utilization and extraction of useful information\nfrom historic production and customer details. A comparison of the current\nworkflow software architecture with existing practices in industry like Service\nOriented Architecture (SOA), Extensible Markup Languages (XML), and Event based\narchitectures has been made. A new hybrid approach based on the industry\npractices is proposed to improve the existing workflow.</p>"
  },
  {
    "title": "Blockchain-oriented Software Engineering: Challenges and New Directions",
    "date": "2017-02-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Simone Porru",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1702.05146v1"
    },
    "publicTags": [],
    "summary": "The Blockchain technology is reshaping finance, economy, money to the extent\nthat its disruptive power is compared to that of the Internet and the Web in\ntheir early days. As a result, all the software development revolving around\nthe Blockchain technology is growing at a staggering rate. In this paper, we\nacknowledge the need for software engineers to devise specialized tools and\ntechniques for blockchain-oriented software development. From current\nchallenges concerning the definition of new professional roles, demanding\ntesting activities and novel tools for software architecture, we take a step\nforward by proposing new directions on the basis of a curate corpus of\nblockchain-oriented software repositories, detected by exploiting the\ninformation enclosed in the 2016 Moody's Blockchain Report and teh market\ncapitalization of cryptocurrencies. Ensuring effective testing activities,\nenhancing collaboration in large teams, and facilitating the development of\nsmart contracts all appear as key factors in the future of blockchain-oriented\nsoftware development.",
    "sourceUrl": "http://arxiv.org/abs/1702.05146v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The Blockchain technology is reshaping finance, economy, money to the extent\nthat its disruptive power is compared to that of the Internet and the Web in\ntheir early days. As a result, all the software development revolving around\nthe Blockchain technology is growing at a staggering rate. In this paper, we\nacknowledge the need for software engineers to devise specialized tools and\ntechniques for blockchain-oriented software development. From current\nchallenges concerning the definition of new professional roles, demanding\ntesting activities and novel tools for software architecture, we take a step\nforward by proposing new directions on the basis of a curate corpus of\nblockchain-oriented software repositories, detected by exploiting the\ninformation enclosed in the 2016 Moody's Blockchain Report and teh market\ncapitalization of cryptocurrencies. Ensuring effective testing activities,\nenhancing collaboration in large teams, and facilitating the development of\nsmart contracts all appear as key factors in the future of blockchain-oriented\nsoftware development.</p>"
  },
  {
    "title": "Foundation Model Engineering: Engineering Foundation Models Just as\n  Engineering Software",
    "date": "2024-07-11",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Dezhi Ran",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2407.08176v1"
    },
    "publicTags": [],
    "summary": "By treating data and models as the source code, Foundation Models (FMs)\nbecome a new type of software. Mirroring the concept of software crisis, the\nincreasing complexity of FMs making FM crisis a tangible concern in the coming\ndecade, appealing for new theories and methodologies from the field of software\nengineering. In this paper, we outline our vision of introducing Foundation\nModel (FM) engineering, a strategic response to the anticipated FM crisis with\nprincipled engineering methodologies. FM engineering aims to mitigate potential\nissues in FM development and application through the introduction of\ndeclarative, automated, and unified programming interfaces for both data and\nmodel management, reducing the complexities involved in working with FMs by\nproviding a more structured and intuitive process for developers. Through the\nestablishment of FM engineering, we aim to provide a robust, automated, and\nextensible framework that addresses the imminent challenges, and discovering\nnew research opportunities for the software engineering field.",
    "sourceUrl": "http://arxiv.org/abs/2407.08176v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>By treating data and models as the source code, Foundation Models (FMs)\nbecome a new type of software. Mirroring the concept of software crisis, the\nincreasing complexity of FMs making FM crisis a tangible concern in the coming\ndecade, appealing for new theories and methodologies from the field of software\nengineering. In this paper, we outline our vision of introducing Foundation\nModel (FM) engineering, a strategic response to the anticipated FM crisis with\nprincipled engineering methodologies. FM engineering aims to mitigate potential\nissues in FM development and application through the introduction of\ndeclarative, automated, and unified programming interfaces for both data and\nmodel management, reducing the complexities involved in working with FMs by\nproviding a more structured and intuitive process for developers. Through the\nestablishment of FM engineering, we aim to provide a robust, automated, and\nextensible framework that addresses the imminent challenges, and discovering\nnew research opportunities for the software engineering field.</p>"
  },
  {
    "title": "Probabilistic Model Checking of DTMC Models of User Activity Patterns",
    "date": "2014-03-20",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Oana Andrei",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1403.6678v1"
    },
    "publicTags": [],
    "summary": "Software developers cannot always anticipate how users will actually use\ntheir software as it may vary from user to user, and even from use to use for\nan individual user. In order to address questions raised by system developers\nand evaluators about software usage, we define new probabilistic models that\ncharacterise user behaviour, based on activity patterns inferred from actual\nlogged user traces. We encode these new models in a probabilistic model checker\nand use probabilistic temporal logics to gain insight into software usage. We\nmotivate and illustrate our approach by application to the logged user traces\nof an iOS app.",
    "sourceUrl": "http://arxiv.org/abs/1403.6678v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software developers cannot always anticipate how users will actually use\ntheir software as it may vary from user to user, and even from use to use for\nan individual user. In order to address questions raised by system developers\nand evaluators about software usage, we define new probabilistic models that\ncharacterise user behaviour, based on activity patterns inferred from actual\nlogged user traces. We encode these new models in a probabilistic model checker\nand use probabilistic temporal logics to gain insight into software usage. We\nmotivate and illustrate our approach by application to the logged user traces\nof an iOS app.</p>"
  },
  {
    "title": "Introducing Traceability in GitHub for Medical Software Development",
    "date": "2021-10-25",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Vlad Stirbu",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2110.13034v1"
    },
    "publicTags": [],
    "summary": "Assuring traceability from requirements to implementation is a key element\nwhen developing safety critical software systems. Traditionally, this\ntraceability is ensured by a waterfall-like process, where phases follow each\nother, and tracing between different phases can be managed. However, new\nsoftware development paradigms, such as continuous software engineering and\nDevOps, which encourage a steady stream of new features, committed by\ndevelopers in a seemingly uncontrolled fashion in terms of former phasing,\nchallenge this view. In this paper, we introduce our approach that adds\ntraceability capabilities to GitHub, so that the developers can act like they\nnormally do in GitHub context but produce the documentation needed by the\nregulatory purposes in the process.",
    "sourceUrl": "http://arxiv.org/abs/2110.13034v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Assuring traceability from requirements to implementation is a key element\nwhen developing safety critical software systems. Traditionally, this\ntraceability is ensured by a waterfall-like process, where phases follow each\nother, and tracing between different phases can be managed. However, new\nsoftware development paradigms, such as continuous software engineering and\nDevOps, which encourage a steady stream of new features, committed by\ndevelopers in a seemingly uncontrolled fashion in terms of former phasing,\nchallenge this view. In this paper, we introduce our approach that adds\ntraceability capabilities to GitHub, so that the developers can act like they\nnormally do in GitHub context but produce the documentation needed by the\nregulatory purposes in the process.</p>"
  },
  {
    "title": "Recommending More Efficient Workflows to Software Developers",
    "date": "2021-02-06",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Dylan Bates",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2102.03670v1"
    },
    "publicTags": [],
    "summary": "Existing recommendation systems can help developers improve their software\ndevelopment abilities by recommending new programming tools, such as a\nrefactoring tool or a program navigation tool. However, simply recommending\ntools in isolation may not, in and of itself, allow developers to successfully\ncomplete their tasks. In this paper, I introduce a new recommendation system\nthat recommends workflows, or sequences of tools, to developers. By learning\nmore efficient workflows, the system could make software developers more\nefficient.",
    "sourceUrl": "http://arxiv.org/abs/2102.03670v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Existing recommendation systems can help developers improve their software\ndevelopment abilities by recommending new programming tools, such as a\nrefactoring tool or a program navigation tool. However, simply recommending\ntools in isolation may not, in and of itself, allow developers to successfully\ncomplete their tasks. In this paper, I introduce a new recommendation system\nthat recommends workflows, or sequences of tools, to developers. By learning\nmore efficient workflows, the system could make software developers more\nefficient.</p>"
  },
  {
    "title": "QBugs: A Collection of Reproducible Bugs in Quantum Algorithms and a\n  Supporting Infrastructure to Enable Controlled Quantum Software Testing and\n  Debugging Experiments",
    "date": "2021-03-31",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "JosÃ© Campos",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2103.16968v1"
    },
    "publicTags": [],
    "summary": "Reproducibility and comparability of empirical results are at the core tenet\nof the scientific method in any scientific field. To ease reproducibility of\nempirical studies, several benchmarks in software engineering research, such as\nDefects4J, have been developed and widely used. For quantum software\nengineering research, however, no benchmark has been established yet. In this\nposition paper, we propose a new benchmark -- named QBugs -- which will provide\nexperimental subjects and an experimental infrastructure to ease the evaluation\nof new research and the reproducibility of previously published results on\nquantum software engineering.",
    "sourceUrl": "http://arxiv.org/abs/2103.16968v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Reproducibility and comparability of empirical results are at the core tenet\nof the scientific method in any scientific field. To ease reproducibility of\nempirical studies, several benchmarks in software engineering research, such as\nDefects4J, have been developed and widely used. For quantum software\nengineering research, however, no benchmark has been established yet. In this\nposition paper, we propose a new benchmark -- named QBugs -- which will provide\nexperimental subjects and an experimental infrastructure to ease the evaluation\nof new research and the reproducibility of previously published results on\nquantum software engineering.</p>"
  },
  {
    "title": "A New Framework of Software Obfuscation Evaluation Criteria",
    "date": "2025-02-19",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Bjorn De Sutter",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2502.14093v1"
    },
    "publicTags": [],
    "summary": "In the domain of practical software protection against man-at-the-end attacks\nsuch as software reverse engineering and tampering, much of the scientific\nliterature is plagued by the use of subpar methods to evaluate the protections'\nstrength and even by the absence of such evaluations. Several criteria have\nbeen proposed in the past to assess the strength of protections, such as\npotency, resilience, stealth, and cost. We analyze their evolving definitions\nand uses. We formulate a number of critiques, from which we conclude that the\nexisting definitions are unsatisfactory and need to be revised. We present a\nnew framework of software protection evaluation criteria: relevance,\neffectiveness (or efficacy), robustness, concealment, stubbornness,\nsensitivity, predictability, and cost.",
    "sourceUrl": "http://arxiv.org/abs/2502.14093v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In the domain of practical software protection against man-at-the-end attacks\nsuch as software reverse engineering and tampering, much of the scientific\nliterature is plagued by the use of subpar methods to evaluate the protections'\nstrength and even by the absence of such evaluations. Several criteria have\nbeen proposed in the past to assess the strength of protections, such as\npotency, resilience, stealth, and cost. We analyze their evolving definitions\nand uses. We formulate a number of critiques, from which we conclude that the\nexisting definitions are unsatisfactory and need to be revised. We present a\nnew framework of software protection evaluation criteria: relevance,\neffectiveness (or efficacy), robustness, concealment, stubbornness,\nsensitivity, predictability, and cost.</p>"
  },
  {
    "title": "Don't mention it: An approach to assess challenges to using software\n  mentions for citation and discoverability research",
    "date": "2024-02-22",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Stephan Druskat",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2402.14602v1"
    },
    "publicTags": [],
    "summary": "Datasets collecting software mentions from scholarly publications can\npotentially be used for research into the software that has been used in the\npublished research, as well as into the practice of software citation.\nRecently, new software mention datasets with different characteristics have\nbeen published. We present an approach to assess the usability of such datasets\nfor research on research software. Our approach includes sampling and data\npreparation, manual annotation for quality and mention characteristics, and\nannotation analysis. We applied it to two software mention datasets for\nevaluation based on qualitative observation. Doing this, we were able to find\nchallenges to working with the selected datasets to do research. Main issues\nrefer to the structure of the dataset, the quality of the extracted mentions\n(54% and 23% of mentions respectively are not to software), and software\naccessibility. While one dataset does not provide links to mentioned software\nat all, the other does so in a way that can impede quantitative research\nendeavors: (1) Links may come from different sources and each point to\ndifferent software for the same mention. (2) The quality of the automatically\nretrieved links is generally poor (in our sample, 65.4% link the wrong\nsoftware). (3) Links exist only for a small subset (in our sample, 20.5%) of\nmentions, which may lead to skewed or disproportionate samples. However, the\ngreatest challenge and underlying issue in working with software mention\ndatasets is the still suboptimal practice of software citation: Software should\nnot be mentioned, it should be cited following the software citation\nprinciples.",
    "sourceUrl": "http://arxiv.org/abs/2402.14602v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Datasets collecting software mentions from scholarly publications can\npotentially be used for research into the software that has been used in the\npublished research, as well as into the practice of software citation.\nRecently, new software mention datasets with different characteristics have\nbeen published. We present an approach to assess the usability of such datasets\nfor research on research software. Our approach includes sampling and data\npreparation, manual annotation for quality and mention characteristics, and\nannotation analysis. We applied it to two software mention datasets for\nevaluation based on qualitative observation. Doing this, we were able to find\nchallenges to working with the selected datasets to do research. Main issues\nrefer to the structure of the dataset, the quality of the extracted mentions\n(54% and 23% of mentions respectively are not to software), and software\naccessibility. While one dataset does not provide links to mentioned software\nat all, the other does so in a way that can impede quantitative research\nendeavors: (1) Links may come from different sources and each point to\ndifferent software for the same mention. (2) The quality of the automatically\nretrieved links is generally poor (in our sample, 65.4% link the wrong\nsoftware). (3) Links exist only for a small subset (in our sample, 20.5%) of\nmentions, which may lead to skewed or disproportionate samples. However, the\ngreatest challenge and underlying issue in working with software mention\ndatasets is the still suboptimal practice of software citation: Software should\nnot be mentioned, it should be cited following the software citation\nprinciples.</p>"
  },
  {
    "title": "Using the Astrophysics Source Code Library: Find, cite, download, parse,\n  study, and submit",
    "date": "2022-12-24",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Alice Allen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2212.12682v1"
    },
    "publicTags": [],
    "summary": "The Astrophysics Source Code Library (ASCL) contains 3000 metadata records\nabout astrophysics research software and serves primarily as a registry of\nsoftware, though it also can and does accept code deposit. Though the ASCL was\nstarted in 1999, many astronomers, especially those new to the field, are not\nvery familiar with it. This hands-on virtual tutorial was geared to new users\nof the resource to teach them how to use the ASCL, with a focus on finding\nsoftware and information about software not only in this resource, but also by\nusing Google and NASA's Astrophysics Data System (ADS). With computational\nmethods so important to research, finding these methods is useful for examining\n(for transparency) and possibly reusing the software (for reproducibility or to\nenable new research). Metadata about software is useful for, for example,\nknowing how to cite software when it is used for research and studying trends\nin the computational landscape. Though the tutorial was primarily aimed at new\nusers, advanced users were also likely to learn something new.",
    "sourceUrl": "http://arxiv.org/abs/2212.12682v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The Astrophysics Source Code Library (ASCL) contains 3000 metadata records\nabout astrophysics research software and serves primarily as a registry of\nsoftware, though it also can and does accept code deposit. Though the ASCL was\nstarted in 1999, many astronomers, especially those new to the field, are not\nvery familiar with it. This hands-on virtual tutorial was geared to new users\nof the resource to teach them how to use the ASCL, with a focus on finding\nsoftware and information about software not only in this resource, but also by\nusing Google and NASA's Astrophysics Data System (ADS). With computational\nmethods so important to research, finding these methods is useful for examining\n(for transparency) and possibly reusing the software (for reproducibility or to\nenable new research). Metadata about software is useful for, for example,\nknowing how to cite software when it is used for research and studying trends\nin the computational landscape. Though the tutorial was primarily aimed at new\nusers, advanced users were also likely to learn something new.</p>"
  },
  {
    "title": "Comparing Soft Computing Techniques For Early Stage Software Development\n  Effort Estimations",
    "date": "2012-04-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Roheet Bhatnagar",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1204.6396v1"
    },
    "publicTags": [],
    "summary": "Accurately estimating the software size, cost, effort and schedule is\nprobably the biggest challenge facing software developers today. It has major\nimplications for the management of software development because both the\noverestimates and underestimates have direct impact for causing damage to\nsoftware companies. Lot of models have been proposed over the years by various\nresearchers for carrying out effort estimations. Also some of the studies for\nearly stage effort estimations suggest the importance of early estimations. New\nparadigms offer alternatives to estimate the software development effort, in\nparticular the Computational Intelligence (CI) that exploits mechanisms of\ninteraction between humans and processes domain knowledge with the intention of\nbuilding intelligent systems (IS). Among IS, Artificial Neural Network and\nFuzzy Logic are the two most popular soft computing techniques for software\ndevelopment effort estimation. In this paper neural network models and Mamdani\nFIS model have been used to predict the early stage effort estimations using\nthe student dataset. It has been found that Mamdani FIS was able to predict the\nearly stage efforts more efficiently in comparison to the neural network models\nbased models.",
    "sourceUrl": "http://arxiv.org/abs/1204.6396v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Accurately estimating the software size, cost, effort and schedule is\nprobably the biggest challenge facing software developers today. It has major\nimplications for the management of software development because both the\noverestimates and underestimates have direct impact for causing damage to\nsoftware companies. Lot of models have been proposed over the years by various\nresearchers for carrying out effort estimations. Also some of the studies for\nearly stage effort estimations suggest the importance of early estimations. New\nparadigms offer alternatives to estimate the software development effort, in\nparticular the Computational Intelligence (CI) that exploits mechanisms of\ninteraction between humans and processes domain knowledge with the intention of\nbuilding intelligent systems (IS). Among IS, Artificial Neural Network and\nFuzzy Logic are the two most popular soft computing techniques for software\ndevelopment effort estimation. In this paper neural network models and Mamdani\nFIS model have been used to predict the early stage effort estimations using\nthe student dataset. It has been found that Mamdani FIS was able to predict the\nearly stage efforts more efficiently in comparison to the neural network models\nbased models.</p>"
  },
  {
    "title": "Software Cloning in Extreme Programming Environment",
    "date": "2014-08-21",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ginika Mahajan",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1408.4899v1"
    },
    "publicTags": [],
    "summary": "Software systems are evolving by adding new functions and modifying existing\nfunctions over time. Through the evolution, the structure of software is\nbecoming more complex and so the understandability and maintainability of\nsoftware systems is deteriorating day by day. These are not only important but\none of the most expensive activities in software development. Refactoring has\noften been applied to the software to improve them. One of the targets of\nrefactoring is to limit Code Cloning because it hinders software maintenance\nand affects its quality. And in order to cope with the constant changes,\nrefactoring is seen as an essential component of Extreme Programming. Agile\nMethods use refactoring as important key practice and are first choice for\ndeveloping clone-free code. This paper summarizes my overview talk on software\ncloning analysis. It first discusses the notion of code cloning, types of\nclones, reasons, its consequences and analysis. It highlights Code Cloning in\nExtreme Programming Environment and finds Clone Detection as effective tool for\nRefactoring.",
    "sourceUrl": "http://arxiv.org/abs/1408.4899v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software systems are evolving by adding new functions and modifying existing\nfunctions over time. Through the evolution, the structure of software is\nbecoming more complex and so the understandability and maintainability of\nsoftware systems is deteriorating day by day. These are not only important but\none of the most expensive activities in software development. Refactoring has\noften been applied to the software to improve them. One of the targets of\nrefactoring is to limit Code Cloning because it hinders software maintenance\nand affects its quality. And in order to cope with the constant changes,\nrefactoring is seen as an essential component of Extreme Programming. Agile\nMethods use refactoring as important key practice and are first choice for\ndeveloping clone-free code. This paper summarizes my overview talk on software\ncloning analysis. It first discusses the notion of code cloning, types of\nclones, reasons, its consequences and analysis. It highlights Code Cloning in\nExtreme Programming Environment and finds Clone Detection as effective tool for\nRefactoring.</p>"
  },
  {
    "title": "Delta-oriented Architectural Variability Using MontiCore",
    "date": "2014-09-08",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Arne Haber",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1409.2317v1"
    },
    "publicTags": [],
    "summary": "Modeling of software architectures is a fundamental part of software\ndevelopment processes. Reuse of software components and early analysis of\nsoftware topologies allow the reduction of development costs and increases\nsoftware quality. Integrating variability modeling concepts into architecture\ndescription languages (ADLs) is essential for the development of diverse\nsoftware systems with high demands on software quality. In this paper, we\npresent the integration of delta modeling into the existing ADL MontiArc. Delta\nmodeling is a language-independent variability modeling approach supporting\nproactive, reactive and extractive product line development. We show how\n?-MontiArc, a language for explicit modeling of architectural variability based\non delta modeling, is implemented as domain-specific language (DSL) using the\nDSL development framework MontiCore. We also demonstrate how MontiCore's\nlanguage reuse mechanisms provide efficient means to derive an implementation\nof ?-MontiArc tool implementation. We evaluate ?-Monti-Arc by comparing it with\nannotative variability modeling.",
    "sourceUrl": "http://arxiv.org/abs/1409.2317v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Modeling of software architectures is a fundamental part of software\ndevelopment processes. Reuse of software components and early analysis of\nsoftware topologies allow the reduction of development costs and increases\nsoftware quality. Integrating variability modeling concepts into architecture\ndescription languages (ADLs) is essential for the development of diverse\nsoftware systems with high demands on software quality. In this paper, we\npresent the integration of delta modeling into the existing ADL MontiArc. Delta\nmodeling is a language-independent variability modeling approach supporting\nproactive, reactive and extractive product line development. We show how\n?-MontiArc, a language for explicit modeling of architectural variability based\non delta modeling, is implemented as domain-specific language (DSL) using the\nDSL development framework MontiCore. We also demonstrate how MontiCore's\nlanguage reuse mechanisms provide efficient means to derive an implementation\nof ?-MontiArc tool implementation. We evaluate ?-Monti-Arc by comparing it with\nannotative variability modeling.</p>"
  },
  {
    "title": "Identifying Talented Software Engineering Students through Data-driven\n  Skill Assessment",
    "date": "2014-11-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jun Lin",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1411.6197v1"
    },
    "publicTags": [],
    "summary": "For software development companies, one of the most important objectives is\nto identify and acquire talented software engineers in order to maintain a\nskilled team that can produce competitive products. Traditional approaches for\nfinding talented young software engineers are mainly through programming\ncontests of various forms which mostly test participants' programming skills.\nHowever, successful software engineering in practice requires a wider range of\nskills from team members including analysis, design, programming, testing,\ncommunication, collaboration, and self-management, etc. In this paper, we\nexplore potential ways to identify talented software engineering students in a\ndata-driven manner through an Agile Project Management (APM) platform. Through\nour proposed HASE online APM tool, we conducted a study involving 21 Scrum\nteams consisting of over 100 undergraduate software engineering students in\nmulti-week coursework projects in 2014. During this study, students performed\nover 10,000 ASD activities logged by HASE. We demonstrate the possibility and\npotentials of this new research direction, and discuss its implications for\nsoftware engineering education and industry recruitment.",
    "sourceUrl": "http://arxiv.org/abs/1411.6197v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>For software development companies, one of the most important objectives is\nto identify and acquire talented software engineers in order to maintain a\nskilled team that can produce competitive products. Traditional approaches for\nfinding talented young software engineers are mainly through programming\ncontests of various forms which mostly test participants' programming skills.\nHowever, successful software engineering in practice requires a wider range of\nskills from team members including analysis, design, programming, testing,\ncommunication, collaboration, and self-management, etc. In this paper, we\nexplore potential ways to identify talented software engineering students in a\ndata-driven manner through an Agile Project Management (APM) platform. Through\nour proposed HASE online APM tool, we conducted a study involving 21 Scrum\nteams consisting of over 100 undergraduate software engineering students in\nmulti-week coursework projects in 2014. During this study, students performed\nover 10,000 ASD activities logged by HASE. We demonstrate the possibility and\npotentials of this new research direction, and discuss its implications for\nsoftware engineering education and industry recruitment.</p>"
  },
  {
    "title": "Toward an effort estimation model for software projects integrating risk",
    "date": "2015-09-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "S Laqrichi",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1509.00602v1"
    },
    "publicTags": [],
    "summary": "According to a study of The Standish Group International, 44% of software\nprojects cost more and last longer than expected. More accurate the effort\nestimation is; the better the enterprise gets organized and the more the\nsoftware project respects the commitments on budget, time and quality.\nEnhancing the accuracy of effort estimation remains an ongoing challenge to\nsoftware professionals. Several factors can influence the accuracy of effort\nestimation, namely the immaterial aspect of information system projects, new\ntechnologies and the lack of return on experience. However, the most important\nfactor of cost and delay increase is software risks. A software risk is an\nuncertain event with a negative consequence on the software project. In this\narticle, we propose a methodology to take into account risk exposure analysis\nin the effort estimation model. In the literature, this issue is little\naddressed and few approaches are investigated. In this research work, we first\npresent an overview of these approaches and their limits. Then, we propose an\neffort estimation model that improves the accuracy of estimation by integrating\nsoftware risks. We finally apply this model to a case study and compare its\nresults to the results of a classic model.",
    "sourceUrl": "http://arxiv.org/abs/1509.00602v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>According to a study of The Standish Group International, 44% of software\nprojects cost more and last longer than expected. More accurate the effort\nestimation is; the better the enterprise gets organized and the more the\nsoftware project respects the commitments on budget, time and quality.\nEnhancing the accuracy of effort estimation remains an ongoing challenge to\nsoftware professionals. Several factors can influence the accuracy of effort\nestimation, namely the immaterial aspect of information system projects, new\ntechnologies and the lack of return on experience. However, the most important\nfactor of cost and delay increase is software risks. A software risk is an\nuncertain event with a negative consequence on the software project. In this\narticle, we propose a methodology to take into account risk exposure analysis\nin the effort estimation model. In the literature, this issue is little\naddressed and few approaches are investigated. In this research work, we first\npresent an overview of these approaches and their limits. Then, we propose an\neffort estimation model that improves the accuracy of estimation by integrating\nsoftware risks. We finally apply this model to a case study and compare its\nresults to the results of a classic model.</p>"
  },
  {
    "title": "Explainable Software Analytics",
    "date": "2018-02-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Hoa Khanh Dam",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1802.00603v1"
    },
    "publicTags": [],
    "summary": "Software analytics has been the subject of considerable recent attention but\nis yet to receive significant industry traction. One of the key reasons is that\nsoftware practitioners are reluctant to trust predictions produced by the\nanalytics machinery without understanding the rationale for those predictions.\nWhile complex models such as deep learning and ensemble methods improve\npredictive performance, they have limited explainability. In this paper, we\nargue that making software analytics models explainable to software\npractitioners is as \\emph{important} as achieving accurate predictions.\nExplainability should therefore be a key measure for evaluating software\nanalytics models. We envision that explainability will be a key driver for\ndeveloping software analytics models that are useful in practice. We outline a\nresearch roadmap for this space, building on social science, explainable\nartificial intelligence and software engineering.",
    "sourceUrl": "http://arxiv.org/abs/1802.00603v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software analytics has been the subject of considerable recent attention but\nis yet to receive significant industry traction. One of the key reasons is that\nsoftware practitioners are reluctant to trust predictions produced by the\nanalytics machinery without understanding the rationale for those predictions.\nWhile complex models such as deep learning and ensemble methods improve\npredictive performance, they have limited explainability. In this paper, we\nargue that making software analytics models explainable to software\npractitioners is as \\emph{important} as achieving accurate predictions.\nExplainability should therefore be a key measure for evaluating software\nanalytics models. We envision that explainability will be a key driver for\ndeveloping software analytics models that are useful in practice. We outline a\nresearch roadmap for this space, building on social science, explainable\nartificial intelligence and software engineering.</p>"
  },
  {
    "title": "Towards a Theory of Software Development Expertise",
    "date": "2018-07-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Sebastian Baltes",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1807.06087v4"
    },
    "publicTags": [],
    "summary": "Software development includes diverse tasks such as implementing new\nfeatures, analyzing requirements, and fixing bugs. Being an expert in those\ntasks requires a certain set of skills, knowledge, and experience. Several\nstudies investigated individual aspects of software development expertise, but\nwhat is missing is a comprehensive theory. We present a first conceptual theory\nof software development expertise that is grounded in data from a mixed-methods\nsurvey with 335 software developers and in literature on expertise and expert\nperformance. Our theory currently focuses on programming, but already provides\nvaluable insights for researchers, developers, and employers. The theory\ndescribes important properties of software development expertise and which\nfactors foster or hinder its formation, including how developers' performance\nmay decline over time. Moreover, our quantitative results show that developers'\nexpertise self-assessments are context-dependent and that experience is not\nnecessarily related to expertise.",
    "sourceUrl": "http://arxiv.org/abs/1807.06087v4",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software development includes diverse tasks such as implementing new\nfeatures, analyzing requirements, and fixing bugs. Being an expert in those\ntasks requires a certain set of skills, knowledge, and experience. Several\nstudies investigated individual aspects of software development expertise, but\nwhat is missing is a comprehensive theory. We present a first conceptual theory\nof software development expertise that is grounded in data from a mixed-methods\nsurvey with 335 software developers and in literature on expertise and expert\nperformance. Our theory currently focuses on programming, but already provides\nvaluable insights for researchers, developers, and employers. The theory\ndescribes important properties of software development expertise and which\nfactors foster or hinder its formation, including how developers' performance\nmay decline over time. Moreover, our quantitative results show that developers'\nexpertise self-assessments are context-dependent and that experience is not\nnecessarily related to expertise.</p>"
  },
  {
    "title": "Supporting Software Engineering Research and Education by Annotating\n  Public Videos of Developers Programming",
    "date": "2019-05-09",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Abdulaziz Alaboudi",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1905.11366v1"
    },
    "publicTags": [],
    "summary": "Software engineering has long studied how software developers work, building\na body of work which forms the foundation of many software engineering best\npractices, tools, and theories. Recently, some developers have begun recording\nvideos of themselves engaged in programming tasks contributing to open source\nprojects, enabling them to share knowledge and socialize with other developers.\nWe believe that these videos offer an important opportunity for both software\nengineering research and education. In this paper, we discuss the potential use\nof these videos as well as open questions for how to best enable this\nenvisioned use. We propose creating a central repository of programming videos,\nenabling analyzing and annotating videos to illustrate specific behaviors of\ninterest such as asking and answering questions, employing strategies, and\nsoftware engineering theories. Such a repository would offer an important new\nway in which both software engineering researchers and students can understand\nhow software developers work.",
    "sourceUrl": "http://arxiv.org/abs/1905.11366v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software engineering has long studied how software developers work, building\na body of work which forms the foundation of many software engineering best\npractices, tools, and theories. Recently, some developers have begun recording\nvideos of themselves engaged in programming tasks contributing to open source\nprojects, enabling them to share knowledge and socialize with other developers.\nWe believe that these videos offer an important opportunity for both software\nengineering research and education. In this paper, we discuss the potential use\nof these videos as well as open questions for how to best enable this\nenvisioned use. We propose creating a central repository of programming videos,\nenabling analyzing and annotating videos to illustrate specific behaviors of\ninterest such as asking and answering questions, employing strategies, and\nsoftware engineering theories. Such a repository would offer an important new\nway in which both software engineering researchers and students can understand\nhow software developers work.</p>"
  },
  {
    "title": "Design Level Metrics to Measure the Complexity Across Versions of AO\n  Software",
    "date": "2020-12-01",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Parthipan S",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2012.00276v1"
    },
    "publicTags": [],
    "summary": "Software metric plays a vital role in quantitative assessment of any specific\nsoftware development methodology and its impact on the maintenance of software.\nIt can also be used to indicate the degree of interdependence among the\ncomponents by providing valuable feedback about quality attributes such as\nmaintainability, modifiability and understandability. The effort for software\nmaintenance normally has a high correlation with the complexity of its design.\nAspect Oriented Software Design is an emerging methodology that provides\npowerful new techniques to improve the modularity of software from its design.\nIn this paper, evaluation model to capture the symptoms of complexity has been\ndefined consisting of metrics, artifacts and elements of complexity. A tool to\nautomatically capture these metrics across different versions of a case study\napplication, University Automation System has been developed. The values\nobtained for the proposed metrics are used to infer on the complexity of Java\nand AspectJ implementations of the case study application. These measurements\nindicate that AspectJ implementations are less complex compared to the Java\nimplementations and there by positively influencing the maintainability of\nsoftware.",
    "sourceUrl": "http://arxiv.org/abs/2012.00276v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software metric plays a vital role in quantitative assessment of any specific\nsoftware development methodology and its impact on the maintenance of software.\nIt can also be used to indicate the degree of interdependence among the\ncomponents by providing valuable feedback about quality attributes such as\nmaintainability, modifiability and understandability. The effort for software\nmaintenance normally has a high correlation with the complexity of its design.\nAspect Oriented Software Design is an emerging methodology that provides\npowerful new techniques to improve the modularity of software from its design.\nIn this paper, evaluation model to capture the symptoms of complexity has been\ndefined consisting of metrics, artifacts and elements of complexity. A tool to\nautomatically capture these metrics across different versions of a case study\napplication, University Automation System has been developed. The values\nobtained for the proposed metrics are used to infer on the complexity of Java\nand AspectJ implementations of the case study application. These measurements\nindicate that AspectJ implementations are less complex compared to the Java\nimplementations and there by positively influencing the maintainability of\nsoftware.</p>"
  },
  {
    "title": "MOBAFS: A Multi Objective Bee Algorithm for Feature subset selection in\n  Software Product Lines",
    "date": "2021-12-10",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Nahid Hajizadeh",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2112.05358v1"
    },
    "publicTags": [],
    "summary": "Software product line represents software engineering methods, tools and\ntechniques for creating a group of related software systems from a shared set\nof software assets. Each product is a combination of multiple features. These\nfeatures are known as software assets. So, the task of production can be mapped\nto a feature subset selection problem which is an NP-hard combinatorial\noptimization problem. This issue is much significant when the number of\nfeatures in a software product line is huge. In this paper, a new method based\non Multi Objective Bee Swarm Optimization algorithm (called MOBAFS) is\npresented. The MOBAFS is a population based optimization algorithm which is\ninspired by foraging behavior of honey bees. The is used to solve a SBSE\nproblem. This technique is evaluated on five large scale real world software\nproduct lines in the range of 1,244 to 6,888 features. The proposed method is\ncompared with the state-of-the-art, SATIBEA. According to results of three\nsolution quality indicators and two diversity metrics, the proposed method, in\nmost cases, surpasses the other algorithm.",
    "sourceUrl": "http://arxiv.org/abs/2112.05358v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software product line represents software engineering methods, tools and\ntechniques for creating a group of related software systems from a shared set\nof software assets. Each product is a combination of multiple features. These\nfeatures are known as software assets. So, the task of production can be mapped\nto a feature subset selection problem which is an NP-hard combinatorial\noptimization problem. This issue is much significant when the number of\nfeatures in a software product line is huge. In this paper, a new method based\non Multi Objective Bee Swarm Optimization algorithm (called MOBAFS) is\npresented. The MOBAFS is a population based optimization algorithm which is\ninspired by foraging behavior of honey bees. The is used to solve a SBSE\nproblem. This technique is evaluated on five large scale real world software\nproduct lines in the range of 1,244 to 6,888 features. The proposed method is\ncompared with the state-of-the-art, SATIBEA. According to results of three\nsolution quality indicators and two diversity metrics, the proposed method, in\nmost cases, surpasses the other algorithm.</p>"
  },
  {
    "title": "Cognitive Biases in Software Engineering: A Systematic Mapping Study",
    "date": "2017-07-12",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Rahul Mohanani",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1707.03869v3"
    },
    "publicTags": [],
    "summary": "One source of software project challenges and failures is the systematic\nerrors introduced by human cognitive biases. Although extensively explored in\ncognitive psychology, investigations concerning cognitive biases have only\nrecently gained popularity in software engineering (SE) research. This paper\ntherefore systematically maps, aggregates and synthesizes the literature on\ncognitive biases in software engineering to generate a comprehensive body of\nknowledge, understand state of the art research and provide guidelines for\nfuture research and practise. Focusing on bias antecedents, effects and\nmitigation techniques, we identified 65 articles, which investigate 37\ncognitive biases, published between 1990 and 2016. Despite strong and\nincreasing interest, the results reveal a scarcity of research on mitigation\ntechniques and poor theoretical foundations in understanding and interpreting\ncognitive biases. Although bias-related research has generated many new\ninsights in the software engineering community, specific bias mitigation\ntechniques are still needed for software professionals to overcome the\ndeleterious effects of cognitive biases on their work.",
    "sourceUrl": "http://arxiv.org/abs/1707.03869v3",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>One source of software project challenges and failures is the systematic\nerrors introduced by human cognitive biases. Although extensively explored in\ncognitive psychology, investigations concerning cognitive biases have only\nrecently gained popularity in software engineering (SE) research. This paper\ntherefore systematically maps, aggregates and synthesizes the literature on\ncognitive biases in software engineering to generate a comprehensive body of\nknowledge, understand state of the art research and provide guidelines for\nfuture research and practise. Focusing on bias antecedents, effects and\nmitigation techniques, we identified 65 articles, which investigate 37\ncognitive biases, published between 1990 and 2016. Despite strong and\nincreasing interest, the results reveal a scarcity of research on mitigation\ntechniques and poor theoretical foundations in understanding and interpreting\ncognitive biases. Although bias-related research has generated many new\ninsights in the software engineering community, specific bias mitigation\ntechniques are still needed for software professionals to overcome the\ndeleterious effects of cognitive biases on their work.</p>"
  },
  {
    "title": "JavaBERT: Training a transformer-based model for the Java programming\n  language",
    "date": "2021-10-20",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Nelson Tavares de Sousa",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2110.10404v1"
    },
    "publicTags": [],
    "summary": "Code quality is and will be a crucial factor while developing new software\ncode, requiring appropriate tools to ensure functional and reliable code.\nMachine learning techniques are still rarely used for software engineering\ntools, missing out the potential benefits of its application. Natural language\nprocessing has shown the potential to process text data regarding a variety of\ntasks. We argue, that such models can also show similar benefits for software\ncode processing. In this paper, we investigate how models used for natural\nlanguage processing can be trained upon software code. We introduce a data\nretrieval pipeline for software code and train a model upon Java software code.\nThe resulting model, JavaBERT, shows a high accuracy on the masked language\nmodeling task showing its potential for software engineering tools.",
    "sourceUrl": "http://arxiv.org/abs/2110.10404v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Code quality is and will be a crucial factor while developing new software\ncode, requiring appropriate tools to ensure functional and reliable code.\nMachine learning techniques are still rarely used for software engineering\ntools, missing out the potential benefits of its application. Natural language\nprocessing has shown the potential to process text data regarding a variety of\ntasks. We argue, that such models can also show similar benefits for software\ncode processing. In this paper, we investigate how models used for natural\nlanguage processing can be trained upon software code. We introduce a data\nretrieval pipeline for software code and train a model upon Java software code.\nThe resulting model, JavaBERT, shows a high accuracy on the masked language\nmodeling task showing its potential for software engineering tools.</p>"
  },
  {
    "title": "Programming the Universe: The First Commandment of Software Engineering\n  for all Varieties of Information Systems",
    "date": "2016-09-25",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Silvio Meira",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1609.07818v2"
    },
    "publicTags": [],
    "summary": "Since the early days of computers and programs, the process and outcomes of\nsoftware development has been a minefield plagued with problems and failures,\nas much as the complexity and complication of software and its development has\nincreased by a thousandfold in half a century. Over the years, a number of\ntheories, laws, best practices, manifestos and methodologies have emerged, with\nvaried degrees of (un)success. Our experience as software engineers of complex\nand large-scale systems shows that those guidelines are bound to previously\ndefined and often narrow scopes. Enough is enough. Nowadays, nearly every\ncompany is in the software and services business and everything is - or is\nmanaged by - software. It is about time, then, that the laws that govern our\nuniverse ought to be redefined. In this context, we discuss and present a set\nof universal laws that leads us to propose the first commandment of software\nengineering for all varieties of information systems.",
    "sourceUrl": "http://arxiv.org/abs/1609.07818v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Since the early days of computers and programs, the process and outcomes of\nsoftware development has been a minefield plagued with problems and failures,\nas much as the complexity and complication of software and its development has\nincreased by a thousandfold in half a century. Over the years, a number of\ntheories, laws, best practices, manifestos and methodologies have emerged, with\nvaried degrees of (un)success. Our experience as software engineers of complex\nand large-scale systems shows that those guidelines are bound to previously\ndefined and often narrow scopes. Enough is enough. Nowadays, nearly every\ncompany is in the software and services business and everything is - or is\nmanaged by - software. It is about time, then, that the laws that govern our\nuniverse ought to be redefined. In this context, we discuss and present a set\nof universal laws that leads us to propose the first commandment of software\nengineering for all varieties of information systems.</p>"
  },
  {
    "title": "Secure Software Development in the Era of Fluid Multi-party Open\n  Software and Services",
    "date": "2021-03-04",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ivan Pashchenko",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2103.03331v1"
    },
    "publicTags": [],
    "summary": "Pushed by market forces, software development has become fast-paced. As a\nconsequence, modern development projects are assembled from 3rd-party\ncomponents. Security & privacy assurance techniques once designed for large,\ncontrolled updates over months or years, must now cope with small, continuous\nchanges taking place within a week, and happening in sub-components that are\ncontrolled by third-party developers one might not even know they existed. In\nthis paper, we aim to provide an overview of the current software security\napproaches and evaluate their appropriateness in the face of the changed nature\nin software development. Software security assurance could benefit by switching\nfrom a process-based to an artefact-based approach. Further, security\nevaluation might need to be more incremental, automated and decentralized. We\nbelieve this can be achieved by supporting mechanisms for lightweight and\nscalable screenings that are applicable to the entire population of software\ncomponents albeit there might be a price to pay.",
    "sourceUrl": "http://arxiv.org/abs/2103.03331v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Pushed by market forces, software development has become fast-paced. As a\nconsequence, modern development projects are assembled from 3rd-party\ncomponents. Security & privacy assurance techniques once designed for large,\ncontrolled updates over months or years, must now cope with small, continuous\nchanges taking place within a week, and happening in sub-components that are\ncontrolled by third-party developers one might not even know they existed. In\nthis paper, we aim to provide an overview of the current software security\napproaches and evaluate their appropriateness in the face of the changed nature\nin software development. Software security assurance could benefit by switching\nfrom a process-based to an artefact-based approach. Further, security\nevaluation might need to be more incremental, automated and decentralized. We\nbelieve this can be achieved by supporting mechanisms for lightweight and\nscalable screenings that are applicable to the entire population of software\ncomponents albeit there might be a price to pay.</p>"
  },
  {
    "title": "Some Size and Structure Metrics for Quantum Software",
    "date": "2021-03-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jianjun Zhao",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2103.08815v1"
    },
    "publicTags": [],
    "summary": "Quantum software plays a critical role in exploiting the full potential of\nquantum computing systems. As a result, it is drawing increasing attention\nrecently. As research in quantum programming reaches maturity with a number of\nactive research and practical products, software metric researchers need to\nfocus on this new paradigm to evaluate it rigorously and quantitatively. As the\nfirst step, this paper proposes some basic metrics for quantum software, which\nmainly focus on measuring the size and structure of quantum software. These\nmetrics are defined at different abstraction levels to represent various size\nand structure attributes in quantum software explicitly. The proposed metrics\ncan be used to evaluate quantum software from various viewpoints.",
    "sourceUrl": "http://arxiv.org/abs/2103.08815v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Quantum software plays a critical role in exploiting the full potential of\nquantum computing systems. As a result, it is drawing increasing attention\nrecently. As research in quantum programming reaches maturity with a number of\nactive research and practical products, software metric researchers need to\nfocus on this new paradigm to evaluate it rigorously and quantitatively. As the\nfirst step, this paper proposes some basic metrics for quantum software, which\nmainly focus on measuring the size and structure of quantum software. These\nmetrics are defined at different abstraction levels to represent various size\nand structure attributes in quantum software explicitly. The proposed metrics\ncan be used to evaluate quantum software from various viewpoints.</p>"
  },
  {
    "title": "Technical Debts and Faults in Open-source Quantum Software Systems: An\n  Empirical Study",
    "date": "2022-06-01",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Moses Openja",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2206.00666v1"
    },
    "publicTags": [],
    "summary": "Quantum computing is a rapidly growing field attracting the interest of both\nresearchers and software developers. Supported by its numerous open-source\ntools, developers can now build, test, or run their quantum algorithms.\nAlthough the maintenance practices for traditional software systems have been\nextensively studied, the maintenance of quantum software is still a new field\nof study but a critical part to ensure the quality of a whole quantum computing\nsystem. In this work, we set out to investigate the distribution and evolution\nof technical debts in quantum software and their relationship with fault\noccurrences. Understanding these problems could guide future quantum\ndevelopment and provide maintenance recommendations for the key areas where\nquantum software developers and researchers should pay more attention. In this\npaper, we empirically studied 118 open-source quantum projects, which were\nselected from GitHub. The projects are categorized into 10 categories. We found\nthat the studied quantum software suffers from the issues of code convention\nviolation, error-handling, and code design. We also observed a statistically\nsignificant correlation between code design, redundant code or code convention,\nand the occurrences of faults in quantum software.",
    "sourceUrl": "http://arxiv.org/abs/2206.00666v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Quantum computing is a rapidly growing field attracting the interest of both\nresearchers and software developers. Supported by its numerous open-source\ntools, developers can now build, test, or run their quantum algorithms.\nAlthough the maintenance practices for traditional software systems have been\nextensively studied, the maintenance of quantum software is still a new field\nof study but a critical part to ensure the quality of a whole quantum computing\nsystem. In this work, we set out to investigate the distribution and evolution\nof technical debts in quantum software and their relationship with fault\noccurrences. Understanding these problems could guide future quantum\ndevelopment and provide maintenance recommendations for the key areas where\nquantum software developers and researchers should pay more attention. In this\npaper, we empirically studied 118 open-source quantum projects, which were\nselected from GitHub. The projects are categorized into 10 categories. We found\nthat the studied quantum software suffers from the issues of code convention\nviolation, error-handling, and code design. We also observed a statistically\nsignificant correlation between code design, redundant code or code convention,\nand the occurrences of faults in quantum software.</p>"
  },
  {
    "title": "MICOSE4aPS: Industrially Applicable Maturity Metric to Improve\n  Systematic Reuse of Control Software",
    "date": "2022-12-09",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Birgit Vogel-Heuser",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2212.09474v1"
    },
    "publicTags": [],
    "summary": "automated Production Systems (aPS) are highly complex, mechatronic systems\nthat usually have to operate reliably for many decades. Standardization and\nreuse of control software modules is a core prerequisite to achieve the\nrequired system quality in increasingly shorter development cycles. However,\nindustrial case studies in the field of aPS show that many aPS companies still\nstruggle with strategically reusing software. This paper proposes a\nmetric-based approach to objectively measure the maturity of industrial IEC\n61131-based control software in aPS (MICOSE4aPS) to identify potential\nweaknesses and quality issues hampering systematic reuse. Module developers in\nthe machine and plant manufacturing industry can directly benefit as the metric\ncalculation is integrated into the software engineering workflow. An in-depth\nindustrial evaluation in a top-ranked machine manufacturing company in food\npackaging and an expert evaluation with different companies confirmed the\nbenefit to efficiently manage the quality of control software.",
    "sourceUrl": "http://arxiv.org/abs/2212.09474v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>automated Production Systems (aPS) are highly complex, mechatronic systems\nthat usually have to operate reliably for many decades. Standardization and\nreuse of control software modules is a core prerequisite to achieve the\nrequired system quality in increasingly shorter development cycles. However,\nindustrial case studies in the field of aPS show that many aPS companies still\nstruggle with strategically reusing software. This paper proposes a\nmetric-based approach to objectively measure the maturity of industrial IEC\n61131-based control software in aPS (MICOSE4aPS) to identify potential\nweaknesses and quality issues hampering systematic reuse. Module developers in\nthe machine and plant manufacturing industry can directly benefit as the metric\ncalculation is integrated into the software engineering workflow. An in-depth\nindustrial evaluation in a top-ranked machine manufacturing company in food\npackaging and an expert evaluation with different companies confirmed the\nbenefit to efficiently manage the quality of control software.</p>"
  },
  {
    "title": "Quantum Software Engineering Challenges from Developers' Perspective:\n  Mapping Research Challenges to the Proposed Workflow Model",
    "date": "2023-08-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Majid Haghparast",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2308.01141v1"
    },
    "publicTags": [],
    "summary": "Despite the increasing interest in quantum computing, the aspect of\ndevelopment to achieve cost-effective and reliable quantum software\napplications has been slow. One barrier is the software engineering of quantum\nprograms, which can be approached from two directions. On the one hand, many\nsoftware engineering practices, debugging in particular, are bound to classical\ncomputing. On the other hand, quantum programming is closely associated with\nthe phenomena of quantum physics, and consequently, the way we express programs\nresembles the early days of programming. Moreover, much of the software\nengineering research today focuses on agile development, where computing cycles\nare cheap and new software can be rapidly deployed and tested, whereas in the\nquantum context, executions may consume lots of energy, and test runs may\nrequire lots of work to interpret. In this paper, we aim at bridging this gap\nby starting with the quantum computing workflow and by mapping existing\nsoftware engineering research to this workflow. Based on the mapping, we then\nidentify directions for software engineering research for quantum computing.",
    "sourceUrl": "http://arxiv.org/abs/2308.01141v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Despite the increasing interest in quantum computing, the aspect of\ndevelopment to achieve cost-effective and reliable quantum software\napplications has been slow. One barrier is the software engineering of quantum\nprograms, which can be approached from two directions. On the one hand, many\nsoftware engineering practices, debugging in particular, are bound to classical\ncomputing. On the other hand, quantum programming is closely associated with\nthe phenomena of quantum physics, and consequently, the way we express programs\nresembles the early days of programming. Moreover, much of the software\nengineering research today focuses on agile development, where computing cycles\nare cheap and new software can be rapidly deployed and tested, whereas in the\nquantum context, executions may consume lots of energy, and test runs may\nrequire lots of work to interpret. In this paper, we aim at bridging this gap\nby starting with the quantum computing workflow and by mapping existing\nsoftware engineering research to this workflow. Based on the mapping, we then\nidentify directions for software engineering research for quantum computing.</p>"
  },
  {
    "title": "AgileCoder: Dynamic Collaborative Agents for Software Development based\n  on Agile Methodology",
    "date": "2024-06-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Minh Huynh Nguyen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2406.11912v2"
    },
    "publicTags": [],
    "summary": "Software agents have emerged as promising tools for addressing complex\nsoftware engineering tasks. Existing works, on the other hand, frequently\noversimplify software development workflows, despite the fact that such\nworkflows are typically more complex in the real world. Thus, we propose\nAgileCoder, a multi agent system that integrates Agile Methodology (AM) into\nthe framework. This system assigns specific AM roles - such as Product Manager,\nDeveloper, and Tester to different agents, who then collaboratively develop\nsoftware based on user inputs. AgileCoder enhances development efficiency by\norganizing work into sprints, focusing on incrementally developing software\nthrough sprints. Additionally, we introduce Dynamic Code Graph Generator, a\nmodule that creates a Code Dependency Graph dynamically as updates are made to\nthe codebase. This allows agents to better comprehend the codebase, leading to\nmore precise code generation and modifications throughout the software\ndevelopment process. AgileCoder surpasses existing benchmarks, like ChatDev and\nMetaGPT, establishing a new standard and showcasing the capabilities of multi\nagent systems in advanced software engineering environments.",
    "sourceUrl": "http://arxiv.org/abs/2406.11912v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software agents have emerged as promising tools for addressing complex\nsoftware engineering tasks. Existing works, on the other hand, frequently\noversimplify software development workflows, despite the fact that such\nworkflows are typically more complex in the real world. Thus, we propose\nAgileCoder, a multi agent system that integrates Agile Methodology (AM) into\nthe framework. This system assigns specific AM roles - such as Product Manager,\nDeveloper, and Tester to different agents, who then collaboratively develop\nsoftware based on user inputs. AgileCoder enhances development efficiency by\norganizing work into sprints, focusing on incrementally developing software\nthrough sprints. Additionally, we introduce Dynamic Code Graph Generator, a\nmodule that creates a Code Dependency Graph dynamically as updates are made to\nthe codebase. This allows agents to better comprehend the codebase, leading to\nmore precise code generation and modifications throughout the software\ndevelopment process. AgileCoder surpasses existing benchmarks, like ChatDev and\nMetaGPT, establishing a new standard and showcasing the capabilities of multi\nagent systems in advanced software engineering environments.</p>"
  },
  {
    "title": "The Perceptions of Software Engineers Concerning the Utilization of Bots\n  in the OSS Development Process: An Exploratory Survey",
    "date": "2024-11-14",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Danyellias Vaz de Lima Manso",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2411.09467v1"
    },
    "publicTags": [],
    "summary": "Software bots, extensively adopted by Open Source Software (OSS) projects,\nsupport developers across several activities, from automating predefined tasks\nto generating code that aids software engineers. However, with the growing\nprominence of bots, questions have emerged regarding the extension to which\nthey truly assist or hinder software engineers in their routine tasks. To\naddress this, an exploratory survey was conducted with 37 software engineers to\ngather insights into their views on the use of bots within the software\ndevelopment process. The findings suggest that bots are present across multiple\nphases of the software development lifecycle, providing daily support to\nprofessionals by enhancing productivity and facilitating task automation.\nRespondents stated that current bots are not sufficiently intelligent and\nraised new challenges and enhancements to aid bot designers in developing\nadditional functionalities and integrations.",
    "sourceUrl": "http://arxiv.org/abs/2411.09467v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software bots, extensively adopted by Open Source Software (OSS) projects,\nsupport developers across several activities, from automating predefined tasks\nto generating code that aids software engineers. However, with the growing\nprominence of bots, questions have emerged regarding the extension to which\nthey truly assist or hinder software engineers in their routine tasks. To\naddress this, an exploratory survey was conducted with 37 software engineers to\ngather insights into their views on the use of bots within the software\ndevelopment process. The findings suggest that bots are present across multiple\nphases of the software development lifecycle, providing daily support to\nprofessionals by enhancing productivity and facilitating task automation.\nRespondents stated that current bots are not sufficiently intelligent and\nraised new challenges and enhancements to aid bot designers in developing\nadditional functionalities and integrations.</p>"
  },
  {
    "title": "Every Software as an Agent: Blueprint and Case Study",
    "date": "2025-02-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Mengwei Xu",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2502.04747v1"
    },
    "publicTags": [],
    "summary": "The rise of (multimodal) large language models (LLMs) has shed light on\nsoftware agent -- where software can understand and follow user instructions in\nnatural language. However, existing approaches such as API-based and GUI-based\nagents are far from satisfactory at accuracy and efficiency aspects. Instead,\nwe advocate to endow LLMs with access to the software internals (source code\nand runtime context) and the permission to dynamically inject generated code\ninto software for execution. In such a whitebox setting, one may better\nleverage the software context and the coding ability of LLMs. We then present\nan overall design architecture and case studies on two popular web-based\ndesktop applications. We also give in-depth discussion of the challenges and\nfuture directions. We deem that such a new paradigm has the potential to\nfundamentally overturn the existing software agent design, and finally creating\na digital world in which software can comprehend, operate, collaborate, and\neven think to meet complex user needs.",
    "sourceUrl": "http://arxiv.org/abs/2502.04747v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The rise of (multimodal) large language models (LLMs) has shed light on\nsoftware agent -- where software can understand and follow user instructions in\nnatural language. However, existing approaches such as API-based and GUI-based\nagents are far from satisfactory at accuracy and efficiency aspects. Instead,\nwe advocate to endow LLMs with access to the software internals (source code\nand runtime context) and the permission to dynamically inject generated code\ninto software for execution. In such a whitebox setting, one may better\nleverage the software context and the coding ability of LLMs. We then present\nan overall design architecture and case studies on two popular web-based\ndesktop applications. We also give in-depth discussion of the challenges and\nfuture directions. We deem that such a new paradigm has the potential to\nfundamentally overturn the existing software agent design, and finally creating\na digital world in which software can comprehend, operate, collaborate, and\neven think to meet complex user needs.</p>"
  },
  {
    "title": "Action Research Can Swing the Balance in Experimental Software\n  Engineering",
    "date": "2013-06-11",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Paulo Sergio Medeiros dos Santos",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1306.2414v1"
    },
    "publicTags": [],
    "summary": "In general, professionals still ignore scientific evidence in place of expert\nopinions in most of their decision-making. For this reason, it is still common\nto see the adoption of new software technologies in the field without any\nscientific basis or well-grounded criteria, but on the opinions of experts.\nExperimental Software Engineering is of paramount importance to provide the\nfoundations to understand the limits and applicability of software\ntechnologies. The need to better observe and understand the practice of\nSoftware Engineering leads us to look for alternative experimental approaches\nto support our studies. Different research strategies can be used to explore\ndifferent Software Engineering practices. Action Research can be seen as one\nalternative to intensify the conducting of important experimental studies with\nresults of great value while investigating the Software Engineering practices\nin depth. In this paper, a discussion on the use of Action Research in Software\nEngineering is presented. Aiming at better explaining the application of Action\nResearch, an experimental study (in vivo) on the investigation of the\nsubjective decisions of software developers, concerned with the refactoring of\nsource code to improve source code quality in a distributed software\ndevelopment context is depicted. In addition, some guidance on how to\naccomplish an Action Research study in Software Engineering supplement the\ndiscussions.",
    "sourceUrl": "http://arxiv.org/abs/1306.2414v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In general, professionals still ignore scientific evidence in place of expert\nopinions in most of their decision-making. For this reason, it is still common\nto see the adoption of new software technologies in the field without any\nscientific basis or well-grounded criteria, but on the opinions of experts.\nExperimental Software Engineering is of paramount importance to provide the\nfoundations to understand the limits and applicability of software\ntechnologies. The need to better observe and understand the practice of\nSoftware Engineering leads us to look for alternative experimental approaches\nto support our studies. Different research strategies can be used to explore\ndifferent Software Engineering practices. Action Research can be seen as one\nalternative to intensify the conducting of important experimental studies with\nresults of great value while investigating the Software Engineering practices\nin depth. In this paper, a discussion on the use of Action Research in Software\nEngineering is presented. Aiming at better explaining the application of Action\nResearch, an experimental study (in vivo) on the investigation of the\nsubjective decisions of software developers, concerned with the refactoring of\nsource code to improve source code quality in a distributed software\ndevelopment context is depicted. In addition, some guidance on how to\naccomplish an Action Research study in Software Engineering supplement the\ndiscussions.</p>"
  },
  {
    "title": "Descriptions of issues and comments for predicting issue success in\n  software projects",
    "date": "2020-06-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Sandra L. RamÃ­rez-Mora",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2006.01358v1"
    },
    "publicTags": [],
    "summary": "Software development tasks must be performed successfully to achieve software\nquality and customer satisfaction. Knowing whether software tasks are likely to\nfail is essential to ensure the success of software projects. Issue Tracking\nSystems store information of software tasks (issues) and comments, which can be\nuseful to predict issue success; however; almost no research on this topic\nexists. This work studies the usefulness of textual descriptions of issues and\ncomments for predicting whether issues will be resolved successfully or not.\nIssues and comments of 588 software projects were extracted from four popular\nIssue Tracking Systems. Seven machine learning classifiers were trained on 30k\nissues and more than 120k comments, and more than 6000 experiments were\nperformed to predict the success of three types of issues: bugs, improvements\nand new features. The results provided evidence that descriptions of issues and\ncomments are useful for predicting issue success with more than 85% of accuracy\nand precision, and that the predictions of issue success vary over time. Words\nrelated to software development were particularly relevant for predicting issue\nsuccess. Other communication aspects and their relationship to the success of\nsoftware projects must be researched in detail using data from software tools.",
    "sourceUrl": "http://arxiv.org/abs/2006.01358v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software development tasks must be performed successfully to achieve software\nquality and customer satisfaction. Knowing whether software tasks are likely to\nfail is essential to ensure the success of software projects. Issue Tracking\nSystems store information of software tasks (issues) and comments, which can be\nuseful to predict issue success; however; almost no research on this topic\nexists. This work studies the usefulness of textual descriptions of issues and\ncomments for predicting whether issues will be resolved successfully or not.\nIssues and comments of 588 software projects were extracted from four popular\nIssue Tracking Systems. Seven machine learning classifiers were trained on 30k\nissues and more than 120k comments, and more than 6000 experiments were\nperformed to predict the success of three types of issues: bugs, improvements\nand new features. The results provided evidence that descriptions of issues and\ncomments are useful for predicting issue success with more than 85% of accuracy\nand precision, and that the predictions of issue success vary over time. Words\nrelated to software development were particularly relevant for predicting issue\nsuccess. Other communication aspects and their relationship to the success of\nsoftware projects must be researched in detail using data from software tools.</p>"
  },
  {
    "title": "A Requirements Engineering Technology for the IoT Software Systems",
    "date": "2021-03-26",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Danyllo Valente da Silva",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2103.14348v1"
    },
    "publicTags": [],
    "summary": "Contemporary software systems (CSS), such as the internet of things (IoT)\nbased software systems, incorporate new concerns and characteristics inherent\nto the network, software, hardware, context awareness, interoperability, and\nothers, compared to conventional software systems. In this sense, requirements\nengineering (RE) plays a fundamental role in ensuring these software systems'\ncorrect development looking for the business and end-user needs. Several\nsoftware technologies supporting RE are available in the literature, but many\ndo not cover all CSS specificities, notably those based on IoT. This research\narticle presents RETIoT (Requirements Engineering Technology for the Internet\nof Things based software systems), aiming to provide methodological, technical,\nand tooling support to produce IoT software system requirements document. It is\ncomposed of an IoT scenario description technique, a checklist to verify IoT\nscenarios, construction processes, and templates for IoT software systems. A\nfeasibility study was carried out in IoT system projects to observe its\ntemplates and identify improvement opportunities. The results indicate the\nfeasibility of RETIoT templates' when used to capture IoT characteristics.\nHowever, further experimental studies represent research opportunities,\nstrengthen confidence in its elements (construction process, techniques, and\ntemplates), and capture end-user perception.",
    "sourceUrl": "http://arxiv.org/abs/2103.14348v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Contemporary software systems (CSS), such as the internet of things (IoT)\nbased software systems, incorporate new concerns and characteristics inherent\nto the network, software, hardware, context awareness, interoperability, and\nothers, compared to conventional software systems. In this sense, requirements\nengineering (RE) plays a fundamental role in ensuring these software systems'\ncorrect development looking for the business and end-user needs. Several\nsoftware technologies supporting RE are available in the literature, but many\ndo not cover all CSS specificities, notably those based on IoT. This research\narticle presents RETIoT (Requirements Engineering Technology for the Internet\nof Things based software systems), aiming to provide methodological, technical,\nand tooling support to produce IoT software system requirements document. It is\ncomposed of an IoT scenario description technique, a checklist to verify IoT\nscenarios, construction processes, and templates for IoT software systems. A\nfeasibility study was carried out in IoT system projects to observe its\ntemplates and identify improvement opportunities. The results indicate the\nfeasibility of RETIoT templates' when used to capture IoT characteristics.\nHowever, further experimental studies represent research opportunities,\nstrengthen confidence in its elements (construction process, techniques, and\ntemplates), and capture end-user perception.</p>"
  },
  {
    "title": "Do Internal Software Metrics Have Relationship with Fault-proneness and\n  Change-proneness?",
    "date": "2023-09-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Md. Masudur Rahman",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2310.03673v2"
    },
    "publicTags": [],
    "summary": "Fault-proneness is a measure that indicates the possibility of programming\nerrors occurring within a software system. On the other hand, change-proneness\nrefers to the potential for modifications to be made to the software. Both of\nthese measures are crucial indicators of software maintainability, as they\ninfluence internal software metrics such as size, inheritance, and coupling,\nparticularly when numerous changes are made to the system. In the literature,\nresearch has predicted change- and fault-proneness using internal software\nmetrics that is almost a decade old. However, given the continuous evolution of\nsoftware systems, it is essential to revisit and update our understanding of\nthese relationships. Therefore, we have conducted an empirical study to revisit\nthe relationship between internal software metrics and change-proneness, and\nfaultproneness, aiming to provide current and relevant insights. In our study,\nwe identified 25 internal software metrics along with the measures of\nchange-proneness and fault-proneness within the wellknown open-source systems\nfrom the Apache and Eclipse ecosystems. We then analyzed the relationships\nbetween these metrics using statistical correlation methods. Our results\nrevealed that most of the metrics have little to no correlation with\nfault-proneness. However, metrics related to inheritance, coupling, and\ncomments showed a moderate to high correlation with change-proneness. These\nfindings will assist developers to minimize the higher correlated software\nmetrics to enhance maintainability in terms of change- and fault-proneness.\nAdditionally, these insights can guide researchers in developing new approaches\nfor predicting changes and faults by incorporating the metrics that have been\nshown to have stronger correlations.",
    "sourceUrl": "http://arxiv.org/abs/2310.03673v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Fault-proneness is a measure that indicates the possibility of programming\nerrors occurring within a software system. On the other hand, change-proneness\nrefers to the potential for modifications to be made to the software. Both of\nthese measures are crucial indicators of software maintainability, as they\ninfluence internal software metrics such as size, inheritance, and coupling,\nparticularly when numerous changes are made to the system. In the literature,\nresearch has predicted change- and fault-proneness using internal software\nmetrics that is almost a decade old. However, given the continuous evolution of\nsoftware systems, it is essential to revisit and update our understanding of\nthese relationships. Therefore, we have conducted an empirical study to revisit\nthe relationship between internal software metrics and change-proneness, and\nfaultproneness, aiming to provide current and relevant insights. In our study,\nwe identified 25 internal software metrics along with the measures of\nchange-proneness and fault-proneness within the wellknown open-source systems\nfrom the Apache and Eclipse ecosystems. We then analyzed the relationships\nbetween these metrics using statistical correlation methods. Our results\nrevealed that most of the metrics have little to no correlation with\nfault-proneness. However, metrics related to inheritance, coupling, and\ncomments showed a moderate to high correlation with change-proneness. These\nfindings will assist developers to minimize the higher correlated software\nmetrics to enhance maintainability in terms of change- and fault-proneness.\nAdditionally, these insights can guide researchers in developing new approaches\nfor predicting changes and faults by incorporating the metrics that have been\nshown to have stronger correlations.</p>"
  },
  {
    "title": "Software for Microscopy Workshop White Paper",
    "date": "2020-04-30",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Raghav Chhetri",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2005.00082v1"
    },
    "publicTags": [],
    "summary": "Microscopes have morphed from purely optical instruments into motorized,\nrobotic machines that form images on digital sensors rather than eyeballs. This\ncontinuing trend towards automation and digitization enables many new\napproaches to microscopy that would have been impossible or impractical without\ncomputer interfaces. Accordingly, todays development of new microscopes most\noften depends on concurrent software development to bring these custom-build\nsystems to life. This dependence on software brings opportunities and\nchallenges. Most importantly, a key challenge while developing new microscopes\nis to develop the appropriate software. Despite the fact that software is\neasily copied and distributed, remarkably few opportunities are available to\nshare experiences creating microscope control software. In turn, this brings\nchallenges in creating maintainable and flexible software code and writing User\nInterfaces (UIs) that are easily used by researchers, who are primarily life\nscientists. To start to address these challenges by identifying common problems\nand shared solutions, we assembled a small group of researchers that develop or\nuse software to control their custom-build microscopes at the Janelia Research\nCampus for a two-day workshop in February 2020. The outcome of the workshop was\nthe definition of clear milestones, as well as the recognition of an involved\ncommunity, much larger than the one assembled at the workshop. This community\nencounters similar hurdles and shares a great desire to overcome these by\nstronger, community-wide collaborations on Open Source Software. This White\nPaper summarizes the major issues identified, proposes approaches to address\nthese as a community, and outlines the next steps that can be taken to develop\na framework facilitating shared microscope software development, significantly\nspeeding up development of new microscopy systems.",
    "sourceUrl": "http://arxiv.org/abs/2005.00082v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Microscopes have morphed from purely optical instruments into motorized,\nrobotic machines that form images on digital sensors rather than eyeballs. This\ncontinuing trend towards automation and digitization enables many new\napproaches to microscopy that would have been impossible or impractical without\ncomputer interfaces. Accordingly, todays development of new microscopes most\noften depends on concurrent software development to bring these custom-build\nsystems to life. This dependence on software brings opportunities and\nchallenges. Most importantly, a key challenge while developing new microscopes\nis to develop the appropriate software. Despite the fact that software is\neasily copied and distributed, remarkably few opportunities are available to\nshare experiences creating microscope control software. In turn, this brings\nchallenges in creating maintainable and flexible software code and writing User\nInterfaces (UIs) that are easily used by researchers, who are primarily life\nscientists. To start to address these challenges by identifying common problems\nand shared solutions, we assembled a small group of researchers that develop or\nuse software to control their custom-build microscopes at the Janelia Research\nCampus for a two-day workshop in February 2020. The outcome of the workshop was\nthe definition of clear milestones, as well as the recognition of an involved\ncommunity, much larger than the one assembled at the workshop. This community\nencounters similar hurdles and shares a great desire to overcome these by\nstronger, community-wide collaborations on Open Source Software. This White\nPaper summarizes the major issues identified, proposes approaches to address\nthese as a community, and outlines the next steps that can be taken to develop\na framework facilitating shared microscope software development, significantly\nspeeding up development of new microscopy systems.</p>"
  },
  {
    "title": "Effects of Visualizing Technical Debts on a Software Maintenance Project",
    "date": "2019-11-18",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ronivon Dias",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1911.07565v1"
    },
    "publicTags": [],
    "summary": "The technical debt (TD) metaphor is widely used to encapsulate numerous\nsoftware quality problems. She describes the trade-off between the short term\nbenefit of taking a shortcut during the design or implementation phase of a\nsoftware product (for example, in order to meet a deadline) and the long term\nconsequences of taking said shortcut, which may affect the quality of the\nsoftware product. TDs must be managed to guarantee the software quality and\nalso reduce its maintenance and evolution costs. However, the tools for TD\ndetection usually provide results only considering the files perspective (class\nand methods), that is not usual during the project management. In this work, a\ntechnique is proposed to identify/visualize TD on a new perspective: software\nfeatures. The proposed technique adopts Mining Software Repository (MRS) tools\nto identify the software features and after the technical debts that affect\nthese features. Additionally, we also proposed an approach to support\nmaintenance tasks guided by TD visualization at the feature level aiming to\nevaluate its applicability on real software projects. The results indicate that\nthe approach can be useful to decrease the existent TDs, as well as avoid the\nintroduction of new TDs.",
    "sourceUrl": "http://arxiv.org/abs/1911.07565v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The technical debt (TD) metaphor is widely used to encapsulate numerous\nsoftware quality problems. She describes the trade-off between the short term\nbenefit of taking a shortcut during the design or implementation phase of a\nsoftware product (for example, in order to meet a deadline) and the long term\nconsequences of taking said shortcut, which may affect the quality of the\nsoftware product. TDs must be managed to guarantee the software quality and\nalso reduce its maintenance and evolution costs. However, the tools for TD\ndetection usually provide results only considering the files perspective (class\nand methods), that is not usual during the project management. In this work, a\ntechnique is proposed to identify/visualize TD on a new perspective: software\nfeatures. The proposed technique adopts Mining Software Repository (MRS) tools\nto identify the software features and after the technical debts that affect\nthese features. Additionally, we also proposed an approach to support\nmaintenance tasks guided by TD visualization at the feature level aiming to\nevaluate its applicability on real software projects. The results indicate that\nthe approach can be useful to decrease the existent TDs, as well as avoid the\nintroduction of new TDs.</p>"
  },
  {
    "title": "Metamorphic Testing: A New Approach for Generating Next Test Cases",
    "date": "2020-02-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "T. Y. Chen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2002.12543v1"
    },
    "publicTags": [],
    "summary": "In software testing, a set of test cases is constructed according to some\npredefined selection criteria. The software is then examined against these test\ncases. Three interesting observations have been made on the current artifacts\nof software testing. Firstly, an error-revealing test case is considered useful\nwhile a successful test case which does not reveal software errors is usually\nnot further investigated. Whether these successful test cases still contain\nuseful information for revealing software errors has not been properly studied.\nSecondly, no matter how extensive the testing has been conducted in the\ndevelopment phase, errors may still exist in the software [5]. These errors, if\nleft undetected, may eventually cause damage to the production system. The\nstudy of techniques for uncovering software errors in the production phase is\nseldom addressed in the literature. Thirdly, as indicated by Weyuker in [6],\nthe availability of test oracles is pragmatically unattainable in most\nsituations. However, the availability of test oracles is generally assumed in\nconventional software testing techniques. In this paper, we propose a novel\ntest case selection technique that derives new test cases from the successful\nones. The selection aims at revealing software errors that are possibly left\nundetected in successful test cases which may be generated using some existing\nstrategies. As such, the proposed technique augments the effectiveness of\nexisting test selection strategies. The technique also helps uncover software\nerrors in the production phase and can be used in the absence of test oracles.",
    "sourceUrl": "http://arxiv.org/abs/2002.12543v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In software testing, a set of test cases is constructed according to some\npredefined selection criteria. The software is then examined against these test\ncases. Three interesting observations have been made on the current artifacts\nof software testing. Firstly, an error-revealing test case is considered useful\nwhile a successful test case which does not reveal software errors is usually\nnot further investigated. Whether these successful test cases still contain\nuseful information for revealing software errors has not been properly studied.\nSecondly, no matter how extensive the testing has been conducted in the\ndevelopment phase, errors may still exist in the software [5]. These errors, if\nleft undetected, may eventually cause damage to the production system. The\nstudy of techniques for uncovering software errors in the production phase is\nseldom addressed in the literature. Thirdly, as indicated by Weyuker in [6],\nthe availability of test oracles is pragmatically unattainable in most\nsituations. However, the availability of test oracles is generally assumed in\nconventional software testing techniques. In this paper, we propose a novel\ntest case selection technique that derives new test cases from the successful\nones. The selection aims at revealing software errors that are possibly left\nundetected in successful test cases which may be generated using some existing\nstrategies. As such, the proposed technique augments the effectiveness of\nexisting test selection strategies. The technique also helps uncover software\nerrors in the production phase and can be used in the absence of test oracles.</p>"
  },
  {
    "title": "A Survey-Based Qualitative Study to Characterize Expectations of\n  Software Developers from Five Stakeholders",
    "date": "2021-07-20",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Khalid Hasan",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2107.09587v1"
    },
    "publicTags": [],
    "summary": "Background: Studies on developer productivity and well-being find that the\nperceptions of productivity in a software team can be a socio-technical\nproblem. Intuitively, problems and challenges can be better handled by managing\nexpectations in software teams. Aim: Our goal is to understand whether the\nexpectations of software developers vary towards diverse stakeholders in\nsoftware teams. Method: We surveyed 181 professional software developers to\nunderstand their expectations from five different stakeholders: (1)\norganizations, (2) managers, (3) peers, (4) new hires, and (5) government and\neducational institutions. The five stakeholders are determined by conducting\nsemi-formal interviews of software developers. We ask open-ended survey\nquestions and analyze the responses using open coding. Results: We observed 18\nmulti-faceted expectations types. While some expectations are more specific to\na stakeholder, other expectations are cross-cutting. For example, developers\nexpect work-benefits from their organizations, but expect the adoption of\nstandard software engineering (SE) practices from their organizations, peers,\nand new hires. Conclusion: Out of the 18 categories, three categories are\nrelated to career growth. This observation supports previous research that\nhappiness cannot be assured by simply offering more money or a promotion. Among\nthe most number of responses, we find expectations from educational\ninstitutions to offer relevant teaching and from governments to improve job\nstability, which indicate the increasingly important roles of these\norganizations to help software developers. This observation can be especially\ntrue during the COVID-19 pandemic.",
    "sourceUrl": "http://arxiv.org/abs/2107.09587v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Background: Studies on developer productivity and well-being find that the\nperceptions of productivity in a software team can be a socio-technical\nproblem. Intuitively, problems and challenges can be better handled by managing\nexpectations in software teams. Aim: Our goal is to understand whether the\nexpectations of software developers vary towards diverse stakeholders in\nsoftware teams. Method: We surveyed 181 professional software developers to\nunderstand their expectations from five different stakeholders: (1)\norganizations, (2) managers, (3) peers, (4) new hires, and (5) government and\neducational institutions. The five stakeholders are determined by conducting\nsemi-formal interviews of software developers. We ask open-ended survey\nquestions and analyze the responses using open coding. Results: We observed 18\nmulti-faceted expectations types. While some expectations are more specific to\na stakeholder, other expectations are cross-cutting. For example, developers\nexpect work-benefits from their organizations, but expect the adoption of\nstandard software engineering (SE) practices from their organizations, peers,\nand new hires. Conclusion: Out of the 18 categories, three categories are\nrelated to career growth. This observation supports previous research that\nhappiness cannot be assured by simply offering more money or a promotion. Among\nthe most number of responses, we find expectations from educational\ninstitutions to offer relevant teaching and from governments to improve job\nstability, which indicate the increasingly important roles of these\norganizations to help software developers. This observation can be especially\ntrue during the COVID-19 pandemic.</p>"
  },
  {
    "title": "Software Architecture for Quantum Computing Systems -- A Systematic\n  Review",
    "date": "2022-02-11",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Arif Ali Khan",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2202.05505v4"
    },
    "publicTags": [],
    "summary": "Quantum computing systems rely on the principles of quantum mechanics to\nperform a multitude of computationally challenging tasks more efficiently than\ntheir classical counterparts. The architecture of software-intensive systems\ncan empower architects who can leverage architecture-centric processes,\npractices, description languages, etc., to model, develop, and evolve quantum\ncomputing software (quantum software for short) at higher abstraction levels.\nWe conducted a systematic literature review (SLR) to investigate (i)\narchitectural process, (ii) modeling notations, (iii) architecture design\npatterns, (iv) tool support, and (iv) challenging factors for quantum software\narchitecture. Results of the SLR indicate that quantum software represents a\nnew genre of software-intensive systems; however, existing processes and\nnotations can be tailored to derive the architecting activities and develop\nmodeling languages for quantum software. Quantum bits (Qubits) mapped to\nQuantum gates (Qugates) can be represented as architectural components and\nconnectors that implement quantum software. Tool-chains can incorporate\nreusable knowledge and human roles (e.g., quantum domain engineers, quantum\ncode developers) to automate and customize the architectural process. Results\nof this SLR can facilitate researchers and practitioners to develop new\nhypotheses to be tested, derive reference architectures, and leverage\narchitecture-centric principles and practices to engineer emerging and next\ngenerations of quantum software.",
    "sourceUrl": "http://arxiv.org/abs/2202.05505v4",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Quantum computing systems rely on the principles of quantum mechanics to\nperform a multitude of computationally challenging tasks more efficiently than\ntheir classical counterparts. The architecture of software-intensive systems\ncan empower architects who can leverage architecture-centric processes,\npractices, description languages, etc., to model, develop, and evolve quantum\ncomputing software (quantum software for short) at higher abstraction levels.\nWe conducted a systematic literature review (SLR) to investigate (i)\narchitectural process, (ii) modeling notations, (iii) architecture design\npatterns, (iv) tool support, and (iv) challenging factors for quantum software\narchitecture. Results of the SLR indicate that quantum software represents a\nnew genre of software-intensive systems; however, existing processes and\nnotations can be tailored to derive the architecting activities and develop\nmodeling languages for quantum software. Quantum bits (Qubits) mapped to\nQuantum gates (Qugates) can be represented as architectural components and\nconnectors that implement quantum software. Tool-chains can incorporate\nreusable knowledge and human roles (e.g., quantum domain engineers, quantum\ncode developers) to automate and customize the architectural process. Results\nof this SLR can facilitate researchers and practitioners to develop new\nhypotheses to be tested, derive reference architectures, and leverage\narchitecture-centric principles and practices to engineer emerging and next\ngenerations of quantum software.</p>"
  },
  {
    "title": "A Research Software Engineering Workflow for Computational Science and\n  Engineering",
    "date": "2022-08-15",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Tomislav Maric",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2208.07460v1"
    },
    "publicTags": [],
    "summary": "University research groups in Computational Science and Engineering (CSE)\ngenerally lack dedicated funding and personnel for Research Software\nEngineering (RSE), which, combined with the pressure to maximize the number of\nscientific publications, shifts the focus away from sustainable research\nsoftware development and reproducible results. The neglect of RSE in CSE at\nUniversity research groups negatively impacts the scientific output: research\ndata - including research software - related to a CSE publication cannot be\nfound, reproduced, or re-used, different ideas are not combined easily into new\nideas, and published methods must very often be re-implemented to be\ninvestigated further. This slows down CSE research significantly, resulting in\nconsiderable losses in time and, consequentially, public funding.\n  We propose a RSE workflow for Computational Science and Engineering (CSE)\nthat addresses these challenges, that improves the quality of research output\nin CSE. Our workflow applies established software engineering practices adapted\nfor CSE: software testing, result visualization, and periodical cross-linking\nof software with reports/publications and data, timed by milestones in the\nscientific publication process. The workflow introduces minimal work overhead,\ncrucial for university research groups, and delivers modular and tested\nsoftware linked to publications whose results can easily be reproduced. We\ndefine research software quality from a perspective of a pragmatic researcher:\nthe ability to quickly find the publication, data, and software related to a\npublished research idea, quickly reproduce results, understand or re-use a CSE\nmethod, and finally extend the method with new research ideas.",
    "sourceUrl": "http://arxiv.org/abs/2208.07460v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>University research groups in Computational Science and Engineering (CSE)\ngenerally lack dedicated funding and personnel for Research Software\nEngineering (RSE), which, combined with the pressure to maximize the number of\nscientific publications, shifts the focus away from sustainable research\nsoftware development and reproducible results. The neglect of RSE in CSE at\nUniversity research groups negatively impacts the scientific output: research\ndata - including research software - related to a CSE publication cannot be\nfound, reproduced, or re-used, different ideas are not combined easily into new\nideas, and published methods must very often be re-implemented to be\ninvestigated further. This slows down CSE research significantly, resulting in\nconsiderable losses in time and, consequentially, public funding.\n  We propose a RSE workflow for Computational Science and Engineering (CSE)\nthat addresses these challenges, that improves the quality of research output\nin CSE. Our workflow applies established software engineering practices adapted\nfor CSE: software testing, result visualization, and periodical cross-linking\nof software with reports/publications and data, timed by milestones in the\nscientific publication process. The workflow introduces minimal work overhead,\ncrucial for university research groups, and delivers modular and tested\nsoftware linked to publications whose results can easily be reproduced. We\ndefine research software quality from a perspective of a pragmatic researcher:\nthe ability to quickly find the publication, data, and software related to a\npublished research idea, quickly reproduce results, understand or re-use a CSE\nmethod, and finally extend the method with new research ideas.</p>"
  },
  {
    "title": "A pragmatic workflow for research software engineering in computational\n  science",
    "date": "2023-10-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Tomislav MariÄ",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2310.00960v1"
    },
    "publicTags": [],
    "summary": "University research groups in Computational Science and Engineering (CSE)\ngenerally lack dedicated funding and personnel for Research Software\nEngineering (RSE), which, combined with the pressure to maximize the number of\nscientific publications, shifts the focus away from sustainable research\nsoftware development and reproducible results. The neglect of RSE in CSE at\nUniversity research groups negatively impacts the scientific output: research\ndata - including research software - related to a CSE publication cannot be\nfound, reproduced, or re-used, different ideas are not combined easily into new\nideas, and published methods must very often be re-implemented to be\ninvestigated further. This slows down CSE research significantly, resulting in\nconsiderable losses in time and, consequentially, public funding.\n  We propose a RSE workflow for Computational Science and Engineering (CSE)\nthat addresses these challenges, that improves the quality of research output\nin CSE. Our workflow applies established software engineering practices adapted\nfor CSE: software testing, result visualization, and periodical cross-linking\nof software with reports/publications and data, timed by milestones in the\nscientific publication process. The workflow introduces minimal work overhead,\ncrucial for university research groups, and delivers modular and tested\nsoftware linked to publications whose results can easily be reproduced. We\ndefine research software quality from a perspective of a pragmatic researcher:\nthe ability to quickly find the publication, data, and software related to a\npublished research idea, quickly reproduce results, understand or re-use a CSE\nmethod, and finally extend the method with new research ideas.",
    "sourceUrl": "http://arxiv.org/abs/2310.00960v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>University research groups in Computational Science and Engineering (CSE)\ngenerally lack dedicated funding and personnel for Research Software\nEngineering (RSE), which, combined with the pressure to maximize the number of\nscientific publications, shifts the focus away from sustainable research\nsoftware development and reproducible results. The neglect of RSE in CSE at\nUniversity research groups negatively impacts the scientific output: research\ndata - including research software - related to a CSE publication cannot be\nfound, reproduced, or re-used, different ideas are not combined easily into new\nideas, and published methods must very often be re-implemented to be\ninvestigated further. This slows down CSE research significantly, resulting in\nconsiderable losses in time and, consequentially, public funding.\n  We propose a RSE workflow for Computational Science and Engineering (CSE)\nthat addresses these challenges, that improves the quality of research output\nin CSE. Our workflow applies established software engineering practices adapted\nfor CSE: software testing, result visualization, and periodical cross-linking\nof software with reports/publications and data, timed by milestones in the\nscientific publication process. The workflow introduces minimal work overhead,\ncrucial for university research groups, and delivers modular and tested\nsoftware linked to publications whose results can easily be reproduced. We\ndefine research software quality from a perspective of a pragmatic researcher:\nthe ability to quickly find the publication, data, and software related to a\npublished research idea, quickly reproduce results, understand or re-use a CSE\nmethod, and finally extend the method with new research ideas.</p>"
  },
  {
    "title": "Contemporary Software Modernization: Perspectives and Challenges to Deal\n  with Legacy Systems",
    "date": "2024-07-04",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Wesley K. G. AssunÃ§Ã£o",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2407.04017v1"
    },
    "publicTags": [],
    "summary": "Software modernization is an inherent activity of software engineering, as\ntechnology advances and systems inevitably become outdated. The term \"software\nmodernization\" emerged as a research topic in the early 2000s, with a\ndifferentiation from traditional software evolution. Studies on this topic\nbecame popular due to new programming paradigms, technologies, and\narchitectural styles. Given the pervasive nature of software today, modernizing\nlegacy systems is paramount to provide users with competitive and innovative\nproducts and services. Despite the large amount of work available in the\nliterature, there are significant limitations: (i) proposed approaches are\nstrictly specific to one scenario or technology, lacking flexibility; (ii) most\nof the proposed approaches are not aligned with the current modern software\ndevelopment scenario; and (iii) due to a myriad of proposed modernization\napproaches, practitioners may be misguided on how to modernize legacies. In\nthis work, our goal is to call attention to the need for advances in research\nand practices toward a well-defined software modernization domain. The focus is\non enabling organizations to preserve the knowledge represented in legacy\nsystems while taking advantages of disruptive and emerging technologies. Based\non this goal, we put the different perspectives of software modernization in\nthe context of contemporary software development. We also present a research\nagenda with 10 challenges to motivate new studies.",
    "sourceUrl": "http://arxiv.org/abs/2407.04017v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software modernization is an inherent activity of software engineering, as\ntechnology advances and systems inevitably become outdated. The term \"software\nmodernization\" emerged as a research topic in the early 2000s, with a\ndifferentiation from traditional software evolution. Studies on this topic\nbecame popular due to new programming paradigms, technologies, and\narchitectural styles. Given the pervasive nature of software today, modernizing\nlegacy systems is paramount to provide users with competitive and innovative\nproducts and services. Despite the large amount of work available in the\nliterature, there are significant limitations: (i) proposed approaches are\nstrictly specific to one scenario or technology, lacking flexibility; (ii) most\nof the proposed approaches are not aligned with the current modern software\ndevelopment scenario; and (iii) due to a myriad of proposed modernization\napproaches, practitioners may be misguided on how to modernize legacies. In\nthis work, our goal is to call attention to the need for advances in research\nand practices toward a well-defined software modernization domain. The focus is\non enabling organizations to preserve the knowledge represented in legacy\nsystems while taking advantages of disruptive and emerging technologies. Based\non this goal, we put the different perspectives of software modernization in\nthe context of contemporary software development. We also present a research\nagenda with 10 challenges to motivate new studies.</p>"
  },
  {
    "title": "Detecting Vulnerabilities in Encrypted Software Code while Ensuring Code\n  Privacy",
    "date": "2025-01-15",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jorge Martins",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2501.09191v1"
    },
    "publicTags": [],
    "summary": "Software vulnerabilities continue to be the main cause of occurrence for\ncyber attacks. In an attempt to reduce them and improve software quality,\nsoftware code analysis has emerged as a service offered by companies\nspecialising in software testing. However, this service requires software\ncompanies to provide access to their software's code, which raises concerns\nabout code privacy and intellectual property theft. This paper presents a novel\napproach to Software Quality and Privacy, in which testing companies can\nperform code analysis tasks on encrypted software code provided by software\ncompanies while code privacy is preserved. The approach combines Static Code\nAnalysis and Searchable Symmetric Encryption in order to process the source\ncode and build an encrypted inverted index that represents its data and control\nflows. The index is then used to discover vulnerabilities by carrying out\nstatic analysis tasks in a confidential way. With this approach, this paper\nalso defines a new research field -- Confidential Code Analysis --, from which\nother types of code analysis tasks and approaches can be derived. We\nimplemented the approach in a new tool called CoCoA and evaluated it\nexperimentally with synthetic and real PHP web applications. The results show\nthat the tool has similar precision as standard (non-confidential) static\nanalysis tools and a modest average performance overhead of 42.7%.",
    "sourceUrl": "http://arxiv.org/abs/2501.09191v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software vulnerabilities continue to be the main cause of occurrence for\ncyber attacks. In an attempt to reduce them and improve software quality,\nsoftware code analysis has emerged as a service offered by companies\nspecialising in software testing. However, this service requires software\ncompanies to provide access to their software's code, which raises concerns\nabout code privacy and intellectual property theft. This paper presents a novel\napproach to Software Quality and Privacy, in which testing companies can\nperform code analysis tasks on encrypted software code provided by software\ncompanies while code privacy is preserved. The approach combines Static Code\nAnalysis and Searchable Symmetric Encryption in order to process the source\ncode and build an encrypted inverted index that represents its data and control\nflows. The index is then used to discover vulnerabilities by carrying out\nstatic analysis tasks in a confidential way. With this approach, this paper\nalso defines a new research field -- Confidential Code Analysis --, from which\nother types of code analysis tasks and approaches can be derived. We\nimplemented the approach in a new tool called CoCoA and evaluated it\nexperimentally with synthetic and real PHP web applications. The results show\nthat the tool has similar precision as standard (non-confidential) static\nanalysis tools and a modest average performance overhead of 42.7%.</p>"
  },
  {
    "title": "Innovating the software engineering class through multi-team development",
    "date": "2025-02-04",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Allan Brockenbrough",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2502.02578v1"
    },
    "publicTags": [],
    "summary": "Often software engineering classes have the student concentrate on designing\nand planning the project but stop short of actual student team development of\ncode. This leads to criticism by employers of new graduates that they are\nmissing skills in working in teams and coordinating multiple overlapping\nchanges to a code base. Additionally, students that are not actively\nexperiencing team development are unprepared to understand and modify existing\nlegacy-code bases written by others. This paper presents a new approach to\nteaching undergraduate software engineering that emphasizes not only software\nengineering methodology but also experiencing development as a member of a team\nand modifying a legacy code base. Our innovative software engineering course\nbegins with learning the fundamentals of software engineering, followed by\nexamining an existing framework of a social media application. The students are\nthen grouped into multiple software teams, each focusing on a different aspect\nof the app. The separate teams must define requirements, design, and provide\ndocumentation on the services. Using an Agile development approach, the teams\nincrementally add to the code base and demonstrate features as the application\nevolves. Subsequent iterations of the class pick up the prior students code\nbase, providing experience working with a legacy code base. Preliminary results\nof using this approach at the university are presented in this paper including\nquantitative analysis. Analysis of student software submissions to the\ncloud-based code repository shows student engagement and contributions over the\nspan of the course. Positive student evaluations show the effectiveness of\napplying the principles of software engineering to the development of a complex\nsolution in a team environment. Keywords: Software engineering, teaching,\ncollege computer science, innovative methods, agile.",
    "sourceUrl": "http://arxiv.org/abs/2502.02578v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Often software engineering classes have the student concentrate on designing\nand planning the project but stop short of actual student team development of\ncode. This leads to criticism by employers of new graduates that they are\nmissing skills in working in teams and coordinating multiple overlapping\nchanges to a code base. Additionally, students that are not actively\nexperiencing team development are unprepared to understand and modify existing\nlegacy-code bases written by others. This paper presents a new approach to\nteaching undergraduate software engineering that emphasizes not only software\nengineering methodology but also experiencing development as a member of a team\nand modifying a legacy code base. Our innovative software engineering course\nbegins with learning the fundamentals of software engineering, followed by\nexamining an existing framework of a social media application. The students are\nthen grouped into multiple software teams, each focusing on a different aspect\nof the app. The separate teams must define requirements, design, and provide\ndocumentation on the services. Using an Agile development approach, the teams\nincrementally add to the code base and demonstrate features as the application\nevolves. Subsequent iterations of the class pick up the prior students code\nbase, providing experience working with a legacy code base. Preliminary results\nof using this approach at the university are presented in this paper including\nquantitative analysis. Analysis of student software submissions to the\ncloud-based code repository shows student engagement and contributions over the\nspan of the course. Positive student evaluations show the effectiveness of\napplying the principles of software engineering to the development of a complex\nsolution in a team environment. Keywords: Software engineering, teaching,\ncollege computer science, innovative methods, agile.</p>"
  },
  {
    "title": "Survey on software testing techniques in cloud computing",
    "date": "2014-02-09",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Priyadharshini. V",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1402.1925v1"
    },
    "publicTags": [],
    "summary": "Cloud computing is the next stage of the internet evolution. It relies on\nsharing of resources to achieve coherence on a network. It is emerged as new\ncomputing standard that impacts several different research fields, including\nsoftware testing. There are various software techniques used for testing\napplication. It not only changes the way of obtaining computing resources but\nalso changes the way of managing and delivering computing services,\ntechnologies and solutions, meanwhile it causes new issues, challenges and\nneeds in software testing. Software testing in cloud can reduce the need for\nhardware and software resources and offer a flexible and efficient alternative\nto the traditional software testing process. This paper provides an overview\nregarding trends, oppurtunities, challenges, issues, and needs in cloud testing\nand cloud based application.",
    "sourceUrl": "http://arxiv.org/abs/1402.1925v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Cloud computing is the next stage of the internet evolution. It relies on\nsharing of resources to achieve coherence on a network. It is emerged as new\ncomputing standard that impacts several different research fields, including\nsoftware testing. There are various software techniques used for testing\napplication. It not only changes the way of obtaining computing resources but\nalso changes the way of managing and delivering computing services,\ntechnologies and solutions, meanwhile it causes new issues, challenges and\nneeds in software testing. Software testing in cloud can reduce the need for\nhardware and software resources and offer a flexible and efficient alternative\nto the traditional software testing process. This paper provides an overview\nregarding trends, oppurtunities, challenges, issues, and needs in cloud testing\nand cloud based application.</p>"
  },
  {
    "title": "The First 50 Years of Software Reliability Engineering: A History of SRE\n  with First Person Accounts",
    "date": "2019-02-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "James J. Cusick",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1902.06140v2"
    },
    "publicTags": [],
    "summary": "Software Reliability has just passed the 50-year milestone as a technical\ndiscipline along with Software Engineering. This paper traces the roots of\nSoftware Reliability Engineering (SRE) from its pre-software history to the\nbeginnings of the field with the first software reliability model in 1967\nthrough its maturation in the 1980s to the current challenges in proving\napplication reliability on smartphones and in other areas. This history began\nas a thesis proposal for a History of Science research program and includes\nmultiple previously unpublished interviews with founders of the field. The\nproject evolved to also provide a survey of the development of SRE from notable\nprior histories and from citations of new work in the field including\nreliability applications to Agile Methods. This history concludes at the\nmodern-day providing bookends in the theory, models, literature, and practice\nof Software Reliability Engineering from 1968 to 2018 and pointing towards new\nopportunities to deepen and broaden the field.",
    "sourceUrl": "http://arxiv.org/abs/1902.06140v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software Reliability has just passed the 50-year milestone as a technical\ndiscipline along with Software Engineering. This paper traces the roots of\nSoftware Reliability Engineering (SRE) from its pre-software history to the\nbeginnings of the field with the first software reliability model in 1967\nthrough its maturation in the 1980s to the current challenges in proving\napplication reliability on smartphones and in other areas. This history began\nas a thesis proposal for a History of Science research program and includes\nmultiple previously unpublished interviews with founders of the field. The\nproject evolved to also provide a survey of the development of SRE from notable\nprior histories and from citations of new work in the field including\nreliability applications to Agile Methods. This history concludes at the\nmodern-day providing bookends in the theory, models, literature, and practice\nof Software Reliability Engineering from 1968 to 2018 and pointing towards new\nopportunities to deepen and broaden the field.</p>"
  },
  {
    "title": "Predicting Software Reliability in Softwarized Networks",
    "date": "2024-07-30",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Hasan Yagiz Ozkan",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2407.21224v1"
    },
    "publicTags": [],
    "summary": "Providing high quality software and evaluating the software reliability in\nsoftwarized networks are crucial for vendors and customers. These networks rely\non open source code, which are sensitive to contain high number of bugs. Both,\nthe knowledge about the code of previous releases as well as the bug history of\nthe particular project can be used to evaluate the software reliability of a\nnew software release based on SRGM. In this work a framework to predict the\nnumber of the bugs of a new release, as well as other reliability parameters,\nis proposed. An exemplary implementation of this framework to two particular\nopen source projects, is described in detail. The difference between the\nprediction accuracy of the two projects is presented. Different alternatives to\nincrease the prediction accuracy are proposed and compared in this paper.",
    "sourceUrl": "http://arxiv.org/abs/2407.21224v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Providing high quality software and evaluating the software reliability in\nsoftwarized networks are crucial for vendors and customers. These networks rely\non open source code, which are sensitive to contain high number of bugs. Both,\nthe knowledge about the code of previous releases as well as the bug history of\nthe particular project can be used to evaluate the software reliability of a\nnew software release based on SRGM. In this work a framework to predict the\nnumber of the bugs of a new release, as well as other reliability parameters,\nis proposed. An exemplary implementation of this framework to two particular\nopen source projects, is described in detail. The difference between the\nprediction accuracy of the two projects is presented. Different alternatives to\nincrease the prediction accuracy are proposed and compared in this paper.</p>"
  },
  {
    "title": "Requirements engineering current practice and capability in small and\n  medium software development enterprises in New Zealand",
    "date": "2014-07-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Alison Talbot",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1407.6102v1"
    },
    "publicTags": [],
    "summary": "This paper presents research on current industry practices with respect to\nrequirements engineering as implemented within software development companies\nin New Zealand. A survey instrument is designed and deployed. The results are\nanalysed and compared against what is internationally considered \"best\npractice\" and previous New Zealand and Australian studies. An attempt is made\nto assess the requirements engineering capability of New Zealand companies\nusing both formal and informal frameworks.",
    "sourceUrl": "http://arxiv.org/abs/1407.6102v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>This paper presents research on current industry practices with respect to\nrequirements engineering as implemented within software development companies\nin New Zealand. A survey instrument is designed and deployed. The results are\nanalysed and compared against what is internationally considered \"best\npractice\" and previous New Zealand and Australian studies. An attempt is made\nto assess the requirements engineering capability of New Zealand companies\nusing both formal and informal frameworks.</p>"
  },
  {
    "title": "The history and future prospects of open data and open source software",
    "date": "2021-08-03",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Feras A. Batarseh",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2108.01592v1"
    },
    "publicTags": [],
    "summary": "Open data for all New Yorkers is the tagline on New York City's open data\nwebsite. Open government is being promoted at most countries of the western\nworld. Government transparency levels are being measured by the amount of data\nthey share through their online public repositories. Additionally, open source\nsoftware is promoted at governments, academia, and the industry. This is the\nnew digital story of this century, and the new testament between the Gods of\ntechnology and there users. Data and software openness will redefine the path\nforward and aim to rekindle our collective intelligence. Data and software\nopenness can redefine Data Democracy and be the catalyst for its progress. This\nchapter provides a historical insight into data and software openness, the\nbeginnings, the heroes, prospects for the future, and all things we cannot\nafford to negotiate or lose.",
    "sourceUrl": "http://arxiv.org/abs/2108.01592v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Open data for all New Yorkers is the tagline on New York City's open data\nwebsite. Open government is being promoted at most countries of the western\nworld. Government transparency levels are being measured by the amount of data\nthey share through their online public repositories. Additionally, open source\nsoftware is promoted at governments, academia, and the industry. This is the\nnew digital story of this century, and the new testament between the Gods of\ntechnology and there users. Data and software openness will redefine the path\nforward and aim to rekindle our collective intelligence. Data and software\nopenness can redefine Data Democracy and be the catalyst for its progress. This\nchapter provides a historical insight into data and software openness, the\nbeginnings, the heroes, prospects for the future, and all things we cannot\nafford to negotiate or lose.</p>"
  },
  {
    "title": "Proof Driven Development",
    "date": "2015-12-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ben Goodspeed",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1512.02102v1"
    },
    "publicTags": [],
    "summary": "A new workflow for software development (proof-driven development) is\npresented. An extension of test-driven development, the new workflow utilizes\nthe paradigm of dependently typed programming. The differences in design,\ncomplexity and provability of software are discussed, based on the technique\nused to create the system. Furthermore, the difference in what properties can\nbe expressed in a proof-driven development workflow versus a traditional\ntest-driven development workflow or using test-last development.",
    "sourceUrl": "http://arxiv.org/abs/1512.02102v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>A new workflow for software development (proof-driven development) is\npresented. An extension of test-driven development, the new workflow utilizes\nthe paradigm of dependently typed programming. The differences in design,\ncomplexity and provability of software are discussed, based on the technique\nused to create the system. Furthermore, the difference in what properties can\nbe expressed in a proof-driven development workflow versus a traditional\ntest-driven development workflow or using test-last development.</p>"
  },
  {
    "title": "Demonstration of a Response Time Based Remaining Useful Life (RUL)\n  Prediction for Software Systems",
    "date": "2023-07-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ray Islam",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2307.12237v1"
    },
    "publicTags": [],
    "summary": "Prognostic and Health Management (PHM) has been widely applied to hardware\nsystems in the electronics and non-electronics domains but has not been\nexplored for software. While software does not decay over time, it can degrade\nover release cycles. Software health management is confined to diagnostic\nassessments that identify problems, whereas prognostic assessment potentially\nindicates when in the future a problem will become detrimental. Relevant\nresearch areas such as software defect prediction, software reliability\nprediction, predictive maintenance of software, software degradation, and\nsoftware performance prediction, exist, but all of these represent diagnostic\nmodels built upon historical data, none of which can predict an RUL for\nsoftware. This paper addresses the application of PHM concepts to software\nsystems for fault predictions and RUL estimation. Specifically, this paper\naddresses how PHM can be used to make decisions for software systems such as\nversion update and upgrade, module changes, system reengineering, rejuvenation,\nmaintenance scheduling, budgeting, and total abandonment. This paper presents a\nmethod to prognostically and continuously predict the RUL of a software system\nbased on usage parameters (e.g., the numbers and categories of releases) and\nperformance parameters (e.g., response time). The model developed has been\nvalidated by comparing actual data, with the results that were generated by\npredictive models. Statistical validation (regression validation, and k-fold\ncross validation) has also been carried out. A case study, based on publicly\navailable data for the Bugzilla application is presented. This case study\ndemonstrates that PHM concepts can be applied to software systems and RUL can\nbe calculated to make system management decisions.",
    "sourceUrl": "http://arxiv.org/abs/2307.12237v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Prognostic and Health Management (PHM) has been widely applied to hardware\nsystems in the electronics and non-electronics domains but has not been\nexplored for software. While software does not decay over time, it can degrade\nover release cycles. Software health management is confined to diagnostic\nassessments that identify problems, whereas prognostic assessment potentially\nindicates when in the future a problem will become detrimental. Relevant\nresearch areas such as software defect prediction, software reliability\nprediction, predictive maintenance of software, software degradation, and\nsoftware performance prediction, exist, but all of these represent diagnostic\nmodels built upon historical data, none of which can predict an RUL for\nsoftware. This paper addresses the application of PHM concepts to software\nsystems for fault predictions and RUL estimation. Specifically, this paper\naddresses how PHM can be used to make decisions for software systems such as\nversion update and upgrade, module changes, system reengineering, rejuvenation,\nmaintenance scheduling, budgeting, and total abandonment. This paper presents a\nmethod to prognostically and continuously predict the RUL of a software system\nbased on usage parameters (e.g., the numbers and categories of releases) and\nperformance parameters (e.g., response time). The model developed has been\nvalidated by comparing actual data, with the results that were generated by\npredictive models. Statistical validation (regression validation, and k-fold\ncross validation) has also been carried out. A case study, based on publicly\navailable data for the Bugzilla application is presented. This case study\ndemonstrates that PHM concepts can be applied to software systems and RUL can\nbe calculated to make system management decisions.</p>"
  },
  {
    "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
    "date": "2023-10-03",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jenny T. Liang",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2310.01727v3"
    },
    "publicTags": [],
    "summary": "Empirical software engineering research on production systems has brought\nforth a better understanding of the software engineering process for\npractitioners and researchers alike. However, only a small subset of production\nsystems is studied, limiting the impact of this research. While software\nengineering practitioners could benefit from replicating research on their own\ndata, this poses its own set of challenges, since performing replications\nrequires a deep understanding of research methodologies and subtle nuances in\nsoftware engineering data. Given that large language models (LLMs), such as\nGPT-4, show promise in tackling both software engineering- and science-related\ntasks, these models could help replicate and thus democratize empirical\nsoftware engineering research.\n  In this paper, we examine GPT-4's abilities to perform replications of\nempirical software engineering research on new data. We study their ability to\nsurface assumptions made in empirical software engineering research\nmethodologies, as well as their ability to plan and generate code for analysis\npipelines on seven empirical software engineering papers. We perform a user\nstudy with 14 participants with software engineering research expertise, who\nevaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module\nspecifications) from the papers. We find that GPT-4 is able to surface correct\nassumptions, but struggles to generate ones that apply common knowledge about\nsoftware engineering data. In a manual analysis of the generated code, we find\nthat the GPT-4-generated code contains correct high-level logic, given a subset\nof the methodology. However, the code contains many small implementation-level\nerrors, reflecting a lack of software engineering knowledge. Our findings have\nimplications for leveraging LLMs for software engineering research as well as\npractitioner data scientists in software teams.",
    "sourceUrl": "http://arxiv.org/abs/2310.01727v3",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Empirical software engineering research on production systems has brought\nforth a better understanding of the software engineering process for\npractitioners and researchers alike. However, only a small subset of production\nsystems is studied, limiting the impact of this research. While software\nengineering practitioners could benefit from replicating research on their own\ndata, this poses its own set of challenges, since performing replications\nrequires a deep understanding of research methodologies and subtle nuances in\nsoftware engineering data. Given that large language models (LLMs), such as\nGPT-4, show promise in tackling both software engineering- and science-related\ntasks, these models could help replicate and thus democratize empirical\nsoftware engineering research.\n  In this paper, we examine GPT-4's abilities to perform replications of\nempirical software engineering research on new data. We study their ability to\nsurface assumptions made in empirical software engineering research\nmethodologies, as well as their ability to plan and generate code for analysis\npipelines on seven empirical software engineering papers. We perform a user\nstudy with 14 participants with software engineering research expertise, who\nevaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module\nspecifications) from the papers. We find that GPT-4 is able to surface correct\nassumptions, but struggles to generate ones that apply common knowledge about\nsoftware engineering data. In a manual analysis of the generated code, we find\nthat the GPT-4-generated code contains correct high-level logic, given a subset\nof the methodology. However, the code contains many small implementation-level\nerrors, reflecting a lack of software engineering knowledge. Our findings have\nimplications for leveraging LLMs for software engineering research as well as\npractitioner data scientists in software teams.</p>"
  },
  {
    "title": "Pattern-Oriented Analysis and Design (POAD) Theory",
    "date": "2008-02-26",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jerry Overton",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/0802.3784v1"
    },
    "publicTags": [],
    "summary": "Pattern-Oriented Analysis and Design (POAD) is the practice of building\ncomplex software by applying proven designs to specific problem domains.\nAlthough a great deal of research and practice has been devoted to formalizing\nexisting design patterns and discovering new ones, there has been relatively\nlittle research into methods for combining these patterns into software\napplications. This is partly because the creation of complex software\napplications is so expensive. This paper proposes a mathematical model of POAD\nthat may allow future research in pattern-oriented techniques to be performed\nusing less expensive formal techniques rather than expensive, complex software\ndevelopment.",
    "sourceUrl": "http://arxiv.org/abs/0802.3784v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Pattern-Oriented Analysis and Design (POAD) is the practice of building\ncomplex software by applying proven designs to specific problem domains.\nAlthough a great deal of research and practice has been devoted to formalizing\nexisting design patterns and discovering new ones, there has been relatively\nlittle research into methods for combining these patterns into software\napplications. This is partly because the creation of complex software\napplications is so expensive. This paper proposes a mathematical model of POAD\nthat may allow future research in pattern-oriented techniques to be performed\nusing less expensive formal techniques rather than expensive, complex software\ndevelopment.</p>"
  },
  {
    "title": "On verification of software components",
    "date": "2012-10-14",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Basem Y. Alkazemi",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1210.3758v1"
    },
    "publicTags": [],
    "summary": "Utilizing third party software components in the development of new systems\nbecame somewhat unfavourable approach among many organizations nowadays. This\nreluctance is primarily built due to the lack of support to verify the quality\nattributes of software components in order to avoid potential mismatches with\nsystems requirements. This paper presents an approach to overcome this problem\nby providing a tool support to check component compatibility to a specification\nprovided by developers. So, components compatibility can be checked and\ndevelopers can verify components that match their quality attributes prior of\nintegrating them into their system.",
    "sourceUrl": "http://arxiv.org/abs/1210.3758v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Utilizing third party software components in the development of new systems\nbecame somewhat unfavourable approach among many organizations nowadays. This\nreluctance is primarily built due to the lack of support to verify the quality\nattributes of software components in order to avoid potential mismatches with\nsystems requirements. This paper presents an approach to overcome this problem\nby providing a tool support to check component compatibility to a specification\nprovided by developers. So, components compatibility can be checked and\ndevelopers can verify components that match their quality attributes prior of\nintegrating them into their system.</p>"
  },
  {
    "title": "Code Drones",
    "date": "2014-11-22",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Mithun P. Acharya",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1411.6118v4"
    },
    "publicTags": [],
    "summary": "We propose and explore a new paradigm called Code Drones in which every\nsoftware artifact such as a class is an intelligent and socially active entity.\nIn this paradigm, humanized artifacts take the lead and choreograph (socially,\nin collaboration with other intelligent software artifacts and humans)\nautomated software engineering solutions to a myriad of development and\nmaintenance challenges, including API migration, reuse, documentation, testing,\npatching, and refactoring. We discuss the implications of having social and\nintelligent/cognitive software artifacts that guide their own self-improvement.",
    "sourceUrl": "http://arxiv.org/abs/1411.6118v4",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>We propose and explore a new paradigm called Code Drones in which every\nsoftware artifact such as a class is an intelligent and socially active entity.\nIn this paradigm, humanized artifacts take the lead and choreograph (socially,\nin collaboration with other intelligent software artifacts and humans)\nautomated software engineering solutions to a myriad of development and\nmaintenance challenges, including API migration, reuse, documentation, testing,\npatching, and refactoring. We discuss the implications of having social and\nintelligent/cognitive software artifacts that guide their own self-improvement.</p>"
  },
  {
    "title": "Software Effort Estimation using Neuro Fuzzy Inference System: Past and\n  Present",
    "date": "2019-12-26",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Aditi Sharma",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1912.11855v1"
    },
    "publicTags": [],
    "summary": "Most important reason for project failure is poor effort estimation. Software\ndevelopment effort estimation is needed for assigning appropriate team members\nfor development, allocating resources for software development, binding etc.\nInaccurate software estimation may lead to delay in project, over-budget or\ncancellation of the project. But the effort estimation models are not very\nefficient. In this paper, we are analyzing the new approach for estimation i.e.\nNeuro Fuzzy Inference System (NFIS). It is a mixture model that consolidates\nthe components of artificial neural network with fuzzy logic for giving a\nbetter estimation.",
    "sourceUrl": "http://arxiv.org/abs/1912.11855v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Most important reason for project failure is poor effort estimation. Software\ndevelopment effort estimation is needed for assigning appropriate team members\nfor development, allocating resources for software development, binding etc.\nInaccurate software estimation may lead to delay in project, over-budget or\ncancellation of the project. But the effort estimation models are not very\nefficient. In this paper, we are analyzing the new approach for estimation i.e.\nNeuro Fuzzy Inference System (NFIS). It is a mixture model that consolidates\nthe components of artificial neural network with fuzzy logic for giving a\nbetter estimation.</p>"
  },
  {
    "title": "Mutant Density: A Measure of Fault-Sensitive Complexity",
    "date": "2021-04-25",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Ali Parsai",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2104.12121v1"
    },
    "publicTags": [],
    "summary": "Software code complexity is a well-studied property to determine software\ncomponent health. However, the existing code complexity metrics do not directly\ntake into account the fault-proneness aspect of the code. We propose a metric\ncalled mutant density where we use mutation as a method to introduce artificial\nfaults in code, and count the number of possible mutations per line. We show\nhow this metric can be used to perform helpful analysis of real-life software\nprojects.",
    "sourceUrl": "http://arxiv.org/abs/2104.12121v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software code complexity is a well-studied property to determine software\ncomponent health. However, the existing code complexity metrics do not directly\ntake into account the fault-proneness aspect of the code. We propose a metric\ncalled mutant density where we use mutation as a method to introduce artificial\nfaults in code, and count the number of possible mutations per line. We show\nhow this metric can be used to perform helpful analysis of real-life software\nprojects.</p>"
  },
  {
    "title": "Interactive Visualization for Exploring Information Fragments in\n  Software Repositories",
    "date": "2021-04-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Youngtaek Kim",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2104.13568v1"
    },
    "publicTags": [],
    "summary": "Software developers explore and inspect software repository data to obtain\ndetailed information archived in the development history. However, developers\nwho are not acquainted with the development context suffer from delving into\nthe repositories with a handful of information; they have difficulty\ndiscovering and expanding information fragments considering the topological and\nsequential multi-dimensional structure of repositories. We introduce ExIF, an\ninteractive visualization for exploring information fragments in software\nrepositories. ExIF helps users discover new information fragments within\nclusters or topological neighbors and identify revisions incorporating\nuser-collected fragments.",
    "sourceUrl": "http://arxiv.org/abs/2104.13568v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software developers explore and inspect software repository data to obtain\ndetailed information archived in the development history. However, developers\nwho are not acquainted with the development context suffer from delving into\nthe repositories with a handful of information; they have difficulty\ndiscovering and expanding information fragments considering the topological and\nsequential multi-dimensional structure of repositories. We introduce ExIF, an\ninteractive visualization for exploring information fragments in software\nrepositories. ExIF helps users discover new information fragments within\nclusters or topological neighbors and identify revisions incorporating\nuser-collected fragments.</p>"
  },
  {
    "title": "Enumerating Hardware-Software Splits with Program Rewriting",
    "date": "2020-02-29",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Gus Smith",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2003.00290v1"
    },
    "publicTags": [],
    "summary": "A core problem in hardware-software codesign is in the sheer size of the\ndesign space. Without a set ISA to constrain the hardware-software interface,\nthe design space explodes. This work presents a strategy for managing the\nmassive hardware-software design space within the domain of machine learning\ninference workloads and accelerators. We first propose EngineIR, a new language\nfor representing machine learning hardware and software in a single program.\nThen, using equality graphs -- a data structure from the compilers literature\n-- we suggest a method for efficiently enumerating the design space by\nperforming rewrites over our representation.",
    "sourceUrl": "http://arxiv.org/abs/2003.00290v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>A core problem in hardware-software codesign is in the sheer size of the\ndesign space. Without a set ISA to constrain the hardware-software interface,\nthe design space explodes. This work presents a strategy for managing the\nmassive hardware-software design space within the domain of machine learning\ninference workloads and accelerators. We first propose EngineIR, a new language\nfor representing machine learning hardware and software in a single program.\nThen, using equality graphs -- a data structure from the compilers literature\n-- we suggest a method for efficiently enumerating the design space by\nperforming rewrites over our representation.</p>"
  },
  {
    "title": "ProFIPy: Programmable Software Fault Injection as-a-Service",
    "date": "2020-05-11",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Domenico Cotroneo",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2005.04990v1"
    },
    "publicTags": [],
    "summary": "In this paper, we present a new fault injection tool (ProFIPy) for Python\nsoftware. The tool is designed to be programmable, in order to enable users to\nspecify their software fault model, using a domain-specific language (DSL) for\nfault injection. Moreover, to achieve better usability, ProFIPy is provided as\nsoftware-as-a-service and supports the user through the configuration of the\nfaultload and workload, failure data analysis, and full automation of the\nexperiments using container-based virtualization and parallelization.",
    "sourceUrl": "http://arxiv.org/abs/2005.04990v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In this paper, we present a new fault injection tool (ProFIPy) for Python\nsoftware. The tool is designed to be programmable, in order to enable users to\nspecify their software fault model, using a domain-specific language (DSL) for\nfault injection. Moreover, to achieve better usability, ProFIPy is provided as\nsoftware-as-a-service and supports the user through the configuration of the\nfaultload and workload, failure data analysis, and full automation of the\nexperiments using container-based virtualization and parallelization.</p>"
  },
  {
    "title": "An Integrated Framework for DevSecOps Adoption",
    "date": "2022-07-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Akanksha Gupta",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2207.04093v1"
    },
    "publicTags": [],
    "summary": "Introduction of DevOps into the software development life cycle represents a\ncultural shift in the IT culture, amalgamating development and operations to\nimprove delivery speed in a rapid and maintainable manner. At the same time,\nsecurity threats and breaches are expected to grow as more enterprises move to\nnew agile frameworks for rapid product delivery. Meanwhile, DevSecOps is a\nmindset change that revolutionizes software development by embedding security\nat each step of the software cycle, leading to resilient software. This paper\ndiscusses a framework organization can use to embed DevSecOps swiftly and\nefficiently into the general IT culture.",
    "sourceUrl": "http://arxiv.org/abs/2207.04093v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Introduction of DevOps into the software development life cycle represents a\ncultural shift in the IT culture, amalgamating development and operations to\nimprove delivery speed in a rapid and maintainable manner. At the same time,\nsecurity threats and breaches are expected to grow as more enterprises move to\nnew agile frameworks for rapid product delivery. Meanwhile, DevSecOps is a\nmindset change that revolutionizes software development by embedding security\nat each step of the software cycle, leading to resilient software. This paper\ndiscusses a framework organization can use to embed DevSecOps swiftly and\nefficiently into the general IT culture.</p>"
  },
  {
    "title": "Curating Model Problems for Software Designing",
    "date": "2025-03-09",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Mary Shaw",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2503.06400v1"
    },
    "publicTags": [],
    "summary": "Many disciplines use standard examples for education and to share and compare\nresearch results. The examples are rich enough to study from multiple points of\nview; they are often called model problems. Software design lacks such a\ncommunity resource. We propose an activity for Designing 2025 in which\nparticipants improve some existing model problem descriptions and initiate new\nones -- with a focus on use in software design education, plus potential\nutility in research.",
    "sourceUrl": "http://arxiv.org/abs/2503.06400v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Many disciplines use standard examples for education and to share and compare\nresearch results. The examples are rich enough to study from multiple points of\nview; they are often called model problems. Software design lacks such a\ncommunity resource. We propose an activity for Designing 2025 in which\nparticipants improve some existing model problem descriptions and initiate new\nones -- with a focus on use in software design education, plus potential\nutility in research.</p>"
  },
  {
    "title": "Dynamic Software Updating in Java -- Comparing Concepts and Resource\n  Demands",
    "date": "2025-06-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Danijel Mlinaric",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2506.01875v1"
    },
    "publicTags": [],
    "summary": "Dynamic software updating (DSU) is an extremely useful feature to be used\nduring the software evolution. It can be used to reduce downtime costs, for\nsecurity enhancements, profiling and testing the new functionalities. There are\nmany researches and solutions on dynamic software updating regarding diverse\nproblems introduced by the topic, but there is a lack of research which compare\nvarious approaches concerning supported changes and demands on re-sources. In\nthis paper we are comparing currently available con-cepts for Java programming\nlanguage that deal with dynamically applied changes and impact of those changes\non computer resource demands.",
    "sourceUrl": "http://arxiv.org/abs/2506.01875v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Dynamic software updating (DSU) is an extremely useful feature to be used\nduring the software evolution. It can be used to reduce downtime costs, for\nsecurity enhancements, profiling and testing the new functionalities. There are\nmany researches and solutions on dynamic software updating regarding diverse\nproblems introduced by the topic, but there is a lack of research which compare\nvarious approaches concerning supported changes and demands on re-sources. In\nthis paper we are comparing currently available con-cepts for Java programming\nlanguage that deal with dynamically applied changes and impact of those changes\non computer resource demands.</p>"
  },
  {
    "title": "A Systematic Review of Strong Gravitational Lens Modeling Software",
    "date": "2012-06-20",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Alan T. Lefor",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1206.4382v2"
    },
    "publicTags": [],
    "summary": "Despite expanding research activity in gravitational lens modeling, there is\nno particular software which is considered a standard. Much of the\ngravitational lens modeling software is written by individual investigators for\ntheir own use. Some gravitational lens modeling software is freely available\nfor download but is widely variable with regard to ease of use and quality of\ndocumentation. This review of 13 software packages was undertaken to provide a\nsingle source of information. Gravitational lens models are classified as\nparametric models or non-parametric models, and can be further divided into\nresearch and educational software. Software used in research includes the\nGRAVLENS package (with both gravlens and lensmodel), Lenstool, LensPerfect,\nglafic, PixeLens, SimpLens, Lensview, and GRALE. In this review, GravLensHD,\nG-Lens, Gravitational Lensing, lens and MOWGLI are categorized as educational\nprograms that are useful for demonstrating various aspects of lensing. Each of\nthe 13 software packages is reviewed with regard to software features\n(installation, documentation, files provided, etc.) and lensing features (type\nof model, input data, output data, etc.) as well as a brief review of studies\nwhere they have been used. Recent studies have demonstrated the utility of\nstrong gravitational lensing data for mass mapping, and suggest increased use\nof these techniques in the future. Coupled with the advent of greatly improved\nimaging, new approaches to modeling of strong gravitational lens systems are\nneeded. This is the first systematic review of strong gravitational lens\nmodeling software, providing investigators with a starting point for future\nsoftware development to further advance gravitational lens modeling research.",
    "sourceUrl": "http://arxiv.org/abs/1206.4382v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Despite expanding research activity in gravitational lens modeling, there is\nno particular software which is considered a standard. Much of the\ngravitational lens modeling software is written by individual investigators for\ntheir own use. Some gravitational lens modeling software is freely available\nfor download but is widely variable with regard to ease of use and quality of\ndocumentation. This review of 13 software packages was undertaken to provide a\nsingle source of information. Gravitational lens models are classified as\nparametric models or non-parametric models, and can be further divided into\nresearch and educational software. Software used in research includes the\nGRAVLENS package (with both gravlens and lensmodel), Lenstool, LensPerfect,\nglafic, PixeLens, SimpLens, Lensview, and GRALE. In this review, GravLensHD,\nG-Lens, Gravitational Lensing, lens and MOWGLI are categorized as educational\nprograms that are useful for demonstrating various aspects of lensing. Each of\nthe 13 software packages is reviewed with regard to software features\n(installation, documentation, files provided, etc.) and lensing features (type\nof model, input data, output data, etc.) as well as a brief review of studies\nwhere they have been used. Recent studies have demonstrated the utility of\nstrong gravitational lensing data for mass mapping, and suggest increased use\nof these techniques in the future. Coupled with the advent of greatly improved\nimaging, new approaches to modeling of strong gravitational lens systems are\nneeded. This is the first systematic review of strong gravitational lens\nmodeling software, providing investigators with a starting point for future\nsoftware development to further advance gravitational lens modeling research.</p>"
  },
  {
    "title": "Myths and Realities about Online Forums in Open Source Software\n  Development: An Empirical Study",
    "date": "2015-07-24",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Faheem Ahmed",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1507.06927v1"
    },
    "publicTags": [],
    "summary": "The use of free and open source software (OSS) is gaining momentum due to the\never increasing availability and use of the Internet. Organizations are also\nnow adopting open source software, despite some reservations, in particular\nregarding the provision and availability of support. Some of the biggest\nconcerns about free and open source software are post release software defects\nand their rectification, management of dynamic requirements and support to the\nusers. A common belief is that there is no appropriate support available for\nthis class of software. A contradictory argument is that due to the active\ninvolvement of Internet users in online forums, there is in fact a large\nresource available that communicates and manages the provision of support. The\nresearch model of this empirical investigation examines the evidence available\nto assess whether this commonly held belief is based on facts given the current\ndevelopments in OSS or simply a myth, which has developed around OSS\ndevelopment. We analyzed a dataset consisting of 1880 open source software\nprojects covering a broad range of categories in this investigation. The\nresults show that online forums play a significant role in managing software\ndefects, implementation of new requirements and providing support to the users\nin open source software and have become a major source of assistance in\nmaintenance of the open source projects.",
    "sourceUrl": "http://arxiv.org/abs/1507.06927v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The use of free and open source software (OSS) is gaining momentum due to the\never increasing availability and use of the Internet. Organizations are also\nnow adopting open source software, despite some reservations, in particular\nregarding the provision and availability of support. Some of the biggest\nconcerns about free and open source software are post release software defects\nand their rectification, management of dynamic requirements and support to the\nusers. A common belief is that there is no appropriate support available for\nthis class of software. A contradictory argument is that due to the active\ninvolvement of Internet users in online forums, there is in fact a large\nresource available that communicates and manages the provision of support. The\nresearch model of this empirical investigation examines the evidence available\nto assess whether this commonly held belief is based on facts given the current\ndevelopments in OSS or simply a myth, which has developed around OSS\ndevelopment. We analyzed a dataset consisting of 1880 open source software\nprojects covering a broad range of categories in this investigation. The\nresults show that online forums play a significant role in managing software\ndefects, implementation of new requirements and providing support to the users\nin open source software and have become a major source of assistance in\nmaintenance of the open source projects.</p>"
  },
  {
    "title": "Systematic Evaluation of Sandboxed Software Deployment for Real-time\n  Software on the Example of a Self-Driving Heavy Vehicle",
    "date": "2016-08-24",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Philip Masek",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1608.06759v1"
    },
    "publicTags": [],
    "summary": "Companies developing and maintaining software-only products like web shops\naim for establishing persistent links to their software running in the field.\nMonitoring data from real usage scenarios allows for a number of improvements\nin the software life-cycle, such as quick identification and solution of\nissues, and elicitation of requirements from previously unexpected usage. While\nthe processes of continuous integration, continuous deployment, and continuous\nexperimentation using sandboxing technologies are becoming well established in\nsaid software-only products, adopting similar practices for the automotive\ndomain is more complex mainly due to real-time and safety constraints. In this\npaper, we systematically evaluate sandboxed software deployment in the context\nof a self-driving heavy vehicle that participated in the 2016 Grand Cooperative\nDriving Challenge (GCDC) in The Netherlands. We measured the system's\nscheduling precision after deploying applications in four different execution\nenvironments. Our results indicate that there is no significant difference in\nperformance and overhead when sandboxed environments are used compared to\nnatively deployed software. Thus, recent trends in software architecting,\npackaging, and maintenance using microservices encapsulated in sandboxes will\nhelp to realize similar software and system engineering for cyber-physical\nsystems.",
    "sourceUrl": "http://arxiv.org/abs/1608.06759v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Companies developing and maintaining software-only products like web shops\naim for establishing persistent links to their software running in the field.\nMonitoring data from real usage scenarios allows for a number of improvements\nin the software life-cycle, such as quick identification and solution of\nissues, and elicitation of requirements from previously unexpected usage. While\nthe processes of continuous integration, continuous deployment, and continuous\nexperimentation using sandboxing technologies are becoming well established in\nsaid software-only products, adopting similar practices for the automotive\ndomain is more complex mainly due to real-time and safety constraints. In this\npaper, we systematically evaluate sandboxed software deployment in the context\nof a self-driving heavy vehicle that participated in the 2016 Grand Cooperative\nDriving Challenge (GCDC) in The Netherlands. We measured the system's\nscheduling precision after deploying applications in four different execution\nenvironments. Our results indicate that there is no significant difference in\nperformance and overhead when sandboxed environments are used compared to\nnatively deployed software. Thus, recent trends in software architecting,\npackaging, and maintenance using microservices encapsulated in sandboxes will\nhelp to realize similar software and system engineering for cyber-physical\nsystems.</p>"
  },
  {
    "title": "Consequences of Unhappiness While Developing Software",
    "date": "2017-01-20",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Daniel Graziotin",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1701.05789v2"
    },
    "publicTags": [],
    "summary": "The growing literature on affect among software developers mostly reports on\nthe linkage between happiness, software quality, and developer productivity.\nUnderstanding the positive side of happiness -- positive emotions and moods --\nis an attractive and important endeavor. Scholars in industrial and\norganizational psychology have suggested that also studying the negative side\n-- unhappiness -- could lead to cost-effective ways of enhancing working\nconditions, job performance, and to limiting the occurrence of psychological\ndisorders. Our comprehension of the consequences of (un)happiness among\ndevelopers is still too shallow, and is mainly expressed in terms of\ndevelopment productivity and software quality. In this paper, we attempt to\nuncover the experienced consequences of unhappiness among software developers.\nUsing qualitative data analysis of the responses given by 181 questionnaire\nparticipants, we identified 49 consequences of unhappiness while doing software\ndevelopment. We found detrimental consequences on developers' mental\nwell-being, the software development process, and the produced artifacts. Our\nclassification scheme, available as open data, will spawn new happiness\nresearch opportunities of cause-effect type, and it can act as a guideline for\npractitioners for identifying damaging effects of unhappiness and for fostering\nhappiness on the job.",
    "sourceUrl": "http://arxiv.org/abs/1701.05789v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The growing literature on affect among software developers mostly reports on\nthe linkage between happiness, software quality, and developer productivity.\nUnderstanding the positive side of happiness -- positive emotions and moods --\nis an attractive and important endeavor. Scholars in industrial and\norganizational psychology have suggested that also studying the negative side\n-- unhappiness -- could lead to cost-effective ways of enhancing working\nconditions, job performance, and to limiting the occurrence of psychological\ndisorders. Our comprehension of the consequences of (un)happiness among\ndevelopers is still too shallow, and is mainly expressed in terms of\ndevelopment productivity and software quality. In this paper, we attempt to\nuncover the experienced consequences of unhappiness among software developers.\nUsing qualitative data analysis of the responses given by 181 questionnaire\nparticipants, we identified 49 consequences of unhappiness while doing software\ndevelopment. We found detrimental consequences on developers' mental\nwell-being, the software development process, and the produced artifacts. Our\nclassification scheme, available as open data, will spawn new happiness\nresearch opportunities of cause-effect type, and it can act as a guideline for\npractitioners for identifying damaging effects of unhappiness and for fostering\nhappiness on the job.</p>"
  },
  {
    "title": "Supporting Robotic Software Migration Using Static Analysis and\n  Model-Driven Engineering",
    "date": "2020-08-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Sophie Wood",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2008.02164v1"
    },
    "publicTags": [],
    "summary": "The wide use of robotic systems contributed to developing robotic software\nhighly coupled to the hardware platform running the robotic system. Due to\nincreased maintenance cost or changing business priorities, the robotic\nhardware is infrequently upgraded, thus increasing the risk for technology\nstagnation. Reducing this risk entails migrating the system and its software to\na new hardware platform. Conventional software engineering practices such as\ncomplete re-development and code-based migration, albeit useful in mitigating\nthese obsolescence issues, they are time-consuming and overly expensive. Our\nRoboSMi model-driven approach supports the migration of the software\ncontrolling a robotic system between hardware platforms. First, RoboSMi\nexecutes static analysis on the robotic software of the source hardware\nplatform to identify platform-dependent and platform-agnostic software\nconstructs. By analysing a model that expresses the architecture of robotic\ncomponents on the target platform, RoboSMi establishes the hardware\nconfiguration of those components and suggests software libraries for each\ncomponent whose execution will enable the robotic software to control the\ncomponents. Finally, RoboSMi through code-generation produces software for the\ntarget platform and indicates areas that require manual intervention by robotic\nengineers to complete the migration. We evaluate the applicability of RoboSMi\nand analyse the level of automation and performance provided from its use by\nmigrating two robotic systems deployed for an environmental monitoring and a\nline following mission from a Propeller Activity Board to an Arduino Uno.",
    "sourceUrl": "http://arxiv.org/abs/2008.02164v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The wide use of robotic systems contributed to developing robotic software\nhighly coupled to the hardware platform running the robotic system. Due to\nincreased maintenance cost or changing business priorities, the robotic\nhardware is infrequently upgraded, thus increasing the risk for technology\nstagnation. Reducing this risk entails migrating the system and its software to\na new hardware platform. Conventional software engineering practices such as\ncomplete re-development and code-based migration, albeit useful in mitigating\nthese obsolescence issues, they are time-consuming and overly expensive. Our\nRoboSMi model-driven approach supports the migration of the software\ncontrolling a robotic system between hardware platforms. First, RoboSMi\nexecutes static analysis on the robotic software of the source hardware\nplatform to identify platform-dependent and platform-agnostic software\nconstructs. By analysing a model that expresses the architecture of robotic\ncomponents on the target platform, RoboSMi establishes the hardware\nconfiguration of those components and suggests software libraries for each\ncomponent whose execution will enable the robotic software to control the\ncomponents. Finally, RoboSMi through code-generation produces software for the\ntarget platform and indicates areas that require manual intervention by robotic\nengineers to complete the migration. We evaluate the applicability of RoboSMi\nand analyse the level of automation and performance provided from its use by\nmigrating two robotic systems deployed for an environmental monitoring and a\nline following mission from a Propeller Activity Board to an Arduino Uno.</p>"
  },
  {
    "title": "Mining DEV for social and technical insights about software development",
    "date": "2021-03-31",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Maria Papoutsoglou",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2103.17054v2"
    },
    "publicTags": [],
    "summary": "Software developers are social creatures: they communicate, collaborate, and\npromote their work in a variety of channels. Twitter, GitHub, Stack Overflow,\nand other platforms offer developers opportunities to network and exchange\nideas. Researchers analyze content on these sites to learn about trends and\ntopics in software engineering. However, insight mined from the text of Stack\nOverflow questions or GitHub issues is highly focused on detailed and technical\naspects of software development. In this paper, we present a relatively new\nonline community for software developers called DEV. On DEV users write\nlong-form posts about their experiences, preferences, and working life in\nsoftware, zooming out from specific issues and files to reflect on broader\ntopics. About 50,000 users have posted over 140,000 articles related to\nsoftware development. In this work, we describe the content of posts on DEV\nusing a topic model, showing that developers discuss a rich variety and mixture\nof social and technical aspects of software development. We show that\ndevelopers use DEV to promote themselves and their work: 83% link their\nprofiles to their GitHub profiles and 56% to their Twitter profiles. 14% of\nusers pin specific GitHub repos in their profiles. We argue that DEV is\nemerging as an important hub for software developers, and a valuable source of\ninsight for researchers to complement data from platforms like GitHub and Stack\nOverflow.",
    "sourceUrl": "http://arxiv.org/abs/2103.17054v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software developers are social creatures: they communicate, collaborate, and\npromote their work in a variety of channels. Twitter, GitHub, Stack Overflow,\nand other platforms offer developers opportunities to network and exchange\nideas. Researchers analyze content on these sites to learn about trends and\ntopics in software engineering. However, insight mined from the text of Stack\nOverflow questions or GitHub issues is highly focused on detailed and technical\naspects of software development. In this paper, we present a relatively new\nonline community for software developers called DEV. On DEV users write\nlong-form posts about their experiences, preferences, and working life in\nsoftware, zooming out from specific issues and files to reflect on broader\ntopics. About 50,000 users have posted over 140,000 articles related to\nsoftware development. In this work, we describe the content of posts on DEV\nusing a topic model, showing that developers discuss a rich variety and mixture\nof social and technical aspects of software development. We show that\ndevelopers use DEV to promote themselves and their work: 83% link their\nprofiles to their GitHub profiles and 56% to their Twitter profiles. 14% of\nusers pin specific GitHub repos in their profiles. We argue that DEV is\nemerging as an important hub for software developers, and a valuable source of\ninsight for researchers to complement data from platforms like GitHub and Stack\nOverflow.</p>"
  },
  {
    "title": "Software engineering in start-up companies: An analysis of 88 experience\n  reports",
    "date": "2023-11-20",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Eriks Klotins",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2311.12139v1"
    },
    "publicTags": [],
    "summary": "Context: Start-up companies have become an important supplier of innovation\nand software-intensive products. The flexibility and reactiveness of start-ups\nenables fast development and launch of innovative products. However, a majority\nof software start-up companies fail before achieving any success. Among other\nfactors, poor software engineering could be a significant contributor to the\nchallenges experienced by start-ups. However, the state-of-practice of software\nengineering in start-ups, as well as the utilization of state-of-the-art is\nlargely an unexplored area. Objective: In this study we investigate how\nsoftware engineering is applied in start-up context with a focus to identify\nkey knowledge areas and opportunities for further research. Method: We perform\na multi-vocal exploratory study of 88 start-up experience reports. We develop a\ncustom taxonomy to categorize the reported software engineering practices and\ntheir interrelation with business aspects, and apply qualitative data analysis\nto explore influences and dependencies between the knowledge areas. Results: We\nidentify the most frequently reported software engineering (requirements\nengineering, software design and quality) and business aspect (vision and\nstrategy development) knowledge areas, and illustrate their relationships. We\nalso present a summary of how relevant software engineering knowledge areas are\nimplemented in start-ups and identify potentially useful practices for adoption\nin start-ups. Conclusions: The results enable a more focused research on\nengineering practices in start-ups. We conclude that most engineering\nchallenges in start-ups stem from inadequacies in requirements engineering.\nMany promising practices to address specific engineering challenges exists,\nhowever more research on adaptation of established practices, and validation of\nnew start-up specific practices is needed.",
    "sourceUrl": "http://arxiv.org/abs/2311.12139v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Context: Start-up companies have become an important supplier of innovation\nand software-intensive products. The flexibility and reactiveness of start-ups\nenables fast development and launch of innovative products. However, a majority\nof software start-up companies fail before achieving any success. Among other\nfactors, poor software engineering could be a significant contributor to the\nchallenges experienced by start-ups. However, the state-of-practice of software\nengineering in start-ups, as well as the utilization of state-of-the-art is\nlargely an unexplored area. Objective: In this study we investigate how\nsoftware engineering is applied in start-up context with a focus to identify\nkey knowledge areas and opportunities for further research. Method: We perform\na multi-vocal exploratory study of 88 start-up experience reports. We develop a\ncustom taxonomy to categorize the reported software engineering practices and\ntheir interrelation with business aspects, and apply qualitative data analysis\nto explore influences and dependencies between the knowledge areas. Results: We\nidentify the most frequently reported software engineering (requirements\nengineering, software design and quality) and business aspect (vision and\nstrategy development) knowledge areas, and illustrate their relationships. We\nalso present a summary of how relevant software engineering knowledge areas are\nimplemented in start-ups and identify potentially useful practices for adoption\nin start-ups. Conclusions: The results enable a more focused research on\nengineering practices in start-ups. We conclude that most engineering\nchallenges in start-ups stem from inadequacies in requirements engineering.\nMany promising practices to address specific engineering challenges exists,\nhowever more research on adaptation of established practices, and validation of\nnew start-up specific practices is needed.</p>"
  },
  {
    "title": "ACTesting: Automated Cross-modal Testing Method of Text-to-Image\n  Software",
    "date": "2023-12-20",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Siqi Gu",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2312.12933v3"
    },
    "publicTags": [],
    "summary": "Recently, creative generative artificial intelligence software has emerged as\na pivotal assistant, enabling users to generate content and seek inspiration\nrapidly. Text-to-Image (T2I) software, one of the most widely used, synthesizes\nimages with text input by engaging in a cross-modal process. However, despite\nsubstantial advancements in the T2I engine, T2I software still encounters\nerrors when generating complex or non-realistic scenes, including omitting\nfocal entities, low image realism, and mismatched text-image information. The\ncross-modal nature of T2I software complicates error detection for traditional\ntesting methods, and the absence of test oracles further exacerbates the\ncomplexity of the testing process. To fill this gap, we propose ACTesting, an\nAutomated Cross-modal Testing Method of Text-to-Image Software, the first\ntesting method explicitly designed for T2I software. ACTesting utilizes the\nmetamorphic testing principle to address the oracle problem and identifies\ncross-modal semantic consistency as its fundamental Metamorphic relation (MR)\nby employing the Entity-relationship (ER) triples. We design three kinds of\nmutation operators under the guidance of MR and the adaptability density\nconstraint to construct the new input text. After generating the images based\non the text, ACTesting verifies whether MR is satisfied by detecting the ER\ntriples across two modalities to detect the errors of T2I software. In our\nexperiments across five popular T2I software, ACTesting effectively generates\nerror-revealing tests, resulting in a decrease in text-image consistency by up\nto 20% when compared to the baseline. Additionally, an ablation study\ndemonstrates the efficacy of the proposed mutation operators. The experimental\nresults validate that ACTesting can reliably identify errors within T2I\nsoftware.",
    "sourceUrl": "http://arxiv.org/abs/2312.12933v3",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Recently, creative generative artificial intelligence software has emerged as\na pivotal assistant, enabling users to generate content and seek inspiration\nrapidly. Text-to-Image (T2I) software, one of the most widely used, synthesizes\nimages with text input by engaging in a cross-modal process. However, despite\nsubstantial advancements in the T2I engine, T2I software still encounters\nerrors when generating complex or non-realistic scenes, including omitting\nfocal entities, low image realism, and mismatched text-image information. The\ncross-modal nature of T2I software complicates error detection for traditional\ntesting methods, and the absence of test oracles further exacerbates the\ncomplexity of the testing process. To fill this gap, we propose ACTesting, an\nAutomated Cross-modal Testing Method of Text-to-Image Software, the first\ntesting method explicitly designed for T2I software. ACTesting utilizes the\nmetamorphic testing principle to address the oracle problem and identifies\ncross-modal semantic consistency as its fundamental Metamorphic relation (MR)\nby employing the Entity-relationship (ER) triples. We design three kinds of\nmutation operators under the guidance of MR and the adaptability density\nconstraint to construct the new input text. After generating the images based\non the text, ACTesting verifies whether MR is satisfied by detecting the ER\ntriples across two modalities to detect the errors of T2I software. In our\nexperiments across five popular T2I software, ACTesting effectively generates\nerror-revealing tests, resulting in a decrease in text-image consistency by up\nto 20% when compared to the baseline. Additionally, an ablation study\ndemonstrates the efficacy of the proposed mutation operators. The experimental\nresults validate that ACTesting can reliably identify errors within T2I\nsoftware.</p>"
  },
  {
    "title": "Charting a Path to Efficient Onboarding: The Role of Software\n  Visualization",
    "date": "2024-01-17",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Fernando Padoan",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2401.09605v1"
    },
    "publicTags": [],
    "summary": "Background. Within the software industry, it is commonly estimated that\nsoftware professionals invest a substantial portion of their work hours in the\nprocess of understanding existing systems. In this context, an ineffective\ntechnical onboarding process, which introduces newcomers to software under\ndevelopment, can result in a prolonged period for them to absorb the necessary\nknowledge required to become productive in their roles. Goal. The present study\naims to explore the familiarity of managers, leaders, and developers with\nsoftware visualization tools and how these tools are employed to facilitate the\ntechnical onboarding of new team members. Method. To address the research\nproblem, we built upon the insights gained through the literature and embraced\na sequential exploratory approach. This approach incorporated quantitative and\nqualitative analyses of data collected from practitioners using questionnaires\nand semi-structured interviews. Findings. Our findings demonstrate a gap\nbetween the concept of software visualization and the practical use of\nonboarding tools and techniques. Overall, practitioners do not systematically\nincorporate software visualization tools into their technical onboarding\nprocesses due to a lack of conceptual understanding and awareness of their\npotential benefits. Conclusion. The software industry could benefit from\nstandardized and evolving onboarding models, improved by incorporating software\nvisualization techniques and tools to support program comprehension of\nnewcomers in the software projects.",
    "sourceUrl": "http://arxiv.org/abs/2401.09605v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Background. Within the software industry, it is commonly estimated that\nsoftware professionals invest a substantial portion of their work hours in the\nprocess of understanding existing systems. In this context, an ineffective\ntechnical onboarding process, which introduces newcomers to software under\ndevelopment, can result in a prolonged period for them to absorb the necessary\nknowledge required to become productive in their roles. Goal. The present study\naims to explore the familiarity of managers, leaders, and developers with\nsoftware visualization tools and how these tools are employed to facilitate the\ntechnical onboarding of new team members. Method. To address the research\nproblem, we built upon the insights gained through the literature and embraced\na sequential exploratory approach. This approach incorporated quantitative and\nqualitative analyses of data collected from practitioners using questionnaires\nand semi-structured interviews. Findings. Our findings demonstrate a gap\nbetween the concept of software visualization and the practical use of\nonboarding tools and techniques. Overall, practitioners do not systematically\nincorporate software visualization tools into their technical onboarding\nprocesses due to a lack of conceptual understanding and awareness of their\npotential benefits. Conclusion. The software industry could benefit from\nstandardized and evolving onboarding models, improved by incorporating software\nvisualization techniques and tools to support program comprehension of\nnewcomers in the software projects.</p>"
  },
  {
    "title": "LLMs' Reshaping of People, Processes, Products, and Society in Software\n  Development: A Comprehensive Exploration with Early Adopters",
    "date": "2025-03-06",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Benyamin Tabarsi",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2503.05012v1"
    },
    "publicTags": [],
    "summary": "Large language models (LLMs) like OpenAI ChatGPT, Google Gemini, and GitHub\nCopilot are rapidly gaining traction in the software industry, but their full\nimpact on software engineering remains insufficiently explored. Despite their\ngrowing adoption, there is a notable lack of formal, qualitative assessments of\nhow LLMs are applied in real-world software development contexts. To fill this\ngap, we conducted semi-structured interviews with sixteen early-adopter\nprofessional developers to explore their use of LLMs throughout various stages\nof the software development life cycle. Our investigation examines four\ndimensions: people - how LLMs affect individual developers and teams; process -\nhow LLMs alter software engineering workflows; product - LLM impact on software\nquality and innovation; and society - the broader socioeconomic and ethical\nimplications of LLM adoption. Thematic analysis of our data reveals that while\nLLMs have not fundamentally revolutionized the development process, they have\nsubstantially enhanced routine coding tasks, including code generation,\nrefactoring, and debugging. Developers reported the most effective outcomes\nwhen providing LLMs with clear, well-defined problem statements, indicating\nthat LLMs excel with decomposed problems and specific requirements.\nFurthermore, these early-adopters identified that LLMs offer significant value\nfor personal and professional development, aiding in learning new languages and\nconcepts. Early-adopters, highly skilled in software engineering and how LLMs\nwork, identified early and persisting challenges for software engineering, such\nas inaccuracies in generated content and the need for careful manual review\nbefore integrating LLM outputs into production environments. Our study provides\na nuanced understanding of how LLMs are shaping the landscape of software\ndevelopment, with their benefits, limitations, and ongoing implications.",
    "sourceUrl": "http://arxiv.org/abs/2503.05012v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Large language models (LLMs) like OpenAI ChatGPT, Google Gemini, and GitHub\nCopilot are rapidly gaining traction in the software industry, but their full\nimpact on software engineering remains insufficiently explored. Despite their\ngrowing adoption, there is a notable lack of formal, qualitative assessments of\nhow LLMs are applied in real-world software development contexts. To fill this\ngap, we conducted semi-structured interviews with sixteen early-adopter\nprofessional developers to explore their use of LLMs throughout various stages\nof the software development life cycle. Our investigation examines four\ndimensions: people - how LLMs affect individual developers and teams; process -\nhow LLMs alter software engineering workflows; product - LLM impact on software\nquality and innovation; and society - the broader socioeconomic and ethical\nimplications of LLM adoption. Thematic analysis of our data reveals that while\nLLMs have not fundamentally revolutionized the development process, they have\nsubstantially enhanced routine coding tasks, including code generation,\nrefactoring, and debugging. Developers reported the most effective outcomes\nwhen providing LLMs with clear, well-defined problem statements, indicating\nthat LLMs excel with decomposed problems and specific requirements.\nFurthermore, these early-adopters identified that LLMs offer significant value\nfor personal and professional development, aiding in learning new languages and\nconcepts. Early-adopters, highly skilled in software engineering and how LLMs\nwork, identified early and persisting challenges for software engineering, such\nas inaccuracies in generated content and the need for careful manual review\nbefore integrating LLM outputs into production environments. Our study provides\na nuanced understanding of how LLMs are shaping the landscape of software\ndevelopment, with their benefits, limitations, and ongoing implications.</p>"
  },
  {
    "title": "Querying Large Automotive Software Models: Agentic vs. Direct LLM\n  Approaches",
    "date": "2025-06-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Lukasz Mazur",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2506.13171v1"
    },
    "publicTags": [],
    "summary": "Large language models (LLMs) offer new opportunities for interacting with\ncomplex software artifacts, such as software models, through natural language.\nThey present especially promising benefits for large software models that are\ndifficult to grasp in their entirety, making traditional interaction and\nanalysis approaches challenging. This paper investigates two approaches for\nleveraging LLMs to answer questions over software models: direct prompting,\nwhere the whole software model is provided in the context, and an agentic\napproach combining LLM-based agents with general-purpose file access tools. We\nevaluate these approaches using an Ecore metamodel designed for timing analysis\nand software optimization in automotive and embedded domains. Our findings show\nthat while the agentic approach achieves accuracy comparable to direct\nprompting, it is significantly more efficient in terms of token usage. This\nefficiency makes the agentic approach particularly suitable for the automotive\nindustry, where the large size of software models makes direct prompting\ninfeasible, establishing LLM agents as not just a practical alternative but the\nonly viable solution. Notably, the evaluation was conducted using small LLMs,\nwhich are more feasible to be executed locally - an essential advantage for\nmeeting strict requirements around privacy, intellectual property protection,\nand regulatory compliance. Future work will investigate software models in\ndiverse formats, explore more complex agent architectures, and extend agentic\nworkflows to support not only querying but also modification of software\nmodels.",
    "sourceUrl": "http://arxiv.org/abs/2506.13171v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Large language models (LLMs) offer new opportunities for interacting with\ncomplex software artifacts, such as software models, through natural language.\nThey present especially promising benefits for large software models that are\ndifficult to grasp in their entirety, making traditional interaction and\nanalysis approaches challenging. This paper investigates two approaches for\nleveraging LLMs to answer questions over software models: direct prompting,\nwhere the whole software model is provided in the context, and an agentic\napproach combining LLM-based agents with general-purpose file access tools. We\nevaluate these approaches using an Ecore metamodel designed for timing analysis\nand software optimization in automotive and embedded domains. Our findings show\nthat while the agentic approach achieves accuracy comparable to direct\nprompting, it is significantly more efficient in terms of token usage. This\nefficiency makes the agentic approach particularly suitable for the automotive\nindustry, where the large size of software models makes direct prompting\ninfeasible, establishing LLM agents as not just a practical alternative but the\nonly viable solution. Notably, the evaluation was conducted using small LLMs,\nwhich are more feasible to be executed locally - an essential advantage for\nmeeting strict requirements around privacy, intellectual property protection,\nand regulatory compliance. Future work will investigate software models in\ndiverse formats, explore more complex agent architectures, and extend agentic\nworkflows to support not only querying but also modification of software\nmodels.</p>"
  },
  {
    "title": "A Metric of Software Size as a Tool for IT Governance",
    "date": "2018-10-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Marcus Vinicius Borela de Castro",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1810.07227v1"
    },
    "publicTags": [],
    "summary": "This paper proposes a new metric for software functional size, which is\nderived from Function Point Analysis (FPA), but overcomes some of its known\ndefi- ciencies. The statistical results show that the new metric, Functional\nElements (EF), and its submetric, Functional Elements of Transaction (EFt),\nhave higher correlation with the effort in software development than FPA in the\ncontext of the analyzed data. The paper illustrates the application of the new\nmetric as a tool to improve IT governance specifically in assessment,\nmonitoring, and giving directions to the software development area.",
    "sourceUrl": "http://arxiv.org/abs/1810.07227v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>This paper proposes a new metric for software functional size, which is\nderived from Function Point Analysis (FPA), but overcomes some of its known\ndefi- ciencies. The statistical results show that the new metric, Functional\nElements (EF), and its submetric, Functional Elements of Transaction (EFt),\nhave higher correlation with the effort in software development than FPA in the\ncontext of the analyzed data. The paper illustrates the application of the new\nmetric as a tool to improve IT governance specifically in assessment,\nmonitoring, and giving directions to the software development area.</p>"
  },
  {
    "title": "A systematic mapping on quantum software development in the context of\n  software engineering",
    "date": "2021-06-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Paulo Eduardo Zanni Junior",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2106.00926v2"
    },
    "publicTags": [],
    "summary": "Quantum Computing is a new paradigm that enables several advances which are\nimpossible using classical technology. With the rise of quantum computers, the\nsoftware is also invited to change so that it can better fit this new\ncomputation way. However, although a lot of research is being conducted in the\nquantum computing field, it is still scarce studies about the differences of\nthe software and software engineering in this new context. Therefore, this\narticle presents a systematic mapping study to present a wide review on the\nparticularities and characteristics of software that are developed for quantum\ncomputers. A total of 24 papers were selected using digital libraries with the\nobjective of answering three research questions elaborated in the conduct of\nthis research.",
    "sourceUrl": "http://arxiv.org/abs/2106.00926v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Quantum Computing is a new paradigm that enables several advances which are\nimpossible using classical technology. With the rise of quantum computers, the\nsoftware is also invited to change so that it can better fit this new\ncomputation way. However, although a lot of research is being conducted in the\nquantum computing field, it is still scarce studies about the differences of\nthe software and software engineering in this new context. Therefore, this\narticle presents a systematic mapping study to present a wide review on the\nparticularities and characteristics of software that are developed for quantum\ncomputers. A total of 24 papers were selected using digital libraries with the\nobjective of answering three research questions elaborated in the conduct of\nthis research.</p>"
  },
  {
    "title": "A New Complete Class Complexity Metric",
    "date": "2014-03-22",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Vinay Singh",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1403.5614v1"
    },
    "publicTags": [],
    "summary": "Software complexity metrics is essential for minimizing the cost of software\nmaintenance. Package level and System level complexity cannot be measured\nwithout class level complexity. This research addresses the class complexity\nmetrics. This paper studies the existing class complexity metrics and proposes\na new class complexity metric CCC (Complete class complexity metric). The CCC\nmetric is then analytically evaluated by Weyuker's property.",
    "sourceUrl": "http://arxiv.org/abs/1403.5614v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software complexity metrics is essential for minimizing the cost of software\nmaintenance. Package level and System level complexity cannot be measured\nwithout class level complexity. This research addresses the class complexity\nmetrics. This paper studies the existing class complexity metrics and proposes\na new class complexity metric CCC (Complete class complexity metric). The CCC\nmetric is then analytically evaluated by Weyuker's property.</p>"
  },
  {
    "title": "Data-Driven Search-based Software Engineering",
    "date": "2018-01-30",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Vivek Nair",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1801.10241v2"
    },
    "publicTags": [],
    "summary": "This paper introduces Data-Driven Search-based Software Engineering (DSE),\nwhich combines insights from Mining Software Repositories (MSR) and\nSearch-based Software Engineering (SBSE). While MSR formulates software\nengineering problems as data mining problems, SBSE reformulates SE problems as\noptimization problems and use meta-heuristic algorithms to solve them. Both MSR\nand SBSE share the common goal of providing insights to improve software\nengineering. The algorithms used in these two areas also have intrinsic\nrelationships. We, therefore, argue that combining these two fields is useful\nfor situations (a) which require learning from a large data source or (b) when\noptimizers need to know the lay of the land to find better solutions, faster.\n  This paper aims to answer the following three questions: (1) What are the\nvarious topics addressed by DSE? (2) What types of data are used by the\nresearchers in this area? (3) What research approaches do researchers use? The\npaper briefly sets out to act as a practical guide to develop new DSE\ntechniques and also to serve as a teaching resource.\n  This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The\nresource contains 89 artifacts which are related to DSE, divided into 13 groups\nsuch as requirements engineering, software product lines, software processes.\nAll the materials in this repository have been used in recent software\nengineering papers; i.e., for all this material, there exist baseline results\nagainst which researchers can comparatively assess their new ideas.",
    "sourceUrl": "http://arxiv.org/abs/1801.10241v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>This paper introduces Data-Driven Search-based Software Engineering (DSE),\nwhich combines insights from Mining Software Repositories (MSR) and\nSearch-based Software Engineering (SBSE). While MSR formulates software\nengineering problems as data mining problems, SBSE reformulates SE problems as\noptimization problems and use meta-heuristic algorithms to solve them. Both MSR\nand SBSE share the common goal of providing insights to improve software\nengineering. The algorithms used in these two areas also have intrinsic\nrelationships. We, therefore, argue that combining these two fields is useful\nfor situations (a) which require learning from a large data source or (b) when\noptimizers need to know the lay of the land to find better solutions, faster.\n  This paper aims to answer the following three questions: (1) What are the\nvarious topics addressed by DSE? (2) What types of data are used by the\nresearchers in this area? (3) What research approaches do researchers use? The\npaper briefly sets out to act as a practical guide to develop new DSE\ntechniques and also to serve as a teaching resource.\n  This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The\nresource contains 89 artifacts which are related to DSE, divided into 13 groups\nsuch as requirements engineering, software product lines, software processes.\nAll the materials in this repository have been used in recent software\nengineering papers; i.e., for all this material, there exist baseline results\nagainst which researchers can comparatively assess their new ideas.</p>"
  },
  {
    "title": "Software Engineering as Instrumentation for the Long Tail of Scientific\n  Software",
    "date": "2013-09-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Daisie Huang",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1309.1806v1"
    },
    "publicTags": [],
    "summary": "The vast majority of the long tail of scientific software, the myriads of\ntools that implement the many analysis and visualization methods for different\nscientific fields, is highly specialized, purpose-built for a research project,\nand has to rely on community uptake and reuse for its continued development and\nmaintenance. Although uptake cannot be controlled over even guaranteed, some of\nthe key factors that influence whether new users or developers decide to adopt\nan existing tool or start a new one are about how easy or difficult it is to\nuse or enhance a tool for a purpose for which it was not originally designed.\nThe science of software engineering has produced techniques and practices that\nwould reduce or remove a variety of barriers to community uptake of software,\nbut for a variety of reasons employing trained software engineers as part of\nthe development of long tail scientific software has proven to be challenging.\nAs a consequence, community uptake of long tail tools is often far more\ndifficult than it would need to be, even though opportunities for reuse abound.\nWe discuss likely reasons why employing software engineering in the long tail\nis challenging, and propose that many of those obstacles could be addressed in\nthe form of a cross-cutting non-profit center of excellence that makes software\nengineering broadly accessible as a shared service, conceptually and in its\neffect similar to shared instrumentation.",
    "sourceUrl": "http://arxiv.org/abs/1309.1806v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The vast majority of the long tail of scientific software, the myriads of\ntools that implement the many analysis and visualization methods for different\nscientific fields, is highly specialized, purpose-built for a research project,\nand has to rely on community uptake and reuse for its continued development and\nmaintenance. Although uptake cannot be controlled over even guaranteed, some of\nthe key factors that influence whether new users or developers decide to adopt\nan existing tool or start a new one are about how easy or difficult it is to\nuse or enhance a tool for a purpose for which it was not originally designed.\nThe science of software engineering has produced techniques and practices that\nwould reduce or remove a variety of barriers to community uptake of software,\nbut for a variety of reasons employing trained software engineers as part of\nthe development of long tail scientific software has proven to be challenging.\nAs a consequence, community uptake of long tail tools is often far more\ndifficult than it would need to be, even though opportunities for reuse abound.\nWe discuss likely reasons why employing software engineering in the long tail\nis challenging, and propose that many of those obstacles could be addressed in\nthe form of a cross-cutting non-profit center of excellence that makes software\nengineering broadly accessible as a shared service, conceptually and in its\neffect similar to shared instrumentation.</p>"
  },
  {
    "title": "Software Security Analysis in 2030 and Beyond: A Research Roadmap",
    "date": "2024-09-26",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Marcel BÃ¶hme",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2409.17844v1"
    },
    "publicTags": [],
    "summary": "As our lives, our businesses, and indeed our world economy become\nincreasingly reliant on the secure operation of many interconnected software\nsystems, the software engineering research community is faced with\nunprecedented research challenges, but also with exciting new opportunities. In\nthis roadmap paper, we outline our vision of Software Security Analysis for the\nsoftware systems of the future. Given the recent advances in generative AI, we\nneed new methods to evaluate and maximize the security of code co-written by\nmachines. As our software systems become increasingly heterogeneous, we need\npractical approaches that work even if some functions are automatically\ngenerated, e.g., by deep neural networks. As software systems depend evermore\non the software supply chain, we need tools that scale to an entire ecosystem.\nWhat kind of vulnerabilities exist in future systems and how do we detect them?\nWhen all the shallow bugs are found, how do we discover vulnerabilities hidden\ndeeply in the system? Assuming we cannot find all security flaws, how can we\nnevertheless protect our system? To answer these questions, we start our\nresearch roadmap with a survey of recent advances in software security, then\ndiscuss open challenges and opportunities, and conclude with a long-term\nperspective for the field.",
    "sourceUrl": "http://arxiv.org/abs/2409.17844v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>As our lives, our businesses, and indeed our world economy become\nincreasingly reliant on the secure operation of many interconnected software\nsystems, the software engineering research community is faced with\nunprecedented research challenges, but also with exciting new opportunities. In\nthis roadmap paper, we outline our vision of Software Security Analysis for the\nsoftware systems of the future. Given the recent advances in generative AI, we\nneed new methods to evaluate and maximize the security of code co-written by\nmachines. As our software systems become increasingly heterogeneous, we need\npractical approaches that work even if some functions are automatically\ngenerated, e.g., by deep neural networks. As software systems depend evermore\non the software supply chain, we need tools that scale to an entire ecosystem.\nWhat kind of vulnerabilities exist in future systems and how do we detect them?\nWhen all the shallow bugs are found, how do we discover vulnerabilities hidden\ndeeply in the system? Assuming we cannot find all security flaws, how can we\nnevertheless protect our system? To answer these questions, we start our\nresearch roadmap with a survey of recent advances in software security, then\ndiscuss open challenges and opportunities, and conclude with a long-term\nperspective for the field.</p>"
  },
  {
    "title": "Tracking Down Software Cluster Bombs: A Current State Analysis of the\n  Free/Libre and Open Source Software (FLOSS) Ecosystem",
    "date": "2025-02-12",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Stefan Tatschner",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2502.08219v1"
    },
    "publicTags": [],
    "summary": "Throughout computer history, it has been repeatedly demonstrated that\ncritical software vulnerabilities can significantly affect the components\ninvolved. In the Free/Libre and Open Source Software (FLOSS) ecosystem, most\nsoftware is distributed through package repositories. Nowadays, monitoring\ncritical dependencies in a software system is essential for maintaining robust\nsecurity practices. This is particularly important due to new legal\nrequirements, such as the European Cyber Resilience Act, which necessitate that\nsoftware projects maintain a transparent track record with Software Bill of\nMaterials (SBOM) and ensure a good overall state. This study provides a summary\nof the current state of available FLOSS package repositories and addresses the\nchallenge of identifying problematic areas within a software ecosystem. These\nareas are analyzed in detail, quantifying the current state of the FLOSS\necosystem. The results indicate that while there are well-maintained projects\nwithin the FLOSS ecosystem, there are also high-impact projects that are\nsusceptible to supply chain attacks. This study proposes a method for analyzing\nthe current state and identifies missing elements, such as interfaces, for\nfuture research.",
    "sourceUrl": "http://arxiv.org/abs/2502.08219v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Throughout computer history, it has been repeatedly demonstrated that\ncritical software vulnerabilities can significantly affect the components\ninvolved. In the Free/Libre and Open Source Software (FLOSS) ecosystem, most\nsoftware is distributed through package repositories. Nowadays, monitoring\ncritical dependencies in a software system is essential for maintaining robust\nsecurity practices. This is particularly important due to new legal\nrequirements, such as the European Cyber Resilience Act, which necessitate that\nsoftware projects maintain a transparent track record with Software Bill of\nMaterials (SBOM) and ensure a good overall state. This study provides a summary\nof the current state of available FLOSS package repositories and addresses the\nchallenge of identifying problematic areas within a software ecosystem. These\nareas are analyzed in detail, quantifying the current state of the FLOSS\necosystem. The results indicate that while there are well-maintained projects\nwithin the FLOSS ecosystem, there are also high-impact projects that are\nsusceptible to supply chain attacks. This study proposes a method for analyzing\nthe current state and identifies missing elements, such as interfaces, for\nfuture research.</p>"
  },
  {
    "title": "Who is in Charge here? Understanding How Runtime Configuration Affects\n  Software along with Variables&Constants",
    "date": "2025-03-31",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Chaopeng Luo",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2503.23774v1"
    },
    "publicTags": [],
    "summary": "Runtime misconfiguration can lead to software performance degradation and\neven cause failure. Developers typically perform sanity checks during the\nconfiguration parsing stage to prevent invalid parameter values. However, we\ndiscovered that even valid values that pass these checks can also lead to\nunexpected severe consequences. Our study reveals the underlying reason: the\nvalue of runtime configuration parameters may interact with other constants and\nvariables when propagated and used, altering its original effect on software\nbehavior. Consequently, parameter values may no longer be valid when\nencountering complex runtime environments and workloads. Therefore, it is\nextremely challenging for users to properly configure the software before it\nstarts running. This paper presents the first comprehensive and in-depth study\n(to the best of our knowledge) on how configuration affects software at runtime\nthrough the interaction with constants, and variables (PCV Interaction).\nParameter values represent user intentions, constants embody developer\nknowledge, and variables are typically defined by the runtime environment and\nworkload. This interaction essentially illustrates how different roles jointly\ndetermine software behavior. In this regard, we studied 705 configuration\nparameters from 10 large-scale software systems. We reveal that a large portion\nof configuration parameters interact with constants/variables after parsing. We\nanalyzed the interaction patterns and their effects on software runtime\nbehavior. Furthermore, we highlighted the risks of PCV interaction and\nidentified potential issues behind specific interaction patterns. Our findings\nexpose the \"double edge\" of PCV interaction, providing new insights and\nmotivating the development of new automated techniques to help users configure\nsoftware appropriately and assist developers in designing better\nconfigurations.",
    "sourceUrl": "http://arxiv.org/abs/2503.23774v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Runtime misconfiguration can lead to software performance degradation and\neven cause failure. Developers typically perform sanity checks during the\nconfiguration parsing stage to prevent invalid parameter values. However, we\ndiscovered that even valid values that pass these checks can also lead to\nunexpected severe consequences. Our study reveals the underlying reason: the\nvalue of runtime configuration parameters may interact with other constants and\nvariables when propagated and used, altering its original effect on software\nbehavior. Consequently, parameter values may no longer be valid when\nencountering complex runtime environments and workloads. Therefore, it is\nextremely challenging for users to properly configure the software before it\nstarts running. This paper presents the first comprehensive and in-depth study\n(to the best of our knowledge) on how configuration affects software at runtime\nthrough the interaction with constants, and variables (PCV Interaction).\nParameter values represent user intentions, constants embody developer\nknowledge, and variables are typically defined by the runtime environment and\nworkload. This interaction essentially illustrates how different roles jointly\ndetermine software behavior. In this regard, we studied 705 configuration\nparameters from 10 large-scale software systems. We reveal that a large portion\nof configuration parameters interact with constants/variables after parsing. We\nanalyzed the interaction patterns and their effects on software runtime\nbehavior. Furthermore, we highlighted the risks of PCV interaction and\nidentified potential issues behind specific interaction patterns. Our findings\nexpose the \"double edge\" of PCV interaction, providing new insights and\nmotivating the development of new automated techniques to help users configure\nsoftware appropriately and assist developers in designing better\nconfigurations.</p>"
  },
  {
    "title": "Applying Slicing Technique to Software Architectures",
    "date": "2001-05-05",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jianjun Zhao",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/cs/0105008v1"
    },
    "publicTags": [],
    "summary": "Software architecture is receiving increasingly attention as a critical\ndesign level for software systems. As software architecture design resources\n(in the form of architectural specifications) are going to be accumulated, the\ndevelopment of techniques and tools to support architectural understanding,\ntesting, reengineering, maintenance, and reuse will become an important issue.\nThis paper introduces a new form of slicing, named architectural slicing, to\naid architectural understanding and reuse. In contrast to traditional slicing,\narchitectural slicing is designed to operate on the architectural specification\nof a software system, rather than the source code of a program. Architectural\nslicing provides knowledge about the high-level structure of a software system,\nrather than the low-level implementation details of a program. In order to\ncompute an architectural slice, we present the architecture information flow\ngraph which can be used to represent information flows in a software\narchitecture. Based on the graph, we give a two-phase algorithm to compute an\narchitectural slice.",
    "sourceUrl": "http://arxiv.org/abs/cs/0105008v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software architecture is receiving increasingly attention as a critical\ndesign level for software systems. As software architecture design resources\n(in the form of architectural specifications) are going to be accumulated, the\ndevelopment of techniques and tools to support architectural understanding,\ntesting, reengineering, maintenance, and reuse will become an important issue.\nThis paper introduces a new form of slicing, named architectural slicing, to\naid architectural understanding and reuse. In contrast to traditional slicing,\narchitectural slicing is designed to operate on the architectural specification\nof a software system, rather than the source code of a program. Architectural\nslicing provides knowledge about the high-level structure of a software system,\nrather than the low-level implementation details of a program. In order to\ncompute an architectural slice, we present the architecture information flow\ngraph which can be used to represent information flows in a software\narchitecture. Based on the graph, we give a two-phase algorithm to compute an\narchitectural slice.</p>"
  },
  {
    "title": "The Implications of Network-Centric Software Systems on Software\n  Architecture: A Critical Evaluation",
    "date": "2006-11-21",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Amine Chigani",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/cs/0611110v3"
    },
    "publicTags": [],
    "summary": "The purpose of this paper is to evaluate the impact of emerging\nnetwork-centric software systems on the field of software architecture. We\nfirst develop an insight concerning the term \"network-centric\" by presenting\nits origin and its implications within the context of software architecture. On\nthe basis of this insight, we present our definition of a network-centric\nframework and its distinguishing characteristics. We then enumerate the\nchallenges that face the field of software architecture as software development\nshifts from a platform-centric to a network-centric model. In order to face\nthese challenges, we propose a formal approach embodied in a new architectural\nstyle that supports overcoming these challenges at the architectural level.\nFinally, we conclude by presenting an illustrative example to demonstrate the\nusefulness of the concepts of network centricity, summarizing our\ncontributions, and linking our approach to future work that needs to be done in\nthis area.",
    "sourceUrl": "http://arxiv.org/abs/cs/0611110v3",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The purpose of this paper is to evaluate the impact of emerging\nnetwork-centric software systems on the field of software architecture. We\nfirst develop an insight concerning the term \"network-centric\" by presenting\nits origin and its implications within the context of software architecture. On\nthe basis of this insight, we present our definition of a network-centric\nframework and its distinguishing characteristics. We then enumerate the\nchallenges that face the field of software architecture as software development\nshifts from a platform-centric to a network-centric model. In order to face\nthese challenges, we propose a formal approach embodied in a new architectural\nstyle that supports overcoming these challenges at the architectural level.\nFinally, we conclude by presenting an illustrative example to demonstrate the\nusefulness of the concepts of network centricity, summarizing our\ncontributions, and linking our approach to future work that needs to be done in\nthis area.</p>"
  },
  {
    "title": "Measuring Cognitive Activities in Software Engineering",
    "date": "2007-02-01",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Pierre Robillard",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/cs/0702001v1"
    },
    "publicTags": [],
    "summary": "This paper presents an approach to the study of cognitive activities in\ncollaborative software development. This approach has been developed by a\nmultidisciplinary team made up of software engineers and cognitive\npsychologists. The basis of this approach is to improve our understanding of\nsoftware development by observing professionals at work. The goal is to derive\nlines of conduct or good practices based on observations and analyses of the\nprocesses that are naturally used by software engineers. The strategy involved\nis derived from a standard approach in cognitive science. It is based on the\nvideotaping of the activities of software engineers, transcription of the\nvideos, coding of the transcription, defining categories from the coded\nepisodes and defining cognitive behaviors or dialogs from the categories. This\nproject presents two original contributions that make this approach generic in\nsoftware engineering. The first contribution is the introduction of a formal\nhierarchical coding scheme, which will enable comparison of various types of\nobservations. The second is the merging of psychological and statistical\nanalysis approaches to build a cognitive model. The details of this new\napproach are illustrated with the initial data obtained from the analysis of\ntechnical review meetings.",
    "sourceUrl": "http://arxiv.org/abs/cs/0702001v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>This paper presents an approach to the study of cognitive activities in\ncollaborative software development. This approach has been developed by a\nmultidisciplinary team made up of software engineers and cognitive\npsychologists. The basis of this approach is to improve our understanding of\nsoftware development by observing professionals at work. The goal is to derive\nlines of conduct or good practices based on observations and analyses of the\nprocesses that are naturally used by software engineers. The strategy involved\nis derived from a standard approach in cognitive science. It is based on the\nvideotaping of the activities of software engineers, transcription of the\nvideos, coding of the transcription, defining categories from the coded\nepisodes and defining cognitive behaviors or dialogs from the categories. This\nproject presents two original contributions that make this approach generic in\nsoftware engineering. The first contribution is the introduction of a formal\nhierarchical coding scheme, which will enable comparison of various types of\nobservations. The second is the merging of psychological and statistical\nanalysis approaches to build a cognitive model. The details of this new\napproach are illustrated with the initial data obtained from the analysis of\ntechnical review meetings.</p>"
  },
  {
    "title": "A Kind of Representation of Common Knowledge and its Application in\n  Requirements Analysis",
    "date": "2011-01-31",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Bojin Zheng",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1101.5957v1"
    },
    "publicTags": [],
    "summary": "Since the birth of software engineering, it always are recognized as one pure\nengineering subject, therefore, the foundational scientific problems are not\npaid much attention. This paper proposes that Requirements Analysis, the kernel\nprocess of software engineering, can be modeled based on the concept of \"common\nknowledge\". Such a model would make us understand the nature of this process.\nThis paper utilizes the formal language as the tool to characterize the \"common\nknowledge\"-based Requirements Analysis model, and theoretically proves that :\n1) the precondition of success of software projects regardless of cost would be\nthat the participants in a software project have fully known the requirement\nspecification, if the participants do not understand the meaning of the other\nparticipants; 2) the precondition of success of software projects regardless of\ncost would be that the union set of knowledge of basic facts of the\nparticipants in a software project can fully cover the requirement\nspecification, if the participants can always understand the meaning of the\nother participants. These two theorems may have potential meanings to propose\nnew software engineering methodology.",
    "sourceUrl": "http://arxiv.org/abs/1101.5957v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Since the birth of software engineering, it always are recognized as one pure\nengineering subject, therefore, the foundational scientific problems are not\npaid much attention. This paper proposes that Requirements Analysis, the kernel\nprocess of software engineering, can be modeled based on the concept of \"common\nknowledge\". Such a model would make us understand the nature of this process.\nThis paper utilizes the formal language as the tool to characterize the \"common\nknowledge\"-based Requirements Analysis model, and theoretically proves that :\n1) the precondition of success of software projects regardless of cost would be\nthat the participants in a software project have fully known the requirement\nspecification, if the participants do not understand the meaning of the other\nparticipants; 2) the precondition of success of software projects regardless of\ncost would be that the union set of knowledge of basic facts of the\nparticipants in a software project can fully cover the requirement\nspecification, if the participants can always understand the meaning of the\nother participants. These two theorems may have potential meanings to propose\nnew software engineering methodology.</p>"
  },
  {
    "title": "A Classical Fuzzy Approach for Software Effort Estimation on Machine\n  Learning Technique",
    "date": "2011-12-16",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "S. Malathi",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1112.3877v1"
    },
    "publicTags": [],
    "summary": "Software Cost Estimation with resounding reliability,productivity and\ndevelopment effort is a challenging and onerous task. This has incited the\nsoftware community to give much needed thrust and delve into extensive research\nin software effort estimation for evolving sophisticated methods. Estimation by\nanalogy is one of the expedient techniques in software effort estimation field.\nHowever, the methodology utilized for the estimation of software effort by\nanalogy is not able to handle the categorical data in an explicit and precise\nmanner. A new approach has been developed in this paper to estimate software\neffort for projects represented by categorical or numerical data using\nreasoning by analogy and fuzzy approach. The existing historical data sets,\nanalyzed with fuzzy logic, produce accurate results in comparison to the data\nset analyzed with the earlier methodologies.",
    "sourceUrl": "http://arxiv.org/abs/1112.3877v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software Cost Estimation with resounding reliability,productivity and\ndevelopment effort is a challenging and onerous task. This has incited the\nsoftware community to give much needed thrust and delve into extensive research\nin software effort estimation for evolving sophisticated methods. Estimation by\nanalogy is one of the expedient techniques in software effort estimation field.\nHowever, the methodology utilized for the estimation of software effort by\nanalogy is not able to handle the categorical data in an explicit and precise\nmanner. A new approach has been developed in this paper to estimate software\neffort for projects represented by categorical or numerical data using\nreasoning by analogy and fuzzy approach. The existing historical data sets,\nanalyzed with fuzzy logic, produce accurate results in comparison to the data\nset analyzed with the earlier methodologies.</p>"
  },
  {
    "title": "The Sensemaking-Coevolution-Implementation Theory of Software Design",
    "date": "2013-02-17",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Paul Ralph",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1302.4061v1"
    },
    "publicTags": [],
    "summary": "Understanding software design practice is critical to understanding modern\ninformation systems development. New developments in empirical software\nengineering, information systems design science and the interdisciplinary\ndesign literature combined with recent advances in process theory and\ntestability have created a situation ripe for innovation. Consequently, this\npaper utilizes these breakthroughs to formulate a process theory of software\ndesign practice: Sensemaking-Coevolution-Implementation Theory explains how\ncomplex software systems are created by collocated software development teams\nin organizations. It posits that an independent agent (design team) creates a\nsoftware system by alternating between three activities: organizing their\nperceptions about the context, mutually refining their understandings of the\ncontext and design space, and manifesting their understanding of the design\nspace in a technological artifact. This theory development paper defines and\nillustrates Sensemaking-Coevolution-Implementation Theory, grounds its concepts\nand relationships in existing literature, conceptually evaluates the theory and\nsituates it in the broader context of information systems development.",
    "sourceUrl": "http://arxiv.org/abs/1302.4061v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Understanding software design practice is critical to understanding modern\ninformation systems development. New developments in empirical software\nengineering, information systems design science and the interdisciplinary\ndesign literature combined with recent advances in process theory and\ntestability have created a situation ripe for innovation. Consequently, this\npaper utilizes these breakthroughs to formulate a process theory of software\ndesign practice: Sensemaking-Coevolution-Implementation Theory explains how\ncomplex software systems are created by collocated software development teams\nin organizations. It posits that an independent agent (design team) creates a\nsoftware system by alternating between three activities: organizing their\nperceptions about the context, mutually refining their understandings of the\ncontext and design space, and manifesting their understanding of the design\nspace in a technological artifact. This theory development paper defines and\nillustrates Sensemaking-Coevolution-Implementation Theory, grounds its concepts\nand relationships in existing literature, conceptually evaluates the theory and\nsituates it in the broader context of information systems development.</p>"
  },
  {
    "title": "Significance of Coupling and Cohesion on Design Quality",
    "date": "2014-02-11",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Poornima U. S.",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1402.2375v1"
    },
    "publicTags": [],
    "summary": "In recent years, the complexity of the software is increasing due to\nautomation of every segment of application. Software is nowhere remained as\none-time development product since its architectural dimension is increasing\nwith addition of new requirements over a short duration. Object Oriented\nDevelopment (OOD) methodology is a popular development approach for such\nsystems which perceives and models the requirements as real world entities.\nClasses and Objects logically represent the entities in the solution space and\nquality of the software is directly depending on the design quality of these\nlogical entities. Cohesion and Coupling (C&C) are two major design decisive\nfactors in OOD which impacts the design of a class and dependency between them\nin complex software. It is also most significant to measure C&C for software to\ncontrol the complexity level as requirements increases. Several metrics are in\npractice to quantify C&C which plays a major role in measuring the design\nquality. The software industries are focusing on increasing and measuring the\nquality of the product through quality design to continue their market image in\nthe competitive world. As a part of our research, this paper highlights on the\nimpact of C&C on design quality of a complex system and its measures to\nquantify the overall quality of software.",
    "sourceUrl": "http://arxiv.org/abs/1402.2375v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In recent years, the complexity of the software is increasing due to\nautomation of every segment of application. Software is nowhere remained as\none-time development product since its architectural dimension is increasing\nwith addition of new requirements over a short duration. Object Oriented\nDevelopment (OOD) methodology is a popular development approach for such\nsystems which perceives and models the requirements as real world entities.\nClasses and Objects logically represent the entities in the solution space and\nquality of the software is directly depending on the design quality of these\nlogical entities. Cohesion and Coupling (C&C) are two major design decisive\nfactors in OOD which impacts the design of a class and dependency between them\nin complex software. It is also most significant to measure C&C for software to\ncontrol the complexity level as requirements increases. Several metrics are in\npractice to quantify C&C which plays a major role in measuring the design\nquality. The software industries are focusing on increasing and measuring the\nquality of the product through quality design to continue their market image in\nthe competitive world. As a part of our research, this paper highlights on the\nimpact of C&C on design quality of a complex system and its measures to\nquantify the overall quality of software.</p>"
  },
  {
    "title": "The Multiple Facets of Software Diversity: Recent Developments in Year\n  2000 and Beyond",
    "date": "2014-09-25",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Benoit Baudry",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1409.7324v1"
    },
    "publicTags": [],
    "summary": "Early experiments with software diversity in the mid 1970's investigated\nN-version programming and recovery blocks to increase the reliability of\nembedded systems. Four decades later, the literature about software diversity\nhas expanded in multiple directions: goals (fault-tolerance, security, software\nengineering); means (managed or automated diversity) and analytical studies\n(quantification of diversity and its impact). Our paper contributes to the\nfield of software diversity as the first paper that adopts an inclusive vision\nof the area, with an emphasis on the most recent advances in the field. This\nsurvey includes classical work about design and data diversity for fault\ntolerance, as well as the cybersecurity literature that investigates\nrandomization at different system levels. It broadens this standard scope of\ndiversity, to include the study and exploitation of natural diversity and the\nmanagement of diverse software products. Our survey includes the most recent\nworks, with an emphasis from 2000 to present. The targeted audience is\nresearchers and practitioners in one of the surveyed fields, who miss the big\npicture of software diversity. Assembling the multiple facets of this\nfascinating topic sheds a new light on the field.",
    "sourceUrl": "http://arxiv.org/abs/1409.7324v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Early experiments with software diversity in the mid 1970's investigated\nN-version programming and recovery blocks to increase the reliability of\nembedded systems. Four decades later, the literature about software diversity\nhas expanded in multiple directions: goals (fault-tolerance, security, software\nengineering); means (managed or automated diversity) and analytical studies\n(quantification of diversity and its impact). Our paper contributes to the\nfield of software diversity as the first paper that adopts an inclusive vision\nof the area, with an emphasis on the most recent advances in the field. This\nsurvey includes classical work about design and data diversity for fault\ntolerance, as well as the cybersecurity literature that investigates\nrandomization at different system levels. It broadens this standard scope of\ndiversity, to include the study and exploitation of natural diversity and the\nmanagement of diverse software products. Our survey includes the most recent\nworks, with an emphasis from 2000 to present. The targeted audience is\nresearchers and practitioners in one of the surveyed fields, who miss the big\npicture of software diversity. Assembling the multiple facets of this\nfascinating topic sheds a new light on the field.</p>"
  },
  {
    "title": "Building Resource Adaptive Software Systems (BRASS): Objectives and\n  System Evaluation",
    "date": "2015-10-07",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Jeffrey Hughes",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1510.02104v1"
    },
    "publicTags": [],
    "summary": "As modern software systems continue inexorably to increase in complexity and\ncapability, users have become accustomed to periodic cycles of updating and\nupgrading to avoid obsolescence -- if at some cost in terms of frustration. In\nthe case of the U.S. military, having access to well-functioning software\nsystems and underlying content is critical to national security, but updates\nare no less problematic than among civilian users and often demand considerable\ntime and expense. To address these challenges, DARPA has announced a new\nfour-year research project to investigate the fundamental computational and\nalgorithmic requirements necessary for software systems and data to remain\nrobust and functional in excess of 100 years. The Building Resource Adaptive\nSoftware Systems, or BRASS, program seeks to realize foundational advances in\nthe design and implementation of long-lived software systems that can\ndynamically adapt to changes in the resources they depend upon and environments\nin which they operate. MIT Lincoln Laboratory will provide the test framework\nand evaluation of proposed software tools in support of this revolutionary\nvision.",
    "sourceUrl": "http://arxiv.org/abs/1510.02104v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>As modern software systems continue inexorably to increase in complexity and\ncapability, users have become accustomed to periodic cycles of updating and\nupgrading to avoid obsolescence -- if at some cost in terms of frustration. In\nthe case of the U.S. military, having access to well-functioning software\nsystems and underlying content is critical to national security, but updates\nare no less problematic than among civilian users and often demand considerable\ntime and expense. To address these challenges, DARPA has announced a new\nfour-year research project to investigate the fundamental computational and\nalgorithmic requirements necessary for software systems and data to remain\nrobust and functional in excess of 100 years. The Building Resource Adaptive\nSoftware Systems, or BRASS, program seeks to realize foundational advances in\nthe design and implementation of long-lived software systems that can\ndynamically adapt to changes in the resources they depend upon and environments\nin which they operate. MIT Lincoln Laboratory will provide the test framework\nand evaluation of proposed software tools in support of this revolutionary\nvision.</p>"
  },
  {
    "title": "An Empirical Study on Leanness and Flexibility in Distributed Software\n  Development",
    "date": "2017-11-03",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Mohammad Abdur Razzak",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1711.01097v1"
    },
    "publicTags": [],
    "summary": "Nowadays, many individuals and teams involved on projects are already using\nagile development techniques as part of their daily work. However, we have much\nless experience in how to scale and manage agile practices in distributed\nsoftware development. Distributed and global development- that requiring\nattention to many technical, organizational, and cultural issues as the teams\ninteract to cooperatively delivery the solution. Alongside, very large team\nsizes, teams of teams, and more complex management structures forcing\nadditional attention to coordination and management. At this level, there is an\nincreasing need to standardize best practices to avoid reinvention and\nmiscommunication across artifacts and processes. Complexity issues in\nenterprise software delivery can have significant impact on the adoption of\nagile approaches. As a consequence, agile strategies will typically need to be\nevaluated, tailored, and perhaps combined with traditional approaches to suit\nthe particular context. The characteristics of software products and software\ndevelopment processes open up new possibilities that are different from those\noffered in other domains to achieve leanness and flexibility. Whilst Lean\nprinciples are universal, a further understanding of the techniques required to\napply such principles from a software development angle. Thus, the aim of this\nresearch is to identify, how leanness facilitate flexibility in distributed\nsoftware development to speed-up development process.",
    "sourceUrl": "http://arxiv.org/abs/1711.01097v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Nowadays, many individuals and teams involved on projects are already using\nagile development techniques as part of their daily work. However, we have much\nless experience in how to scale and manage agile practices in distributed\nsoftware development. Distributed and global development- that requiring\nattention to many technical, organizational, and cultural issues as the teams\ninteract to cooperatively delivery the solution. Alongside, very large team\nsizes, teams of teams, and more complex management structures forcing\nadditional attention to coordination and management. At this level, there is an\nincreasing need to standardize best practices to avoid reinvention and\nmiscommunication across artifacts and processes. Complexity issues in\nenterprise software delivery can have significant impact on the adoption of\nagile approaches. As a consequence, agile strategies will typically need to be\nevaluated, tailored, and perhaps combined with traditional approaches to suit\nthe particular context. The characteristics of software products and software\ndevelopment processes open up new possibilities that are different from those\noffered in other domains to achieve leanness and flexibility. Whilst Lean\nprinciples are universal, a further understanding of the techniques required to\napply such principles from a software development angle. Thus, the aim of this\nresearch is to identify, how leanness facilitate flexibility in distributed\nsoftware development to speed-up development process.</p>"
  },
  {
    "title": "Software engineering and the SP Theory of Intelligence",
    "date": "2017-08-18",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "J Gerard Wolff",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/1708.06665v2"
    },
    "publicTags": [],
    "summary": "This paper describes a novel approach to software engineering derived from\nthe \"SP Theory of Intelligence\" and its realisation in the \"SP Computer Model\".\nDespite superficial appearances, it is shown that many of the key ideas in\nsoftware engineering have counterparts in the structure and workings of the SP\nsystem. Potential benefits of this new approach to software engineering\ninclude: the automation or semi-automation of software development, with\nsupport for programming of the SP system where necessary; allowing programmers\nto concentrate on 'world-oriented' parallelism, without worries about\nparallelism to speed up processing; support for the long-term goal of\nprogramming the SP system via written or spoken natural language; reducing or\neliminating the distinction between 'design' and 'implementation'; reducing or\neliminating operations like compiling or interpretation; reducing or\neliminating the need for verification of software; reducing the need for\nvalidation of software; no formal distinction between program and database; the\npotential for substantial reductions in the number of types of data file and\nthe number of computer languages; benefits for version control; and reducing\ntechnical debt.",
    "sourceUrl": "http://arxiv.org/abs/1708.06665v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>This paper describes a novel approach to software engineering derived from\nthe \"SP Theory of Intelligence\" and its realisation in the \"SP Computer Model\".\nDespite superficial appearances, it is shown that many of the key ideas in\nsoftware engineering have counterparts in the structure and workings of the SP\nsystem. Potential benefits of this new approach to software engineering\ninclude: the automation or semi-automation of software development, with\nsupport for programming of the SP system where necessary; allowing programmers\nto concentrate on 'world-oriented' parallelism, without worries about\nparallelism to speed up processing; support for the long-term goal of\nprogramming the SP system via written or spoken natural language; reducing or\neliminating the distinction between 'design' and 'implementation'; reducing or\neliminating operations like compiling or interpretation; reducing or\neliminating the need for verification of software; reducing the need for\nvalidation of software; no formal distinction between program and database; the\npotential for substantial reductions in the number of types of data file and\nthe number of computer languages; benefits for version control; and reducing\ntechnical debt.</p>"
  },
  {
    "title": "Opening the Software Engineering Toolbox for the Assessment of\n  Trustworthy AI",
    "date": "2020-07-14",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Mohit Kumar Ahuja",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2007.07768v2"
    },
    "publicTags": [],
    "summary": "Trustworthiness is a central requirement for the acceptance and success of\nhuman-centered artificial intelligence (AI). To deem an AI system as\ntrustworthy, it is crucial to assess its behaviour and characteristics against\na gold standard of Trustworthy AI, consisting of guidelines, requirements, or\nonly expectations. While AI systems are highly complex, their implementations\nare still based on software. The software engineering community has a\nlong-established toolbox for the assessment of software systems, especially in\nthe context of software testing. In this paper, we argue for the application of\nsoftware engineering and testing practices for the assessment of trustworthy\nAI. We make the connection between the seven key requirements as defined by the\nEuropean Commission's AI high-level expert group and established procedures\nfrom software engineering and raise questions for future work.",
    "sourceUrl": "http://arxiv.org/abs/2007.07768v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Trustworthiness is a central requirement for the acceptance and success of\nhuman-centered artificial intelligence (AI). To deem an AI system as\ntrustworthy, it is crucial to assess its behaviour and characteristics against\na gold standard of Trustworthy AI, consisting of guidelines, requirements, or\nonly expectations. While AI systems are highly complex, their implementations\nare still based on software. The software engineering community has a\nlong-established toolbox for the assessment of software systems, especially in\nthe context of software testing. In this paper, we argue for the application of\nsoftware engineering and testing practices for the assessment of trustworthy\nAI. We make the connection between the seven key requirements as defined by the\nEuropean Commission's AI high-level expert group and established procedures\nfrom software engineering and raise questions for future work.</p>"
  },
  {
    "title": "Memory visualization tool for training neural network",
    "date": "2021-10-25",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Mahendran N",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2110.13264v1"
    },
    "publicTags": [],
    "summary": "Software developed helps world a better place ranging from system software,\nopen source, application software and so on. Software engineering does have\nneural network models applied to code suggestion, bug report summarizing and so\non to demonstrate their effectiveness at a real SE task. Software and machine\nlearning algorithms combine to make software give better solutions and\nunderstanding of environment. In software, there are both generalized\napplications which helps solve problems for entire world and also some specific\napplications which helps one particular community. To address the computational\nchallenge in deep learning, many tools exploit hardware features such as\nmulti-core CPUs and many-core GPUs to shorten the training time. Machine\nlearning algorithms have a greater impact in the world but there is a\nconsiderable amount of memory utilization during the process. We propose a new\ntool for analysis of memory utilized for developing and training deep learning\nmodels. Our tool results in visual utilization of memory concurrently. Various\nparameters affecting the memory utilization are analysed while training. This\ntool helps in knowing better idea of processes or models which consumes more\nmemory.",
    "sourceUrl": "http://arxiv.org/abs/2110.13264v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software developed helps world a better place ranging from system software,\nopen source, application software and so on. Software engineering does have\nneural network models applied to code suggestion, bug report summarizing and so\non to demonstrate their effectiveness at a real SE task. Software and machine\nlearning algorithms combine to make software give better solutions and\nunderstanding of environment. In software, there are both generalized\napplications which helps solve problems for entire world and also some specific\napplications which helps one particular community. To address the computational\nchallenge in deep learning, many tools exploit hardware features such as\nmulti-core CPUs and many-core GPUs to shorten the training time. Machine\nlearning algorithms have a greater impact in the world but there is a\nconsiderable amount of memory utilization during the process. We propose a new\ntool for analysis of memory utilized for developing and training deep learning\nmodels. Our tool results in visual utilization of memory concurrently. Various\nparameters affecting the memory utilization are analysed while training. This\ntool helps in knowing better idea of processes or models which consumes more\nmemory.</p>"
  },
  {
    "title": "DesCert: Design for Certification",
    "date": "2022-03-29",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Natarajan Shankar",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2203.15178v1"
    },
    "publicTags": [],
    "summary": "The goal of the DARPA Automated Rapid Certification Of Software (ARCOS)\nprogram is to \"automate the evaluation of software assurance evidence to enable\ncertifiers to determine rapidly that system risk is acceptable.\" As part of\nthis program, the DesCert project focuses on the assurance-driven development\nof new software. The DesCert team consists of SRI International, Honeywell\nResearch, and the University of Washington. We have adopted a formal,\ntool-based approach to the construction of software artifacts that are\nsupported by rigorous evidence. The DesCert workflow integrates evidence\ngeneration into a design process that goes from requirements capture and\nanalysis to the decomposition of the high-level software requirements into\narchitecture properties and software components with assertional contracts, and\non to software that can be analyzed both dynamically and statically. The\ngenerated evidence is organized by means of an assurance ontology and\nintegrated into the RACK knowledge base.",
    "sourceUrl": "http://arxiv.org/abs/2203.15178v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The goal of the DARPA Automated Rapid Certification Of Software (ARCOS)\nprogram is to \"automate the evaluation of software assurance evidence to enable\ncertifiers to determine rapidly that system risk is acceptable.\" As part of\nthis program, the DesCert project focuses on the assurance-driven development\nof new software. The DesCert team consists of SRI International, Honeywell\nResearch, and the University of Washington. We have adopted a formal,\ntool-based approach to the construction of software artifacts that are\nsupported by rigorous evidence. The DesCert workflow integrates evidence\ngeneration into a design process that goes from requirements capture and\nanalysis to the decomposition of the high-level software requirements into\narchitecture properties and software components with assertional contracts, and\non to software that can be analyzed both dynamically and statically. The\ngenerated evidence is organized by means of an assurance ontology and\nintegrated into the RACK knowledge base.</p>"
  },
  {
    "title": "Exploring Software Reusability Metrics with Q&A Forum Data",
    "date": "2020-05-18",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Matthew T. Patrick",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2005.08845v1"
    },
    "publicTags": [],
    "summary": "Question and answer (Q&A) forums contain valuable information regarding\nsoftware reuse, but they can be challenging to analyse due to their\nunstructured free text. Here we introduce a new approach (LANLAN), using word\nembeddings and machine learning, to harness information available in\nStackOverflow. Specifically, we consider two different kinds of user\ncommunication describing difficulties encountered in software reuse: 'problem\nreports' point to potential defects, while 'support requests' ask for\nclarification on software usage. Word embeddings were trained on 1.6 billion\ntokens from StackOverflow and applied to identify which Q&A forum messages\n(from two large open source projects: Eclipse and Bioconductor) correspond to\nproblem reports or support requests. LANLAN achieved an area under the receiver\noperator curve (AUROC) of over 0.9; it can be used to explore the relationship\nbetween software reusability metrics and difficulties encountered by users, as\nwell as predict the number of difficulties users will face in the future. Q&A\nforum data can help improve understanding of software reuse, and may be\nharnessed as an additional resource to evaluate software reusability metrics.",
    "sourceUrl": "http://arxiv.org/abs/2005.08845v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Question and answer (Q&A) forums contain valuable information regarding\nsoftware reuse, but they can be challenging to analyse due to their\nunstructured free text. Here we introduce a new approach (LANLAN), using word\nembeddings and machine learning, to harness information available in\nStackOverflow. Specifically, we consider two different kinds of user\ncommunication describing difficulties encountered in software reuse: 'problem\nreports' point to potential defects, while 'support requests' ask for\nclarification on software usage. Word embeddings were trained on 1.6 billion\ntokens from StackOverflow and applied to identify which Q&A forum messages\n(from two large open source projects: Eclipse and Bioconductor) correspond to\nproblem reports or support requests. LANLAN achieved an area under the receiver\noperator curve (AUROC) of over 0.9; it can be used to explore the relationship\nbetween software reusability metrics and difficulties encountered by users, as\nwell as predict the number of difficulties users will face in the future. Q&A\nforum data can help improve understanding of software reuse, and may be\nharnessed as an additional resource to evaluate software reusability metrics.</p>"
  },
  {
    "title": "Software Engineering in Australasia",
    "date": "2022-06-11",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Sherlock A. Licorish",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2206.05397v1"
    },
    "publicTags": [],
    "summary": "Six months ago an important call was made for researchers globally to provide\ninsights into the way Software Engineering is done in their region. Heeding\nthis call we hereby outline the position Software Engineering in Australasia\n(New Zealand and Australia). This article first considers the software\ndevelopment methods practices and tools that are popular in the Australasian\nsoftware engineering community. We then briefly review the particular strengths\nof software engineering researchers in Australasia. Finally we make an open\ncall for collaborators by reflecting on our current position and identifying\nfuture opportunities",
    "sourceUrl": "http://arxiv.org/abs/2206.05397v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Six months ago an important call was made for researchers globally to provide\ninsights into the way Software Engineering is done in their region. Heeding\nthis call we hereby outline the position Software Engineering in Australasia\n(New Zealand and Australia). This article first considers the software\ndevelopment methods practices and tools that are popular in the Australasian\nsoftware engineering community. We then briefly review the particular strengths\nof software engineering researchers in Australasia. Finally we make an open\ncall for collaborators by reflecting on our current position and identifying\nfuture opportunities</p>"
  },
  {
    "title": "Measuring Software Testability via Automatically Generated Test Cases",
    "date": "2023-07-30",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Luca Guglielmo",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2307.16185v1"
    },
    "publicTags": [],
    "summary": "Estimating software testability can crucially assist software managers to\noptimize test budgets and software quality. In this paper, we propose a new\napproach that radically differs from the traditional approach of pursuing\ntestability measurements based on software metrics, e.g., the size of the code\nor the complexity of the designs. Our approach exploits automatic test\ngeneration and mutation analysis to quantify the evidence about the relative\nhardness of developing effective test cases. In the paper, we elaborate on the\nintuitions and the methodological choices that underlie our proposal for\nestimating testability, introduce a technique and a prototype that allows for\nconcretely estimating testability accordingly, and discuss our findings out of\na set of experiments in which we compare the performance of our estimations\nboth against and in combination with traditional software metrics. The results\nshow that our testability estimates capture a complementary dimension of\ntestability that can be synergistically combined with approaches based on\nsoftware metrics to improve the accuracy of predictions.",
    "sourceUrl": "http://arxiv.org/abs/2307.16185v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Estimating software testability can crucially assist software managers to\noptimize test budgets and software quality. In this paper, we propose a new\napproach that radically differs from the traditional approach of pursuing\ntestability measurements based on software metrics, e.g., the size of the code\nor the complexity of the designs. Our approach exploits automatic test\ngeneration and mutation analysis to quantify the evidence about the relative\nhardness of developing effective test cases. In the paper, we elaborate on the\nintuitions and the methodological choices that underlie our proposal for\nestimating testability, introduce a technique and a prototype that allows for\nconcretely estimating testability accordingly, and discuss our findings out of\na set of experiments in which we compare the performance of our estimations\nboth against and in combination with traditional software metrics. The results\nshow that our testability estimates capture a complementary dimension of\ntestability that can be synergistically combined with approaches based on\nsoftware metrics to improve the accuracy of predictions.</p>"
  },
  {
    "title": "Performance of Genetic Algorithms in the Context of Software Model\n  Refactoring",
    "date": "2023-08-26",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Vittorio Cortellessa",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2308.13875v1"
    },
    "publicTags": [],
    "summary": "Software systems continuously evolve due to new functionalities,\nrequirements, or maintenance activities. In the context of software evolution,\nsoftware refactoring has gained a strategic relevance. The space of possible\nsoftware refactoring is usually very large, as it is given by the combinations\nof different refactoring actions that can produce software system alternatives.\nMulti-objective algorithms have shown the ability to discover alternatives by\npursuing different objectives simultaneously. Performance of such algorithms in\nthe context of software model refactoring is of paramount importance.\nTherefore, in this paper, we conduct a performance analysis of three genetic\nalgorithms to compare them in terms of performance and quality of solutions.\nOur results show that there are significant differences in performance among\nthe algorithms (e.g., PESA2 seems to be the fastest one, while NSGA-II shows\nthe least memory usage).",
    "sourceUrl": "http://arxiv.org/abs/2308.13875v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>Software systems continuously evolve due to new functionalities,\nrequirements, or maintenance activities. In the context of software evolution,\nsoftware refactoring has gained a strategic relevance. The space of possible\nsoftware refactoring is usually very large, as it is given by the combinations\nof different refactoring actions that can produce software system alternatives.\nMulti-objective algorithms have shown the ability to discover alternatives by\npursuing different objectives simultaneously. Performance of such algorithms in\nthe context of software model refactoring is of paramount importance.\nTherefore, in this paper, we conduct a performance analysis of three genetic\nalgorithms to compare them in terms of performance and quality of solutions.\nOur results show that there are significant differences in performance among\nthe algorithms (e.g., PESA2 seems to be the fastest one, while NSGA-II shows\nthe least memory usage).</p>"
  },
  {
    "title": "VULNERLIZER: Cross-analysis Between Vulnerabilities and Software\n  Libraries",
    "date": "2023-09-18",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Irdin Pekaric",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2309.09649v1"
    },
    "publicTags": [],
    "summary": "The identification of vulnerabilities is a continuous challenge in software\nprojects. This is due to the evolution of methods that attackers employ as well\nas the constant updates to the software, which reveal additional issues. As a\nresult, new and innovative approaches for the identification of vulnerable\nsoftware are needed. In this paper, we present VULNERLIZER, which is a novel\nframework for cross-analysis between vulnerabilities and software libraries. It\nuses CVE and software library data together with clustering algorithms to\ngenerate links between vulnerabilities and libraries. In addition, the training\nof the model is conducted in order to reevaluate the generated associations.\nThis is achieved by updating the assigned weights. Finally, the approach is\nthen evaluated by making the predictions using the CVE data from the test set.\nThe results show that the VULNERLIZER has a great potential in being able to\npredict future vulnerable libraries based on an initial input CVE entry or a\nsoftware library. The trained model reaches a prediction accuracy of 75% or\nhigher.",
    "sourceUrl": "http://arxiv.org/abs/2309.09649v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The identification of vulnerabilities is a continuous challenge in software\nprojects. This is due to the evolution of methods that attackers employ as well\nas the constant updates to the software, which reveal additional issues. As a\nresult, new and innovative approaches for the identification of vulnerable\nsoftware are needed. In this paper, we present VULNERLIZER, which is a novel\nframework for cross-analysis between vulnerabilities and software libraries. It\nuses CVE and software library data together with clustering algorithms to\ngenerate links between vulnerabilities and libraries. In addition, the training\nof the model is conducted in order to reevaluate the generated associations.\nThis is achieved by updating the assigned weights. Finally, the approach is\nthen evaluated by making the predictions using the CVE data from the test set.\nThe results show that the VULNERLIZER has a great potential in being able to\npredict future vulnerable libraries based on an initial input CVE entry or a\nsoftware library. The trained model reaches a prediction accuracy of 75% or\nhigher.</p>"
  },
  {
    "title": "Improving the visibility and citability of exoplanet research software",
    "date": "2023-12-28",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Alice Allen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2312.17297v1"
    },
    "publicTags": [],
    "summary": "The Astrophysics Source Code Library (ASCL) is a free online registry for\nsource codes of interest to astronomers, astrophysicists, and planetary\nscientists. It lists, and in some cases houses, software that has been used in\nresearch appearing in or submitted to peer-reviewed publications. As of\nDecember 2023, it has over 3300 software entries and is indexed by NASA's\nAstrophysics Data System (ADS) and Clarivate's Web of Science.\n  In 2020, NASA created the Exoplanet Modeling and Analysis Center (EMAC).\nHoused at the Goddard Space Flight Center, EMAC serves, in part, as a catalog\nand repository for exoplanet research resources. EMAC has 240 entries (as of\nDecember 2023), 78% of which are for downloadable software.\n  This oral presentation covered the collaborative work the ASCL, EMAC, and ADS\nare doing to increase the discoverability and citability of EMAC's software\nentries and to strengthen the ASCL's ability to serve the planetary science\ncommunity. It also introduced two new projects, Virtual Astronomy Software\nTalks (VAST) and Exoplanet Virtual Astronomy Software Talks (exoVAST), that\nprovide additional opportunities for discoverability of EMAC software\nresources.",
    "sourceUrl": "http://arxiv.org/abs/2312.17297v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>The Astrophysics Source Code Library (ASCL) is a free online registry for\nsource codes of interest to astronomers, astrophysicists, and planetary\nscientists. It lists, and in some cases houses, software that has been used in\nresearch appearing in or submitted to peer-reviewed publications. As of\nDecember 2023, it has over 3300 software entries and is indexed by NASA's\nAstrophysics Data System (ADS) and Clarivate's Web of Science.\n  In 2020, NASA created the Exoplanet Modeling and Analysis Center (EMAC).\nHoused at the Goddard Space Flight Center, EMAC serves, in part, as a catalog\nand repository for exoplanet research resources. EMAC has 240 entries (as of\nDecember 2023), 78% of which are for downloadable software.\n  This oral presentation covered the collaborative work the ASCL, EMAC, and ADS\nare doing to increase the discoverability and citability of EMAC's software\nentries and to strengthen the ASCL's ability to serve the planetary science\ncommunity. It also introduced two new projects, Virtual Astronomy Software\nTalks (VAST) and Exoplanet Virtual Astronomy Software Talks (exoVAST), that\nprovide additional opportunities for discoverability of EMAC software\nresources.</p>"
  },
  {
    "title": "Octopus: On-device language model for function calling of software APIs",
    "date": "2024-04-02",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Wei Chen",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2404.01549v1"
    },
    "publicTags": [],
    "summary": "In the rapidly evolving domain of artificial intelligence, Large Language\nModels (LLMs) play a crucial role due to their advanced text processing and\ngeneration abilities. This study introduces a new strategy aimed at harnessing\non-device LLMs in invoking software APIs. We meticulously compile a dataset\nderived from software API documentation and apply fine-tuning to LLMs with\ncapacities of 2B, 3B and 7B parameters, specifically to enhance their\nproficiency in software API interactions. Our approach concentrates on refining\nthe models' grasp of API structures and syntax, significantly enhancing the\naccuracy of API function calls. Additionally, we propose \\textit{conditional\nmasking} techniques to ensure outputs in the desired formats and reduce error\nrates while maintaining inference speeds. We also propose a novel benchmark\ndesigned to evaluate the effectiveness of LLMs in API interactions,\nestablishing a foundation for subsequent research. Octopus, the fine-tuned\nmodel, is proved to have better performance than GPT-4 for the software APIs\ncalling. This research aims to advance automated software development and API\nintegration, representing substantial progress in aligning LLM capabilities\nwith the demands of practical software engineering applications.",
    "sourceUrl": "http://arxiv.org/abs/2404.01549v1",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>In the rapidly evolving domain of artificial intelligence, Large Language\nModels (LLMs) play a crucial role due to their advanced text processing and\ngeneration abilities. This study introduces a new strategy aimed at harnessing\non-device LLMs in invoking software APIs. We meticulously compile a dataset\nderived from software API documentation and apply fine-tuning to LLMs with\ncapacities of 2B, 3B and 7B parameters, specifically to enhance their\nproficiency in software API interactions. Our approach concentrates on refining\nthe models' grasp of API structures and syntax, significantly enhancing the\naccuracy of API function calls. Additionally, we propose \\textit{conditional\nmasking} techniques to ensure outputs in the desired formats and reduce error\nrates while maintaining inference speeds. We also propose a novel benchmark\ndesigned to evaluate the effectiveness of LLMs in API interactions,\nestablishing a foundation for subsequent research. Octopus, the fine-tuned\nmodel, is proved to have better performance than GPT-4 for the software APIs\ncalling. This research aims to advance automated software development and API\nintegration, representing substantial progress in aligning LLM capabilities\nwith the demands of practical software engineering applications.</p>"
  },
  {
    "title": "Open Source Software Development Tool Installation: Challenges and\n  Strategies For Novice Developers",
    "date": "2024-04-23",
    "contentGroup": "Articles",
    "internalTags": [
      ""
    ],
    "author": {
      "name": "Larissa Salerno",
      "email": "",
      "organization": ""
    },
    "publication": {
      "name": "arXiv",
      "url": "http://arxiv.org/abs/2404.14637v2"
    },
    "publicTags": [],
    "summary": "As the world of technology advances, so do the tools that software developers\nuse to create new programs. In recent years, software development tools have\nbecome more popular, allowing developers to work more efficiently and produce\nhigher-quality software. Still, installing such tools can be challenging for\nnovice developers at the early stage of their careers, as they may face\nchallenges, such as compatibility issues (e.g., operating systems). Therefore,\nthis work aims to investigate the challenges novice developers face in software\ndevelopment when installing software development tools. To investigate these,\nwe conducted an analysis of 24 live software installation sessions to observe\nchallenges and comprehend their actions, the strategies they apply, and the\ntype of source of information they consult when encountering challenges. Our\nfindings show that unclear documentation, such as installation instructions,\nand inadequate feedback during the installation process are common challenges\nfaced by novice developers. Moreover, reformulating search queries and relying\non non-official documentation were some of the strategies employed to overcome\nchallenges. Based on our findings, we provide practical recommendations for\ntool vendors, tool users, and researchers.",
    "sourceUrl": "http://arxiv.org/abs/2404.14637v2",
    "language": "en",
    "readingTime": 1,
    "imageUrl": "",
    "relatedContent": [],
    "content": "<p>As the world of technology advances, so do the tools that software developers\nuse to create new programs. In recent years, software development tools have\nbecome more popular, allowing developers to work more efficiently and produce\nhigher-quality software. Still, installing such tools can be challenging for\nnovice developers at the early stage of their careers, as they may face\nchallenges, such as compatibility issues (e.g., operating systems). Therefore,\nthis work aims to investigate the challenges novice developers face in software\ndevelopment when installing software development tools. To investigate these,\nwe conducted an analysis of 24 live software installation sessions to observe\nchallenges and comprehend their actions, the strategies they apply, and the\ntype of source of information they consult when encountering challenges. Our\nfindings show that unclear documentation, such as installation instructions,\nand inadequate feedback during the installation process are common challenges\nfaced by novice developers. Moreover, reformulating search queries and relying\non non-official documentation were some of the strategies employed to overcome\nchallenges. Based on our findings, we provide practical recommendations for\ntool vendors, tool users, and researchers.</p>"
  }
]